{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0a596da8-027d-4479-95cb-006662857d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from numba import njit\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a22f59f9-1a55-4858-8ca2-38f228afe7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(v):\n",
    "    return (v - np.mean(v))/np.std(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cd2a6121-4709-498a-9067-bcd297bb2085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(v):\n",
    "    v = v - np.max(v, axis = 1).reshape(-1,1)\n",
    "    exp_array = np.exp(v)\n",
    "    return exp_array / np.sum(exp_array, axis = 1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7081392f-e993-416a-a851-90cd55a7b779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_filter(n, s) :\n",
    "      return np.zeros((n, s, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6e13dfe0-2ec6-4a9a-8a85-f9f48f6f54ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(map):    \n",
    "    output_tensor = np.maximum(map, 0)\n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "82c7aef5-4095-4f08-8619-e58a44ca6a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooling(map, dim):\n",
    "    n_sample = map.shape[0]\n",
    "    n_channels = map.shape[1]\n",
    "    map_size = map.shape[2]\n",
    "    output_size = map.shape[2]//dim\n",
    "\n",
    "    return map.reshape(n_sample, n_channels, output_size, dim, output_size, dim).max(axis=(3,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9312f736-65c4-4890-b180-c0478920ef00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @njit\n",
    "def convolution(sample, kernel):\n",
    "    kernel_size = kernel.shape[1]\n",
    "    pad = kernel_size//2\n",
    "    n_sample, n, h, w = sample.shape\n",
    "    \n",
    "    out_sample = np.zeros((n_sample, n, h+2*pad, w+2*pad))\n",
    "    for j in range(n_sample):\n",
    "        for i in range(n):\n",
    "            out_sample[j][i] = np.pad(sample[j][i], (pad,), 'constant', constant_values = 0)\n",
    "\n",
    "    sample = out_sample\n",
    "    \n",
    "    size_feature_map = h # output dimension for same convolution\n",
    "    n_out_channels = kernel.shape[0] # number of output channels is the number of filters (for now)\n",
    "\n",
    "    #now we perform the convolution\n",
    "\n",
    "    # initializing the output tensor to zeros\n",
    "    output_tensor = np.zeros((n_sample, n_out_channels, size_feature_map, size_feature_map))\n",
    "\n",
    "    # loop for all the kernels\n",
    "    for i in range(n_out_channels):\n",
    "        current_kernel = kernel[i] # get the required kernel\n",
    "\n",
    "        for r in range(size_feature_map): # for the rows\n",
    "            for c in range(size_feature_map): # for the columns\n",
    "                window = sample[:, :, r : r + kernel_size, c : c + kernel_size] #take the window\n",
    "                value = np.sum(window*current_kernel, axis = (1,2,3)) # multiply with the kernel and sum  up the value\n",
    "                output_tensor[:, i, r, c] = value # update the result tensor\n",
    "\n",
    "    return output_tensor # return the result tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d3c6413f-561e-4ce0-97da-c49ecc06cd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @njit\n",
    "def other_convolution(sample, kernel, pad):\n",
    "    kernel_size = kernel.shape[1]\n",
    "    n_sample, n, h, w = sample.shape\n",
    "    \n",
    "    out_sample = np.zeros((n_sample, n, h+2*pad, w+2*pad))\n",
    "    \n",
    "    for j in range(n_sample):\n",
    "        for i in range(n):\n",
    "            out_sample[j][i] = np.pad(sample[j][i], (pad,), 'constant', constant_values = 0)\n",
    "\n",
    "    sample = out_sample\n",
    "    \n",
    "    size_feature_map = h + 2*pad - kernel_size + 1 # output dimension for same convolution\n",
    "    n_out_channels = kernel.shape[0] # number of output channels is the number of filters (for now)\n",
    "\n",
    "    #now we perform the convolution\n",
    "\n",
    "    # initializing the output tensor to zeros\n",
    "    output_tensor = np.zeros((n_sample, n_out_channels, size_feature_map, size_feature_map))\n",
    "\n",
    "    # loop for all the kernels\n",
    "    for i in range(n_out_channels):\n",
    "        current_kernel = kernel[i] # get the required kernel\n",
    "\n",
    "        for r in range(size_feature_map): # for the rows\n",
    "            for c in range(size_feature_map): # for the columns\n",
    "                window = sample[:, :, r : r + kernel_size, c : c + kernel_size] #take the window\n",
    "                value = np.sum(window*current_kernel, axis = (1,2,3)) # multiply with the kernel and sum  up the value\n",
    "                output_tensor[:, i, r, c] = value # update the result tensor\n",
    "\n",
    "    return output_tensor # return the result tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "13f8f060-98b8-4275-a4f7-8feb6f6c015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv2d:\n",
    "    def __init__(self, num, size):\n",
    "        self.kernel = np.random.randn(num, size, size)/9 # Xavier Initialisation\n",
    "        self.bias = np.random.rand(num, size, size)/9\n",
    "        self.layer_input = None\n",
    "        self.layer_output = None\n",
    "        self.layer_activated = None\n",
    "        self.kernel_grad = None\n",
    "        self.size = size\n",
    "    \n",
    "    def forward_pass(self, sample): # DONE\n",
    "        # now I am getting multiple images\n",
    "        # so another dimension has been added to sample\n",
    "        \n",
    "        # input is fine\n",
    "        self.layer_input = sample\n",
    "        \n",
    "        # convolution also fixed to accomodate multiple samples\n",
    "        output_tensor = convolution(sample, self.kernel)\n",
    "    \n",
    "        self.layer_output = output_tensor\n",
    "        self.layer_activated = relu(output_tensor) # relu is totally fine\n",
    "        \n",
    "        return self.layer_activated # return the result tensor\n",
    "        \n",
    "    def backward_pass(self, inp_grad): # DONE\n",
    "        n_sample, n, h, w = self.layer_input.shape\n",
    "        pass_grad = np.zeros((n_sample, n, h, w))\n",
    "        \n",
    "        relu_mat = self.layer_activated\n",
    "        relu_mat[np.nonzero(relu_mat)] = 1\n",
    "        \n",
    "        inp_grad = inp_grad * relu_mat # must be dimensionally equivalent\n",
    "        other_grad = np.sum(inp_grad, axis = 0)/32\n",
    "        \n",
    "        kernel_grad = other_convolution(self.layer_input, other_grad, self.size//2)\n",
    "        # other convolution also fixed\n",
    "        \n",
    "        self.kernel_grad = np.sum(kernel_grad, axis = 0)/32\n",
    "        \n",
    "        flip_kernel = np.flip(self.kernel_grad, axis=(0,1))\n",
    "        not_final = convolution(inp_grad, flip_kernel)\n",
    "        still_not_final = np.sum(not_final, axis = 1)\n",
    "        \n",
    "        \n",
    "        # for j in range(n_sample):\n",
    "        #     for i in range(n):\n",
    "        #         pass_grad[j][i] = still_not_final\n",
    "                \n",
    "        for i in range(n):\n",
    "                pass_grad[:, i] = still_not_final\n",
    "        \n",
    "        return pass_grad\n",
    "        \n",
    "    def update(self): # DONE\n",
    "        # print(self.kernel_grad.shape)\n",
    "        self.kernel -= 0.001*self.kernel_grad # applying gradient descent\n",
    "        return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c90d2bd6-82bd-4a9b-8cf7-e24433d0e556",
   "metadata": {},
   "outputs": [],
   "source": [
    "class maxpool2d:\n",
    "    def __init__(self, dim):\n",
    "        self.dim = dim\n",
    "        self.layer_input = None\n",
    "        self.layer_output = None\n",
    "        \n",
    "    def forward_pass(self , sample):\n",
    "        self.layer_input = sample\n",
    "        self.layer_output = pooling(sample, 2)\n",
    "        return self.layer_output\n",
    "    \n",
    "    # @njit\n",
    "    def backward_pass(self, inp_grad):\n",
    "        n_sample, n, h, w = self.layer_input.shape\n",
    "        x = self.layer_input\n",
    "        \n",
    "        pass_mat = np.zeros((n_sample, n,h,w))\n",
    "    \n",
    "        # for j in range(n_sample):\n",
    "        #     for i in range(n):\n",
    "        #         for r in range(0, h-1, 2):\n",
    "        #             for c in range(0, w-1, 2):\n",
    "        #                 window = x[j, i, r:r+2, c:c+2]\n",
    "        #                 max_ind = np.unravel_index(window.argmax(), window.shape)\n",
    "        #                 pass_mat[j, i, r:r+2, c:c+2][max_ind] = inp_grad[j, i, r//2, c//2]\n",
    "        \n",
    "        for i in range(h//2):\n",
    "            for j in range(w//2):\n",
    "                h_start = i * 2\n",
    "                h_end = h_start + 2\n",
    "                w_start = j * 2\n",
    "                w_end = w_start + 2\n",
    "                \n",
    "                X = self.layer_input\n",
    "                X_pool = X[:, :, h_start:h_end, w_start:w_end]\n",
    "                mask = (X_pool == np.max(X_pool, axis=(2, 3))[:, :, None, None])\n",
    "                pass_mat[:, :, h_start:h_end, w_start:w_end] += mask * (inp_grad[:, :, i, j])[:, :, None, None]\n",
    "                \n",
    "            \n",
    "        return pass_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8f11c194-63c0-4de5-889d-94710e61f1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fc_1:\n",
    "    def __init__(self, size, next_size):\n",
    "        self.weights = np.random.randn(next_size, size)/9\n",
    "        self.bias = np.random.randn(next_size, )/9\n",
    "        self.weights_grad = None\n",
    "        self.bias_grad = None\n",
    "        self.layer_input = None\n",
    "        self.layer_output = None\n",
    "        self.layer_output_active = None\n",
    "        \n",
    "    def forward_pass(self, sample):\n",
    "        self.layer_input = sample\n",
    "        \n",
    "        # need to calculate this correctly\n",
    "        output = sample @ self.weights.T + self.bias\n",
    "        self.layer_output = output\n",
    "        self.layer_output_active = relu(output)\n",
    "        \n",
    "        return self.layer_output_active\n",
    "    \n",
    "    def backward_pass(self, inp_grad):\n",
    "        relu_mat = self.layer_output_active\n",
    "        relu_mat[np.nonzero(relu_mat)] = 1\n",
    "        # print(relu_mat.shape)\n",
    "        \n",
    "        relued_grad = inp_grad * relu_mat\n",
    "        \n",
    "        pass_grad = relued_grad @ self.weights\n",
    "        \n",
    "        self.weights_grad = np.sum(relued_grad.T @ self.layer_input, axis = 0)/32\n",
    "        self.bias_grad = np.sum(relued_grad, axis = 0)/32\n",
    "        \n",
    "        return pass_grad\n",
    "    \n",
    "    def update(self):\n",
    "        self.weights -= 0.001*self.weights_grad\n",
    "        self.bias -= 0.001*self.bias_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "12c47614-e2f5-4ff4-963c-a259cf130fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fc_2:\n",
    "    def __init__(self, size, next_size):\n",
    "        self.weights = np.random.randn(next_size, size)/9\n",
    "        self.bias = np.random.randn(next_size, )/9\n",
    "        self.weights_grad = None\n",
    "        self.bias_grad = None\n",
    "        self.layer_input = None\n",
    "        self.layer_output = None\n",
    "        \n",
    "    def forward_pass(self, sample):\n",
    "        self.layer_input = sample\n",
    "        output = sample @ self.weights.T + self.bias\n",
    "        self.layer_output = output\n",
    "        return self.layer_output\n",
    "    \n",
    "    def backward_pass(self, inp_grad):\n",
    "        pass_grad = inp_grad @ self.weights #this is still missing the RELU gradient\n",
    "        \n",
    "        self.weights_grad = np.sum(inp_grad.T @ self.layer_input, axis = 0)/32\n",
    "        self.bias_grad = np.sum(inp_grad, axis = 0)/32\n",
    "        \n",
    "        return pass_grad\n",
    "    \n",
    "    def update(self):\n",
    "        self.weights -= 0.001*self.weights_grad\n",
    "        self.bias -= 0.001*self.bias_grad\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2cb0075f-f60a-4ef5-a8f2-c67500d31270",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[247., 312., 309., 269.],\n",
       "         [351., 384., 357., 337.],\n",
       "         [283., 367., 277., 201.],\n",
       "         [112., 158.,  78.,  12.]],\n",
       "\n",
       "        [[235., 300., 295., 255.],\n",
       "         [279., 374., 338., 268.],\n",
       "         [242., 353., 259., 174.],\n",
       "         [ 96., 144.,  64.,  22.]],\n",
       "\n",
       "        [[235., 300., 295., 255.],\n",
       "         [285., 380., 345., 275.],\n",
       "         [275., 355., 265., 205.],\n",
       "         [100., 150.,  70.,  20.]]],\n",
       "\n",
       "\n",
       "       [[[247., 312., 309., 269.],\n",
       "         [351., 384., 357., 337.],\n",
       "         [283., 367., 277., 201.],\n",
       "         [112., 158.,  78.,  12.]],\n",
       "\n",
       "        [[235., 300., 295., 255.],\n",
       "         [279., 374., 338., 268.],\n",
       "         [242., 353., 259., 174.],\n",
       "         [ 96., 144.,  64.,  22.]],\n",
       "\n",
       "        [[235., 300., 295., 255.],\n",
       "         [285., 380., 345., 275.],\n",
       "         [275., 355., 265., 205.],\n",
       "         [100., 150.,  70.,  20.]]]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array(([[[1, 1, 2, 4], [1, -1, 1, -1], [1, 1, 1, 1], [1, -1, 1, -1]], [[2, 2, 2, 0], [2, 0, 2, 2], [0, 2, 2, 2], [2, 2, 0, 2]], [[3, 3, 3, 3], [30, 3, 3, 30], [3, 3, 3, -5], [3, 3, 3, -5]]], [[[1, 1, 2, 4], [1, -1, 1, -1], [1, 1, 1, 1], [1, -1, 1, -1]], [[2, 2, 2, 0], [2, 0, 2, 2], [0, 2, 2, 2], [2, 2, 0, 2]], [[3, 3, 3, 3], [30, 3, 3, 30], [3, 3, 3, -5], [3, 3, 3, -5]]]))\n",
    "b = np.array(([[[5,5,5], [5, 7,5], [5,5,5]], [[5,4,5], [5, 5,5], [5,5,5]], [[5,5,5], [5, 5,5], [5,5,5]]]))\n",
    "c = np.array([[6], [6], [6]])\n",
    "d = np.array([[5,5,5], [5, 5,5], [5,5,5]])\n",
    "# np.pad(a[0], (1,), 'constant', constant_values = 0)\n",
    "# a.shape\n",
    "convolution(a, b)\n",
    "# np.tensordot(b,c, axes = (1, 0))\n",
    "# np.dot(d, c)\n",
    "# pooling(a, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2c7ad364-0eea-4c61-a1ce-139db454ccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6638e951-9124-4430-a40a-ca082ddfadac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 3072), (50000,))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set1 = unpickle(\"./content/data_batch_1\")\n",
    "set2 = unpickle(\"./content/data_batch_2\")\n",
    "set3 = unpickle(\"./content/data_batch_3\")\n",
    "set4 = unpickle(\"./content/data_batch_4\")\n",
    "set5 = unpickle(\"./content/data_batch_5\")\n",
    "x_train = np.vstack((set1[b'data'], set2[b'data'], set3[b'data'], set4[b'data'], set5[b'data']))\n",
    "y_train = np.hstack((np.array(set1[b'labels']), np.array(set2[b'labels']), np.array(set3[b'labels']), np.array(set4[b'labels']) ,np.array(set5[b'labels']) ))\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1bb5da4c-255d-424a-b9bd-4b7d34face3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trial = []\n",
    "y_trial = y_train[0:32]\n",
    "for i in range(32):\n",
    "    x_trial.append(x_train[i].reshape(3,32,32))\n",
    "x_trial = np.array(x_trial)\n",
    "y_trial = np.array(y_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4db6b32d-7b2d-45d5-bcb1-bbd8ae630484",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = conv2d(32, 3)\n",
    "pool1 = maxpool2d(2)\n",
    "conv2 = conv2d(64, 5)\n",
    "pool2 = maxpool2d(2)\n",
    "conv3 = conv2d(64, 3)\n",
    "fc1 = fc_1(4096, 64)\n",
    "fc2 = fc_2(64, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7d47fefa-9003-4d40-adbb-6c3de45c49e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(x, y):\n",
    "    a1 = conv1.forward_pass(x)\n",
    "    a2 = pool1.forward_pass(a1)\n",
    "    a3 = conv2.forward_pass(a2)\n",
    "    a4 = pool2.forward_pass(a3)\n",
    "    \n",
    "    # cannot flatten this directly at this position\n",
    "    a5 = conv3.forward_pass(a4).reshape(32, 4096)\n",
    "    # print(a5.shape)\n",
    "    a6 = fc1.forward_pass(a5)\n",
    "    a7 = fc2.forward_pass(a6)\n",
    "    out = softmax(a7)\n",
    "    # print(out.shape)\n",
    "    # print(i, \"actual: \", y[i],  \"prediction: \", np.argmax(out))\n",
    "\n",
    "    onehot = np.zeros(10)\n",
    "    onehot[y] = 1\n",
    "    grad_fc2 = out - onehot\n",
    "    # print(grad_fc2.shape)\n",
    "\n",
    "    grad_fc1 = fc2.backward_pass(grad_fc2)\n",
    "    grad_conv3 = fc1.backward_pass(grad_fc1).reshape(32, 64, 8, 8)\n",
    "    grad_pool2 = conv3.backward_pass(grad_conv3)\n",
    "    grad_conv2 = pool2.backward_pass(grad_pool2)\n",
    "    grad_pool1 = conv2.backward_pass(grad_conv2)\n",
    "    grad_conv1 = pool1.backward_pass(grad_pool1)\n",
    "    init_grad = conv1.backward_pass(grad_conv1)\n",
    "\n",
    "    fc2.update()\n",
    "    fc1.update()\n",
    "    conv3.update()\n",
    "    conv2.update()\n",
    "    conv1.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3e3ccac5-63dd-4cd9-88c3-047e0bcdd90f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6, 4, 3,\n",
       "       6, 6, 2, 6, 3, 5, 4, 0, 0, 9])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7e39820d-eef9-43c0-9ad9-9c1ad3b2dff0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.321716070175171  seconds\n"
     ]
    }
   ],
   "source": [
    "begin = time.time()\n",
    "run(x_trial, y_trial)\n",
    "print(time.time()-begin, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc93eda3-45e4-431c-83ed-a3899e40be07",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = conv2d(32, 3)\n",
    "pool1 = maxpool2d(2)\n",
    "conv2 = conv2d(64, 5)\n",
    "pool2 = maxpool2d(2)\n",
    "conv3 = conv2d(64, 3)\n",
    "fc1 = fc_1(4096, 64)\n",
    "fc2 = fc_2(64, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "541944bd-bb59-43ec-acb3-505f360d102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(X, Y):\n",
    "    begin = time.time()\n",
    "    for i in range(100):\n",
    "        idx = np.random.randint(50000, size=32)\n",
    "        x = X[idx]\n",
    "        y = Y[idx]\n",
    "        \n",
    "        # print(x.shape)\n",
    "\n",
    "        x = x.reshape(32, 3, 32, 32)\n",
    "        \n",
    "        # print(x.shape)\n",
    "    \n",
    "        a1 = conv1.forward_pass(x)\n",
    "        a2 = pool1.forward_pass(a1)\n",
    "        a3 = conv2.forward_pass(a2)\n",
    "        a4 = pool2.forward_pass(a3)\n",
    "\n",
    "        # cannot flatten this directly at this position\n",
    "        a5 = conv3.forward_pass(a4).reshape(32, 4096)\n",
    "        # print(a5.shape)\n",
    "        a6 = fc1.forward_pass(a5)\n",
    "        a7 = fc2.forward_pass(a6)\n",
    "        out = softmax(a7)\n",
    "        # print(out.shape)\n",
    "        # print(i, \"actual: \", y[i],  \"prediction: \", np.argmax(out))\n",
    "\n",
    "        onehot = np.zeros(10)\n",
    "        onehot[y] = 1\n",
    "        grad_fc2 = out - onehot\n",
    "        # print(grad_fc2.shape)\n",
    "\n",
    "        grad_fc1 = fc2.backward_pass(grad_fc2)\n",
    "        grad_conv3 = fc1.backward_pass(grad_fc1).reshape(32, 64, 8, 8)\n",
    "        grad_pool2 = conv3.backward_pass(grad_conv3)\n",
    "        grad_conv2 = pool2.backward_pass(grad_pool2)\n",
    "        grad_pool1 = conv2.backward_pass(grad_conv2)\n",
    "        grad_conv1 = pool1.backward_pass(grad_pool1)\n",
    "        init_grad = conv1.backward_pass(grad_conv1)\n",
    "\n",
    "        fc2.update()\n",
    "        fc1.update()\n",
    "        conv3.update()\n",
    "        conv2.update()\n",
    "        conv1.update()\n",
    "        \n",
    "        print(i, \"time: \", time.time()-begin, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72cb7c6-1b8d-4420-a274-a65e0114061f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 time:  9.375553131103516  seconds\n",
      "1 time:  18.536211013793945  seconds\n",
      "2 time:  27.906214952468872  seconds\n",
      "3 time:  39.283106088638306  seconds\n",
      "4 time:  50.215439319610596  seconds\n",
      "5 time:  60.45882821083069  seconds\n",
      "6 time:  70.59019494056702  seconds\n",
      "7 time:  80.3832471370697  seconds\n",
      "8 time:  89.55738806724548  seconds\n",
      "9 time:  102.79946208000183  seconds\n",
      "10 time:  113.89669990539551  seconds\n",
      "11 time:  123.10000419616699  seconds\n",
      "12 time:  132.6106400489807  seconds\n",
      "13 time:  143.32399916648865  seconds\n",
      "14 time:  153.39506912231445  seconds\n",
      "15 time:  162.510183095932  seconds\n",
      "16 time:  171.66717910766602  seconds\n",
      "17 time:  180.84370708465576  seconds\n",
      "18 time:  190.00481700897217  seconds\n",
      "19 time:  199.49189114570618  seconds\n",
      "20 time:  208.65070128440857  seconds\n",
      "21 time:  218.72846913337708  seconds\n",
      "22 time:  227.8660192489624  seconds\n",
      "23 time:  236.99682903289795  seconds\n",
      "24 time:  246.42545008659363  seconds\n",
      "25 time:  256.05435013771057  seconds\n",
      "26 time:  265.20826411247253  seconds\n",
      "27 time:  274.33370208740234  seconds\n",
      "28 time:  283.4426951408386  seconds\n",
      "29 time:  292.58068108558655  seconds\n",
      "30 time:  301.6834650039673  seconds\n",
      "31 time:  310.8131673336029  seconds\n",
      "32 time:  319.92130517959595  seconds\n",
      "33 time:  329.05874705314636  seconds\n",
      "34 time:  338.179878950119  seconds\n",
      "35 time:  347.2842309474945  seconds\n",
      "36 time:  356.5258700847626  seconds\n",
      "37 time:  365.6461191177368  seconds\n",
      "38 time:  374.7615451812744  seconds\n",
      "39 time:  383.86723732948303  seconds\n",
      "40 time:  393.22872400283813  seconds\n",
      "41 time:  402.3580250740051  seconds\n",
      "42 time:  411.46159315109253  seconds\n",
      "43 time:  420.5678231716156  seconds\n",
      "44 time:  429.6883132457733  seconds\n",
      "45 time:  438.77962708473206  seconds\n",
      "46 time:  447.9003891944885  seconds\n",
      "47 time:  457.1306519508362  seconds\n",
      "48 time:  466.27407813072205  seconds\n",
      "49 time:  475.37829303741455  seconds\n",
      "50 time:  484.49745416641235  seconds\n",
      "51 time:  493.602774143219  seconds\n",
      "52 time:  502.71853613853455  seconds\n",
      "53 time:  512.1106691360474  seconds\n",
      "54 time:  523.2500822544098  seconds\n",
      "55 time:  533.3627300262451  seconds\n"
     ]
    }
   ],
   "source": [
    "sgd(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8662e6-8dfa-48b0-8b40-55ba077cb374",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
