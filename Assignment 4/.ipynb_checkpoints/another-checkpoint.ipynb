{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d50b55a1-889d-4b68-8e81-23e14e9c9f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "121fac86-973e-4f29-bd55-848d11bf89b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_filter(n, s) :\n",
    "      return np.zeros((n, s, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30d03e82-c64e-438d-9534-d8b800c01723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83047d65-efe5-4349-81c4-d41b3144b19a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 3072), (50000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set1 = unpickle(\"./content/data_batch_1\")\n",
    "set2 = unpickle(\"./content/data_batch_2\")\n",
    "set3 = unpickle(\"./content/data_batch_3\")\n",
    "set4 = unpickle(\"./content/data_batch_4\")\n",
    "set5 = unpickle(\"./content/data_batch_5\")\n",
    "x_train = np.vstack((set1[b'data'], set2[b'data'], set3[b'data'], set4[b'data'], set5[b'data']))\n",
    "y_train = np.hstack((np.array(set1[b'labels']), np.array(set2[b'labels']), np.array(set3[b'labels']), np.array(set4[b'labels']) ,np.array(set5[b'labels']) ))\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d46048e1-7d4e-4116-b855-bee569483bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(v):\n",
    "    return (v - np.mean(v))/np.std(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04c1afda-14d9-4816-9987-9ee4516d9f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(map):    \n",
    "    output_tensor = np.maximum(map, 0)\n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f57d9cbc-9ba0-4f63-89f0-75914f7a5a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooling(map, dim):\n",
    "    n_channels = map.shape[0]\n",
    "    map_size = map.shape[1]\n",
    "    output_size = map.shape[1]//dim\n",
    "\n",
    "    return map.reshape(n_channels, output_size, dim, output_size, dim).max(axis=(2,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01d163f5-efb1-4da4-bdbf-cf320ef239d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(v):\n",
    "    output = np.exp(v)\n",
    "    final_output = output/(np.sum(output))\n",
    "    \n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b91eddad-299a-444f-9b6d-0a61e61d2c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution(sample, kernel):\n",
    "    # assert len(sample.shape) == len(kernel.shape) - 1 # check that the filter and sample dimensions match \n",
    "    kernel_size = kernel.shape[1]\n",
    "    pad = kernel_size//2\n",
    "    n, h, w = sample.shape\n",
    "    \n",
    "    out_sample = np.zeros((n, h+2*pad, w+2*pad))\n",
    "    \n",
    "    for i in range(n):\n",
    "        out_sample[i] = np.pad(sample[i], (pad,), 'constant', constant_values = 0)\n",
    "    sample = out_sample\n",
    "    \n",
    "    size_feature_map = h # output dimension for same convolution\n",
    "    n_out_channels = kernel.shape[0] # number of output channels is the number of filters (for now)\n",
    "\n",
    "    #now we perform the convolution\n",
    "\n",
    "    # initializing the output tensor to zeros\n",
    "    output_tensor = create_filter(n_out_channels, size_feature_map)\n",
    "\n",
    "    # loop for all the kernels\n",
    "    for i in range(n_out_channels):\n",
    "        current_kernel = kernel[i] # get the required kernel\n",
    "\n",
    "        for r in range(size_feature_map): # for the rows\n",
    "            for c in range(size_feature_map): # for the columns\n",
    "                window = sample[:, r : r + kernel_size, c : c + kernel_size] #take the window\n",
    "                value = np.sum(window*current_kernel, axis = None) # multiply with the kernel and sum  up the value\n",
    "                output_tensor[i, r, c] = value # update the result tensor\n",
    "\n",
    "    return output_tensor # return the result tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e0d33885-7c35-42e5-90a0-b39eebf70624",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "        self.conv1_kernel = np.random.randn(32,3,3)*np.sqrt(2/2)\n",
    "        self.conv1_output = None\n",
    "        self.conv1_output_activated = None\n",
    "        \n",
    "        self.pool1_output = None\n",
    "        \n",
    "        self.conv2_kernel = np.random.randn(64,5,5)*np.sqrt(2/4)\n",
    "        self.conv2_output = None\n",
    "        self.conv2_output_activated = None\n",
    "        \n",
    "        self.pool2_output = None\n",
    "        \n",
    "        self.conv3_kernel = np.random.randn(64,3,3)*np.sqrt(2/2)\n",
    "        self.conv3_output = None\n",
    "        self.conv3_output_activated = None\n",
    "        \n",
    "        self.fc1_weights = np.random.randn(64,4096)\n",
    "        self.fc1_output = None\n",
    "        self.fc1_output_activated = None\n",
    "        \n",
    "        self.fc2_weights = np.random.randn(10,64)\n",
    "        self.fc2_output = None\n",
    "\n",
    "        self.softmax_output = None\n",
    "        \n",
    "        self.learning_rate = 0.001\n",
    "        self.batch_size = 32\n",
    "        self. epochs = 20\n",
    "        \n",
    "    def forward_prop(self):\n",
    "        self.conv1_output = convolution(self.X, self.conv1_kernel)\n",
    "        self.conv1_output_activated = relu(self.conv1_output)\n",
    "        \n",
    "        self.pool1_output = pooling(self.conv1_output_activated, 2)\n",
    "        \n",
    "        self.conv2_output = convolution(self.pool1_output, self.conv2_kernel)\n",
    "        self.conv2_output_activated = relu(self.conv2_output)\n",
    "        \n",
    "        self.pool2_output = pooling(self.conv2_output_activated, 2)\n",
    "        \n",
    "        self.conv3_output = convolution(self.pool2_output, self.conv3_kernel)\n",
    "        \n",
    "        self.conv3_output_activated = relu(self.conv3_output).flatten()\n",
    "        \n",
    "        self.fc1_output = self.fc1_weights @ self.conv3_output_activated\n",
    "        self.fc1_output_activated = relu(self.fc1_output)\n",
    "        \n",
    "        self.fc2_output = self.fc2_weights @ self.fc1_output_activated\n",
    "        print(self.fc2_output)\n",
    "        \n",
    "        # self.softmax_output = softmax(self.fc2_output)\n",
    "        \n",
    "    def back_prop(self):\n",
    "        one_hot = np.zeros(10)\n",
    "        onehot[self.Y] = 1\n",
    "        grad_fc2 = self.softmax_output - onehot\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "74a0382b-815f-4d17-877b-5d1223459e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = x_train[0].reshape(3, 32, 32)\n",
    "img1 = normalise(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8f6478a7-7381-4cc5-828f-cb37d6059f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cnn = CNN(img1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bbe08949-f27d-4412-bd48-a3fd4bf05929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-37890195.98513866    576591.17078231   7240714.24627781\n",
      "   7323917.72791389 -26779551.46781525  -7829546.49505506\n",
      "  21279698.09960651 -23957930.37243462   4103842.14269739\n",
      "   4833036.73849569]\n"
     ]
    }
   ],
   "source": [
    "my_cnn.forward_prop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "57cf9355-2024-4a03-95dc-4d1720f2a0aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0., nan, nan,  0., nan, nan, nan, nan,  0.,  0.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_cnn.softmax_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bc4cd2-1072-4bdf-bcbd-ab96c7a0a66a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
