{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b47422ed-0f4b-4c4e-b75e-2f4ce1751337",
   "metadata": {},
   "source": [
    "<center>\n",
    "    \n",
    "# COL341 Spring 2023 <br> Assignment 4 : CNN  \n",
    "## Part 1\n",
    "### Amaiya Singhal\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a596da8-027d-4479-95cb-006662857d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd265432-b870-4f46-80e2-e38f9120634f",
   "metadata": {},
   "source": [
    "## Defining some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd2a6121-4709-498a-9067-bcd297bb2085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(v):\n",
    "    exp_array = np.exp(v - np.max(v))\n",
    "    return exp_array / np.sum(exp_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e13dfe0-2ec6-4a9a-8a85-f9f48f6f54ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(map):    \n",
    "    output_tensor = np.maximum(map, 0)\n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82c7aef5-4095-4f08-8619-e58a44ca6a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooling(map, dim):\n",
    "    n_channels = map.shape[0]\n",
    "    map_size = map.shape[1]\n",
    "    output_size = map.shape[1]//dim\n",
    "    return map.reshape(n_channels, output_size, dim, output_size, dim).max(axis=(2,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b4017ce-ec8a-4825-96a4-273b591dca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution(sample, kernel):\n",
    "    kernel_size = kernel.shape[1]\n",
    "    pad = kernel_size//2\n",
    "    n, h, w = sample.shape\n",
    "    \n",
    "    out_sample = np.zeros((n, h+2*pad, w+2*pad))\n",
    "    for i in range(n):\n",
    "        out_sample[i] = np.pad(sample[i], (pad,), 'constant', constant_values = 0)\n",
    "    sample = out_sample\n",
    "    \n",
    "    size_feature_map = h\n",
    "    n_out_channels = kernel.shape[0]\n",
    "\n",
    "    output_tensor = np.zeros((n_out_channels, size_feature_map, size_feature_map))\n",
    "\n",
    "    for i in range(n_out_channels):\n",
    "        current_kernel = kernel[i]\n",
    "\n",
    "        for r in range(size_feature_map):\n",
    "            for c in range(size_feature_map):\n",
    "                window = sample[:, r : r + kernel_size, c : c + kernel_size]\n",
    "                value = np.sum(window*current_kernel, axis = None)\n",
    "                output_tensor[i, r, c] = value\n",
    "\n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5448a29e-bec6-46bf-9908-d02a34e94447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def other_convolution(sample, kernel, pad):\n",
    "    kernel_size = kernel.shape[1]\n",
    "    n, h, w = sample.shape\n",
    "    \n",
    "    out_sample = np.zeros((n, h+2*pad, w+2*pad))\n",
    "    for i in range(n):\n",
    "        out_sample[i] = np.pad(sample[i], (pad,), 'constant', constant_values = 0)\n",
    "    sample = out_sample\n",
    "    \n",
    "    size_feature_map = h + 2*pad - kernel_size + 1\n",
    "    n_out_channels = kernel.shape[0]\n",
    "\n",
    "    output_tensor = np.zeros((n_out_channels, size_feature_map, size_feature_map))\n",
    "\n",
    "    for i in range(n_out_channels):\n",
    "        current_kernel = kernel[i]\n",
    "\n",
    "        for r in range(size_feature_map):\n",
    "            for c in range(size_feature_map):\n",
    "                window = sample[:, r : r + kernel_size, c : c + kernel_size] \n",
    "                value = np.sum(window*current_kernel, axis = None)\n",
    "                output_tensor[i, r, c] = value\n",
    "\n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17752509-813b-40f1-a0cb-27b85d5f8c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve2d(sample, kernel):\n",
    "    size_feature_map = sample.shape[0]\n",
    "    kernel_size = kernel.shape[0]\n",
    "    pad = kernel_size//2\n",
    "    \n",
    "    sample = np.pad(sample, (pad,), 'constant', constant_values = 0)\n",
    "    \n",
    "    output_tensor = np.zeros((size_feature_map, size_feature_map))\n",
    "    for r in range(size_feature_map):\n",
    "        for c in range(size_feature_map):\n",
    "            window = sample[r : r + kernel_size, c : c + kernel_size]\n",
    "            value = np.sum(window*kernel, axis = None)\n",
    "            output_tensor[r, c] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22978e33-dd8e-41eb-9a0c-8e736abced05",
   "metadata": {},
   "source": [
    "## CONV2D Class for the Convolution Layers\n",
    "Forward and Backward Pass has been implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13f8f060-98b8-4275-a4f7-8feb6f6c015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv2d:\n",
    "    def __init__(self, num, size):\n",
    "        self.kernel = np.random.randn(num, size, size)/4096 # Xavier Initialisation\n",
    "        self.bias = np.random.rand(num, size, size)/4096\n",
    "        self.layer_input = None\n",
    "        self.layer_activated = None\n",
    "        self.kernel_grad = None\n",
    "        self.size = size\n",
    "    \n",
    "    def forward_pass(self, sample):\n",
    "        self.layer_input = sample\n",
    "        output_tensor = convolution(sample, self.kernel)\n",
    "        self.layer_activated = relu(output_tensor)\n",
    "        return self.layer_activated\n",
    "        \n",
    "    def backward_pass(self, inp_grad):\n",
    "        n, h, w = self.layer_input.shape\n",
    "        pass_grad = np.zeros((n,h,w))\n",
    "        relu_mat = self.layer_activated\n",
    "        relu_mat[np.nonzero(relu_mat)] = 1\n",
    "        inp_grad = inp_grad * relu_mat\n",
    "        \n",
    "        kernel_grad = other_convolution(self.layer_input, inp_grad, self.size//2)\n",
    "        # print(\"here: \", inp_grad.shape, self.layer_input.shape, kernel_grad.shape)\n",
    "        # check this kernel gradient once, need to perform full convolution\n",
    "        self.kernel_grad = kernel_grad\n",
    "        \n",
    "        not_final = np.zeros((inp_grad.shape[0], h, w))\n",
    "        for i in range(inp_grad.shape[0]):\n",
    "            curr_grad = inp_grad[i]\n",
    "            curr_kernel = np.flip(self.kernel[i], axis = (0,1))\n",
    "            not_final[i] = convolve2d(curr_grad, curr_kernel)\n",
    "        still_not_final = np.sum(pass_grad, axis = 0)\n",
    "        \n",
    "        for i in range(n):\n",
    "            pass_grad[i] = still_not_final\n",
    "        return pass_grad\n",
    "        \n",
    "        # flip_kernel = np.flip(kernel_grad, axis=(0,1))\n",
    "        # not_final = convolution(inp_grad, flip_kernel)\n",
    "        # still_not_final = np.sum(not_final, axis = 0)\n",
    "        \n",
    "    def update(self):\n",
    "        self.kernel -= 0.001*self.kernel_grad\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f95e8d3-a8ca-4881-9465-3844416932e2",
   "metadata": {},
   "source": [
    "## MAXPOOL2D Class for the Pooling Layers\n",
    "Forward and Backward Pass has been implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c90d2bd6-82bd-4a9b-8cf7-e24433d0e556",
   "metadata": {},
   "outputs": [],
   "source": [
    "class maxpool2d:\n",
    "    def __init__(self, dim):\n",
    "        self.dim = dim\n",
    "        self.layer_input = None\n",
    "        self.layer_output = None\n",
    "        \n",
    "    def forward_pass(self , sample):\n",
    "        self.layer_input = sample\n",
    "        self.layer_output = pooling(sample, 2)\n",
    "        return self.layer_output\n",
    "    \n",
    "    def backward_pass(self, inp_grad):\n",
    "        n, h, w = self.layer_input.shape\n",
    "        x = self.layer_input\n",
    "        dim = self.dim\n",
    "        \n",
    "        pass_mat = np.zeros((n,h,w))\n",
    "\n",
    "        for i in range(n):\n",
    "            for r in range(0, h-1, dim):\n",
    "                for c in range(0, w-1, dim):\n",
    "                    window = x[i, r:r+dim, c:c+dim]\n",
    "                    max_ind = np.unravel_index(window.argmax(), window.shape)\n",
    "                    pass_mat[i, r:r+dim, c:c+dim][max_ind] = inp_grad[i, r//2, c//2]\n",
    "            \n",
    "        return pass_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb725ef-ccff-4c40-a451-92294ed14f5c",
   "metadata": {},
   "source": [
    "## FC_1 Class for the First Fully Connected Layer\n",
    "Forward and Backward Pass has been implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f11c194-63c0-4de5-889d-94710e61f1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fc_1:\n",
    "    def __init__(self, size, next_size):\n",
    "        self.weights = np.random.randn(next_size, size)/4096\n",
    "        self.bias = np.random.randn(1, next_size)/4096\n",
    "        self.weights_grad = None\n",
    "        self.bias_grad = None\n",
    "        self.layer_input = None\n",
    "        self.layer_output = None\n",
    "        self.layer_output_active = None\n",
    "        \n",
    "    def forward_pass(self, sample):\n",
    "        self.layer_input = sample\n",
    "        output = sample @ self.weights.T + self.bias\n",
    "        self.layer_output = output\n",
    "        self.layer_output_active = relu(output)\n",
    "        return self.layer_output_active\n",
    "    \n",
    "    def backward_pass(self, inp_grad):\n",
    "        relu_mat = self.layer_output_active\n",
    "        relu_mat[np.nonzero(relu_mat)] = 1\n",
    "        relued_grad = inp_grad * relu_mat\n",
    "        pass_grad = relued_grad.reshape(1,-1) @ self.weights\n",
    "        self.weights_grad = relued_grad.reshape(-1,1) @ self.layer_input.reshape(1,-1)\n",
    "        self.bias_grad = relued_grad.reshape(1,-1)\n",
    "        return pass_grad\n",
    "    \n",
    "    def update(self):\n",
    "        self.weights -= 0.001*self.weights_grad\n",
    "        self.bias -= 0.001*self.bias_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5de94d9-98f2-48fd-938f-acb26851f66b",
   "metadata": {},
   "source": [
    "## FC_2 Class for the Second Fully Connected Layer\n",
    "Forward and Backward Pass has been implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12c47614-e2f5-4ff4-963c-a259cf130fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fc_2:\n",
    "    def __init__(self, size, next_size):\n",
    "        self.weights = np.random.randn(next_size, size)/64\n",
    "        self.bias = np.random.randn(1, next_size)/64\n",
    "        self.weights_grad = None\n",
    "        self.bias_grad = None\n",
    "        self.layer_input = None\n",
    "        self.layer_output = None\n",
    "        \n",
    "    def forward_pass(self, sample):\n",
    "        self.layer_input = sample\n",
    "        output = sample @ self.weights.T + self.bias\n",
    "        self.layer_output = output\n",
    "        return self.layer_output\n",
    "    \n",
    "    def backward_pass(self, inp_grad):\n",
    "        pass_grad = inp_grad.reshape(1,-1) @ self.weights\n",
    "        self.weights_grad = inp_grad.reshape(-1,1) @ self.layer_input.reshape(1,-1)\n",
    "        # print(inp_grad.shape)\n",
    "        self.bias_grad = inp_grad.reshape(1,-1)\n",
    "        return pass_grad\n",
    "    \n",
    "    def update(self):\n",
    "        self.weights -= 0.001*self.weights_grad\n",
    "        self.bias -= 0.001*self.bias_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c24d83d-ef04-475c-b9ca-f2edfbf14f87",
   "metadata": {},
   "source": [
    "## Loading the CIFAR-10 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c7ad364-0eea-4c61-a1ce-139db454ccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6638e951-9124-4430-a40a-ca082ddfadac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 3072), (50000,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set1 = unpickle(\"./content/data_batch_1\")\n",
    "set2 = unpickle(\"./content/data_batch_2\")\n",
    "set3 = unpickle(\"./content/data_batch_3\")\n",
    "set4 = unpickle(\"./content/data_batch_4\")\n",
    "set5 = unpickle(\"./content/data_batch_5\")\n",
    "x_train = np.vstack((set1[b'data'], set2[b'data'], set3[b'data'], set4[b'data'], set5[b'data']))\n",
    "y_train = np.hstack((np.array(set1[b'labels']), np.array(set2[b'labels']), np.array(set3[b'labels']), np.array(set4[b'labels']) ,np.array(set5[b'labels']) ))\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bb5da4c-255d-424a-b9bd-4b7d34face3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trial = []\n",
    "y_trial = y_train[0:1000]\n",
    "for i in range(1000):\n",
    "    x_trial.append(x_train[i].reshape(3,32,32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970963f3-0d64-403e-9ed2-e1d4ebce71fc",
   "metadata": {},
   "source": [
    "## Defining the Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d4ef364-c202-4b7d-9db5-646d4e283edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, y, epochs, batch_size):\n",
    "    conv1 = conv2d(32, 3)\n",
    "    pool1 = maxpool2d(2)\n",
    "    conv2 = conv2d(64, 5)\n",
    "    pool2 = maxpool2d(2)\n",
    "    conv3 = conv2d(64, 3)\n",
    "    fc1 = fc_1(4096, 64)\n",
    "    fc2 = fc_2(64, 10)\n",
    "    num_batch = 150\n",
    "    parameters = []\n",
    "    for i in range(epochs):\n",
    "        perm = np.random.permutation(x.shape[0])\n",
    "        x = x[perm]\n",
    "        y = y[perm]\n",
    "        print(\"EPOCH: \", i)\n",
    "        total_loss = 0\n",
    "        for j in range(num_batch):\n",
    "            update_fc2_w = []\n",
    "            update_fc1_w = []\n",
    "            update_fc2_b = []\n",
    "            update_fc1_b = []\n",
    "            update_conv3 = []\n",
    "            update_conv2 = []\n",
    "            update_conv1 = []\n",
    "            count = 0\n",
    "            loss = 0\n",
    "            for k in range(batch_size):\n",
    "                a1 = conv1.forward_pass(x[j*32 + k])\n",
    "                a2 = pool1.forward_pass(a1)\n",
    "                a3 = conv2.forward_pass(a2)\n",
    "                a4 = pool2.forward_pass(a3)\n",
    "                a5 = conv3.forward_pass(a4).flatten()\n",
    "                a6 = fc1.forward_pass(a5)\n",
    "                a7 = fc2.forward_pass(a6)\n",
    "                out = softmax(a7)\n",
    "                onehot = np.zeros(10)\n",
    "                onehot[y[j*32 + k]] = 1\n",
    "                grad_fc2 = out - onehot\n",
    "                grad_fc1 = fc2.backward_pass(grad_fc2.reshape(1,-1))\n",
    "                grad_conv3 = fc1.backward_pass(grad_fc1).reshape(64, 8, 8)\n",
    "                grad_pool2 = conv3.backward_pass(grad_conv3)\n",
    "                grad_conv2 = pool2.backward_pass(grad_pool2)\n",
    "                grad_pool1 = conv2.backward_pass(grad_conv2)\n",
    "                grad_conv1 = pool1.backward_pass(grad_pool1)\n",
    "                init_grad = conv1.backward_pass(grad_conv1)\n",
    "                update_fc2_w += [fc2.weights]\n",
    "                update_fc1_w += [fc1.weights]\n",
    "                update_fc2_b += [fc2.bias] \n",
    "                update_fc1_b += [fc1.bias]\n",
    "                update_conv3 += [conv3.kernel]\n",
    "                update_conv2 += [conv2.kernel]\n",
    "                update_conv1 += [conv1.kernel]\n",
    "                if out.argmax() == y[j*32 + k]:\n",
    "                    count += 1\n",
    "                loss -= np.log(np.max(out))\n",
    "            print(\"Batch: \", j, \", correct: \", count, \", loss: \", loss/32)\n",
    "            total_loss += loss/32\n",
    "            fc2.weights = np.sum(update_fc2_w, axis = 0)/32 \n",
    "            fc1.weights = np.sum(update_fc1_w, axis = 0)/32\n",
    "            fc2.bias = np.sum(update_fc2_b, axis = 0)/32\n",
    "            fc1.bias = np.sum(update_fc1_b, axis = 0)/32\n",
    "            conv3.kernel = np.sum(update_conv3, axis = 0)/32\n",
    "            conv2.kernel = np.sum(update_conv2, axis = 0)/32\n",
    "            conv1.kernel = np.sum(update_conv1, axis = 0)/32   \n",
    "            fc2.update()\n",
    "            fc1.update()\n",
    "            conv3.update()\n",
    "            conv2.update()\n",
    "            conv1.update()\n",
    "        parameters += [[fc2.weights, fc1.weights, fc2.bias, fc1.bias, conv3.kernel, conv2.kernel, conv1.kernel]]\n",
    "        print(\"Total Loss = \", total_loss/150)\n",
    "        print()\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4619a36c-a20c-4b27-a06d-26a37311171e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_me = []\n",
    "y_me = []\n",
    "for i in range(10):\n",
    "    for j in range(480):\n",
    "        x_me.append(x_train[5000*i+j].reshape(3,32,32))\n",
    "        y_me.append(y_train[5000*i+j])\n",
    "x_me = np.array(x_me)\n",
    "y_me = np.array(y_me)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b564a132-ef2c-48cf-989d-b8798143970b",
   "metadata": {},
   "source": [
    "## Training the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74f27dbe-80f2-4751-bb7c-9b9181c124ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:  0\n",
      "Batch:  0 , correct:  3 , loss:  2.2821732033037847\n",
      "Batch:  1 , correct:  1 , loss:  2.282276138681037\n",
      "Batch:  2 , correct:  4 , loss:  2.2823799926750468\n",
      "Batch:  3 , correct:  2 , loss:  2.2824824413988916\n",
      "Batch:  4 , correct:  5 , loss:  2.2825863539235938\n",
      "Batch:  5 , correct:  4 , loss:  2.282690347989605\n",
      "Batch:  6 , correct:  3 , loss:  2.282794854552202\n",
      "Batch:  7 , correct:  3 , loss:  2.282894971897414\n",
      "Batch:  8 , correct:  4 , loss:  2.2829989895167575\n",
      "Batch:  9 , correct:  1 , loss:  2.2830991785681376\n",
      "Batch:  10 , correct:  6 , loss:  2.283199437021148\n",
      "Batch:  11 , correct:  4 , loss:  2.283301823903812\n",
      "Batch:  12 , correct:  6 , loss:  2.2834018710430435\n",
      "Batch:  13 , correct:  5 , loss:  2.283503698684125\n",
      "Batch:  14 , correct:  2 , loss:  2.2836056337213777\n",
      "Batch:  15 , correct:  2 , loss:  2.283708794722919\n",
      "Batch:  16 , correct:  5 , loss:  2.283808857443812\n",
      "Batch:  17 , correct:  4 , loss:  2.2839090179104624\n",
      "Batch:  18 , correct:  2 , loss:  2.2840109746603603\n",
      "Batch:  19 , correct:  4 , loss:  2.2841144147482875\n",
      "Batch:  20 , correct:  2 , loss:  2.2842145968720406\n",
      "Batch:  21 , correct:  2 , loss:  2.284317754708956\n",
      "Batch:  22 , correct:  3 , loss:  2.2844179924432972\n",
      "Batch:  23 , correct:  4 , loss:  2.2835127923025422\n",
      "Batch:  24 , correct:  5 , loss:  2.283616340875274\n",
      "Batch:  25 , correct:  5 , loss:  2.283718239183312\n",
      "Batch:  26 , correct:  4 , loss:  2.2838202098632827\n",
      "Batch:  27 , correct:  3 , loss:  2.2839222595493074\n",
      "Batch:  28 , correct:  4 , loss:  2.284024218050692\n",
      "Batch:  29 , correct:  4 , loss:  2.284128018032906\n",
      "Batch:  30 , correct:  1 , loss:  2.2842298875567884\n",
      "Batch:  31 , correct:  1 , loss:  2.2843318764619163\n",
      "Batch:  32 , correct:  8 , loss:  2.2844352991214225\n",
      "Batch:  33 , correct:  3 , loss:  2.283530120396328\n",
      "Batch:  34 , correct:  3 , loss:  2.2826251379099016\n",
      "Batch:  35 , correct:  4 , loss:  2.282727326510096\n",
      "Batch:  36 , correct:  4 , loss:  2.2828294226931303\n",
      "Batch:  37 , correct:  4 , loss:  2.282929766813176\n",
      "Batch:  38 , correct:  4 , loss:  2.2830329971543364\n",
      "Batch:  39 , correct:  8 , loss:  2.283135220963549\n",
      "Batch:  40 , correct:  1 , loss:  2.282230309898691\n",
      "Batch:  41 , correct:  5 , loss:  2.28233076312414\n",
      "Batch:  42 , correct:  4 , loss:  2.2824341217687145\n",
      "Batch:  43 , correct:  3 , loss:  2.282536289418615\n",
      "Batch:  44 , correct:  4 , loss:  2.282638602743348\n",
      "Batch:  45 , correct:  6 , loss:  2.2827390695389895\n",
      "Batch:  46 , correct:  1 , loss:  2.2828431790160497\n",
      "Batch:  47 , correct:  3 , loss:  2.282945331009346\n",
      "Batch:  48 , correct:  2 , loss:  2.2830476627621255\n",
      "Batch:  49 , correct:  1 , loss:  2.283151465045113\n",
      "Batch:  50 , correct:  5 , loss:  2.2832553418695287\n",
      "Batch:  51 , correct:  4 , loss:  2.282350401085704\n",
      "Batch:  52 , correct:  5 , loss:  2.2824545889360444\n",
      "Batch:  53 , correct:  4 , loss:  2.282556734703229\n",
      "Batch:  54 , correct:  5 , loss:  2.282660037488029\n",
      "Batch:  55 , correct:  2 , loss:  2.2817552361855946\n",
      "Batch:  56 , correct:  4 , loss:  2.281858772740028\n",
      "Batch:  57 , correct:  3 , loss:  2.2819628002663728\n",
      "Batch:  58 , correct:  5 , loss:  2.282065259409906\n",
      "Batch:  59 , correct:  2 , loss:  2.2821688429662736\n",
      "Batch:  60 , correct:  4 , loss:  2.28227102948345\n",
      "Batch:  61 , correct:  3 , loss:  2.282374676565745\n",
      "Batch:  62 , correct:  3 , loss:  2.2824780178560644\n",
      "Batch:  63 , correct:  3 , loss:  2.2825804621914947\n",
      "Batch:  64 , correct:  4 , loss:  2.2826835485033308\n",
      "Batch:  65 , correct:  0 , loss:  2.2827872104745586\n",
      "Batch:  66 , correct:  3 , loss:  2.2828912983430367\n",
      "Batch:  67 , correct:  5 , loss:  2.2829944415120456\n",
      "Batch:  68 , correct:  4 , loss:  2.283098581981159\n",
      "Batch:  69 , correct:  5 , loss:  2.2832027957643293\n",
      "Batch:  70 , correct:  5 , loss:  2.2833048846107773\n",
      "Batch:  71 , correct:  2 , loss:  2.2834081401383863\n",
      "Batch:  72 , correct:  2 , loss:  2.283511966779793\n",
      "Batch:  73 , correct:  8 , loss:  2.2836121754718293\n",
      "Batch:  74 , correct:  4 , loss:  2.2837145024751595\n",
      "Batch:  75 , correct:  2 , loss:  2.2838175846703277\n",
      "Batch:  76 , correct:  1 , loss:  2.2829125387424343\n",
      "Batch:  77 , correct:  3 , loss:  2.2830128700643257\n",
      "Batch:  78 , correct:  3 , loss:  2.283113283926856\n",
      "Batch:  79 , correct:  3 , loss:  2.282208380164464\n",
      "Batch:  80 , correct:  2 , loss:  2.2823116646029806\n",
      "Batch:  81 , correct:  3 , loss:  2.2824136539186206\n",
      "Batch:  82 , correct:  2 , loss:  2.282514202955768\n",
      "Batch:  83 , correct:  2 , loss:  2.282616657887447\n",
      "Batch:  84 , correct:  2 , loss:  2.282720868878721\n",
      "Batch:  85 , correct:  5 , loss:  2.281816183068659\n",
      "Batch:  86 , correct:  1 , loss:  2.2819188365415597\n",
      "Batch:  87 , correct:  1 , loss:  2.2820222305310023\n",
      "Batch:  88 , correct:  2 , loss:  2.2821257183758736\n",
      "Batch:  89 , correct:  2 , loss:  2.282226595747158\n",
      "Batch:  90 , correct:  1 , loss:  2.2823271906748817\n",
      "Batch:  91 , correct:  2 , loss:  2.2824292154841634\n",
      "Batch:  92 , correct:  1 , loss:  2.2825329109417094\n",
      "Batch:  93 , correct:  3 , loss:  2.282637250407522\n",
      "Batch:  94 , correct:  5 , loss:  2.2827410085467292\n",
      "Batch:  95 , correct:  2 , loss:  2.2828416347413922\n",
      "Batch:  96 , correct:  2 , loss:  2.2829454004891\n",
      "Batch:  97 , correct:  3 , loss:  2.2830474588925704\n",
      "Batch:  98 , correct:  4 , loss:  2.2831513527737455\n",
      "Batch:  99 , correct:  3 , loss:  2.2832530651302196\n",
      "Batch:  100 , correct:  2 , loss:  2.283356305907983\n",
      "Batch:  101 , correct:  5 , loss:  2.283458390721935\n",
      "Batch:  102 , correct:  0 , loss:  2.283561689477405\n",
      "Batch:  103 , correct:  3 , loss:  2.283664774299743\n",
      "Batch:  104 , correct:  4 , loss:  2.2837665608802085\n",
      "Batch:  105 , correct:  2 , loss:  2.283868408186423\n",
      "Batch:  106 , correct:  3 , loss:  2.2839704847717766\n",
      "Batch:  107 , correct:  1 , loss:  2.2840721310725436\n",
      "Batch:  108 , correct:  1 , loss:  2.2831677142237674\n",
      "Batch:  109 , correct:  2 , loss:  2.283268391776309\n",
      "Batch:  110 , correct:  3 , loss:  2.283371731867819\n",
      "Batch:  111 , correct:  3 , loss:  2.283473503772821\n",
      "Batch:  112 , correct:  2 , loss:  2.283576622828849\n",
      "Batch:  113 , correct:  4 , loss:  2.283680000466816\n",
      "Batch:  114 , correct:  6 , loss:  2.283784144842372\n",
      "Batch:  115 , correct:  3 , loss:  2.2838862155905257\n",
      "Batch:  116 , correct:  5 , loss:  2.2839896384578746\n",
      "Batch:  117 , correct:  4 , loss:  2.284091394433705\n",
      "Batch:  118 , correct:  7 , loss:  2.2831869594805156\n",
      "Batch:  119 , correct:  2 , loss:  2.2832888663923794\n",
      "Batch:  120 , correct:  4 , loss:  2.2833926150882555\n",
      "Batch:  121 , correct:  1 , loss:  2.2834955682124067\n",
      "Batch:  122 , correct:  6 , loss:  2.2835997308823672\n",
      "Batch:  123 , correct:  8 , loss:  2.2837016632730265\n",
      "Batch:  124 , correct:  5 , loss:  2.283804731353531\n",
      "Batch:  125 , correct:  4 , loss:  2.2839076982569018\n",
      "Batch:  126 , correct:  2 , loss:  2.284011421646781\n",
      "Batch:  127 , correct:  5 , loss:  2.2841134796706952\n",
      "Batch:  128 , correct:  4 , loss:  2.2842165571471904\n",
      "Batch:  129 , correct:  5 , loss:  2.2843203182437204\n",
      "Batch:  130 , correct:  1 , loss:  2.284423464978967\n",
      "Batch:  131 , correct:  5 , loss:  2.2835189525604465\n",
      "Batch:  132 , correct:  2 , loss:  2.2836222694953667\n",
      "Batch:  133 , correct:  4 , loss:  2.2837256577954776\n",
      "Batch:  134 , correct:  4 , loss:  2.28382901458041\n",
      "Batch:  135 , correct:  2 , loss:  2.2839309044778133\n",
      "Batch:  136 , correct:  2 , loss:  2.2840338432412377\n",
      "Batch:  137 , correct:  3 , loss:  2.284137242456495\n",
      "Batch:  138 , correct:  0 , loss:  2.2842402455897464\n",
      "Batch:  139 , correct:  5 , loss:  2.284344256952571\n",
      "Batch:  140 , correct:  3 , loss:  2.2844483493003533\n",
      "Batch:  141 , correct:  2 , loss:  2.2845525272876483\n",
      "Batch:  142 , correct:  6 , loss:  2.2846558420634264\n",
      "Batch:  143 , correct:  4 , loss:  2.284759509157948\n",
      "Batch:  144 , correct:  3 , loss:  2.2848628694312785\n",
      "Batch:  145 , correct:  6 , loss:  2.2849665941457338\n",
      "Batch:  146 , correct:  3 , loss:  2.2850668368897757\n",
      "Batch:  147 , correct:  2 , loss:  2.285170980471913\n",
      "Batch:  148 , correct:  5 , loss:  2.285275211070124\n",
      "Batch:  149 , correct:  5 , loss:  2.284370543199342\n",
      "Total Loss =  2.2833065306832396\n",
      "\n",
      "EPOCH:  1\n",
      "Batch:  0 , correct:  2 , loss:  2.284473978509362\n",
      "Batch:  1 , correct:  4 , loss:  2.284577289391748\n",
      "Batch:  2 , correct:  2 , loss:  2.2846791274511875\n",
      "Batch:  3 , correct:  0 , loss:  2.284782594326124\n",
      "Batch:  4 , correct:  1 , loss:  2.2848863604874947\n",
      "Batch:  5 , correct:  2 , loss:  2.2849892049885145\n",
      "Batch:  6 , correct:  4 , loss:  2.285092516516\n",
      "Batch:  7 , correct:  3 , loss:  2.285195438838137\n",
      "Batch:  8 , correct:  3 , loss:  2.2852988068314906\n",
      "Batch:  9 , correct:  2 , loss:  2.285400391881988\n",
      "Batch:  10 , correct:  2 , loss:  2.2855021685358494\n",
      "Batch:  11 , correct:  4 , loss:  2.284597441441468\n",
      "Batch:  12 , correct:  3 , loss:  2.284700954103086\n",
      "Batch:  13 , correct:  1 , loss:  2.2848022030174353\n",
      "Batch:  14 , correct:  8 , loss:  2.2849035526629704\n",
      "Batch:  15 , correct:  3 , loss:  2.2839989590430494\n",
      "Batch:  16 , correct:  4 , loss:  2.284101994107595\n",
      "Batch:  17 , correct:  3 , loss:  2.2842055908992385\n",
      "Batch:  18 , correct:  4 , loss:  2.284305781741834\n",
      "Batch:  19 , correct:  7 , loss:  2.2844099865150924\n",
      "Batch:  20 , correct:  1 , loss:  2.2845136243312294\n",
      "Batch:  21 , correct:  3 , loss:  2.2846152669224025\n",
      "Batch:  22 , correct:  2 , loss:  2.2847195234620674\n",
      "Batch:  23 , correct:  4 , loss:  2.283814936442262\n",
      "Batch:  24 , correct:  3 , loss:  2.2839187136235513\n",
      "Batch:  25 , correct:  6 , loss:  2.2833550056474197\n",
      "Batch:  26 , correct:  4 , loss:  2.283457486883762\n",
      "Batch:  27 , correct:  3 , loss:  2.283220039504408\n",
      "Batch:  28 , correct:  4 , loss:  2.2833228206698375\n",
      "Batch:  29 , correct:  7 , loss:  2.2827621633149224\n",
      "Batch:  30 , correct:  0 , loss:  2.2828625744973117\n",
      "Batch:  31 , correct:  4 , loss:  2.2829642358768703\n",
      "Batch:  32 , correct:  5 , loss:  2.2820654482805494\n",
      "Batch:  33 , correct:  4 , loss:  2.2821698768724796\n",
      "Batch:  34 , correct:  0 , loss:  2.2822704157781137\n",
      "Batch:  35 , correct:  3 , loss:  2.2823724943313444\n",
      "Batch:  36 , correct:  3 , loss:  2.2824748798039356\n",
      "Batch:  37 , correct:  6 , loss:  2.282574471878756\n",
      "Batch:  38 , correct:  6 , loss:  2.2816757677221378\n",
      "Batch:  39 , correct:  2 , loss:  2.281778360408355\n",
      "Batch:  40 , correct:  5 , loss:  2.281878959291637\n",
      "Batch:  41 , correct:  5 , loss:  2.281981146789823\n",
      "Batch:  42 , correct:  2 , loss:  2.282082831629965\n",
      "Batch:  43 , correct:  0 , loss:  2.2821834649036576\n",
      "Batch:  44 , correct:  2 , loss:  2.28228589747027\n",
      "Batch:  45 , correct:  0 , loss:  2.2823884084945663\n",
      "Batch:  46 , correct:  3 , loss:  2.2824890747930993\n",
      "Batch:  47 , correct:  4 , loss:  2.282589819187897\n",
      "Batch:  48 , correct:  3 , loss:  2.2826923738570324\n",
      "Batch:  49 , correct:  4 , loss:  2.281793653395463\n",
      "Batch:  50 , correct:  4 , loss:  2.281896019655097\n",
      "Batch:  51 , correct:  3 , loss:  2.2819977326215675\n",
      "Batch:  52 , correct:  4 , loss:  2.2821021794208503\n",
      "Batch:  53 , correct:  3 , loss:  2.282204327099833\n",
      "Batch:  54 , correct:  1 , loss:  2.28230671512769\n",
      "Batch:  55 , correct:  4 , loss:  2.2824089322424395\n",
      "Batch:  56 , correct:  3 , loss:  2.281510256183023\n",
      "Batch:  57 , correct:  3 , loss:  2.2816135174093253\n",
      "Batch:  58 , correct:  3 , loss:  2.281714342065111\n",
      "Batch:  59 , correct:  3 , loss:  2.2818176638899783\n",
      "Batch:  60 , correct:  3 , loss:  2.2819221502896734\n",
      "Batch:  61 , correct:  3 , loss:  2.2820245936734955\n",
      "Batch:  62 , correct:  2 , loss:  2.2811260067202066\n",
      "Batch:  63 , correct:  6 , loss:  2.2812306363174706\n",
      "Batch:  64 , correct:  2 , loss:  2.2813332198156364\n",
      "Batch:  65 , correct:  2 , loss:  2.281434108764122\n",
      "Batch:  66 , correct:  4 , loss:  2.2805356319642383\n",
      "Batch:  67 , correct:  2 , loss:  2.2796373528345604\n",
      "Batch:  68 , correct:  6 , loss:  2.2797399614192515\n",
      "Batch:  69 , correct:  3 , loss:  2.2788418360541245\n",
      "Batch:  70 , correct:  2 , loss:  2.2789446083967912\n",
      "Batch:  71 , correct:  4 , loss:  2.2790465201621553\n",
      "Batch:  72 , correct:  4 , loss:  2.279149293184412\n",
      "Batch:  73 , correct:  1 , loss:  2.2782512906658683\n",
      "Batch:  74 , correct:  4 , loss:  2.278353817428074\n",
      "Batch:  75 , correct:  1 , loss:  2.2784574168340255\n",
      "Batch:  76 , correct:  4 , loss:  2.278557128832349\n",
      "Batch:  77 , correct:  2 , loss:  2.278660007403466\n",
      "Batch:  78 , correct:  4 , loss:  2.278763642394739\n",
      "Batch:  79 , correct:  4 , loss:  2.2788634023290606\n",
      "Batch:  80 , correct:  2 , loss:  2.2789661725801267\n",
      "Batch:  81 , correct:  3 , loss:  2.2790659939123024\n",
      "Batch:  82 , correct:  3 , loss:  2.2791696544369273\n",
      "Batch:  83 , correct:  1 , loss:  2.2792706839065806\n",
      "Batch:  84 , correct:  2 , loss:  2.279371790389694\n",
      "Batch:  85 , correct:  6 , loss:  2.2794744640152205\n",
      "Batch:  86 , correct:  2 , loss:  2.2795742809605284\n",
      "Batch:  87 , correct:  4 , loss:  2.279676645294106\n",
      "Batch:  88 , correct:  1 , loss:  2.2797794043146826\n",
      "Batch:  89 , correct:  2 , loss:  2.2798820987442276\n",
      "Batch:  90 , correct:  3 , loss:  2.279984813165835\n",
      "Batch:  91 , correct:  3 , loss:  2.2800844737477757\n",
      "Batch:  92 , correct:  3 , loss:  2.280187250540129\n",
      "Batch:  93 , correct:  1 , loss:  2.2802901070104733\n",
      "Batch:  94 , correct:  2 , loss:  2.2803919208936216\n",
      "Batch:  95 , correct:  2 , loss:  2.2804944104602036\n",
      "Batch:  96 , correct:  0 , loss:  2.2805940652976275\n",
      "Batch:  97 , correct:  4 , loss:  2.280696591599209\n",
      "Batch:  98 , correct:  7 , loss:  2.2807999214280517\n",
      "Batch:  99 , correct:  5 , loss:  2.2799016363300213\n",
      "Batch:  100 , correct:  2 , loss:  2.2800051383186806\n",
      "Batch:  101 , correct:  2 , loss:  2.280107015723156\n",
      "Batch:  102 , correct:  2 , loss:  2.280209631541488\n",
      "Batch:  103 , correct:  7 , loss:  2.2803115658228363\n",
      "Batch:  104 , correct:  5 , loss:  2.2804142434060264\n",
      "Batch:  105 , correct:  2 , loss:  2.2805150811747223\n",
      "Batch:  106 , correct:  2 , loss:  2.2806178145575458\n",
      "Batch:  107 , correct:  1 , loss:  2.2807206254771764\n",
      "Batch:  108 , correct:  5 , loss:  2.28082256878746\n",
      "Batch:  109 , correct:  1 , loss:  2.2809268567839607\n",
      "Batch:  110 , correct:  3 , loss:  2.2810312240427493\n",
      "Batch:  111 , correct:  3 , loss:  2.281130781424658\n",
      "Batch:  112 , correct:  1 , loss:  2.281231569440635\n",
      "Batch:  113 , correct:  1 , loss:  2.2813342277896917\n",
      "Batch:  114 , correct:  2 , loss:  2.2814369554845007\n",
      "Batch:  115 , correct:  2 , loss:  2.281539315304514\n",
      "Batch:  116 , correct:  4 , loss:  2.2816401299676774\n",
      "Batch:  117 , correct:  3 , loss:  2.2817429005946304\n",
      "Batch:  118 , correct:  2 , loss:  2.281842412500654\n",
      "Batch:  119 , correct:  5 , loss:  2.2819447979341954\n",
      "Batch:  120 , correct:  5 , loss:  2.2820456313615525\n",
      "Batch:  121 , correct:  5 , loss:  2.2821474019216637\n",
      "Batch:  122 , correct:  3 , loss:  2.282248290488229\n",
      "Batch:  123 , correct:  0 , loss:  2.2823510443053623\n",
      "Batch:  124 , correct:  0 , loss:  2.2824538680721367\n",
      "Batch:  125 , correct:  1 , loss:  2.282554794717119\n",
      "Batch:  126 , correct:  6 , loss:  2.282654242913854\n",
      "Batch:  127 , correct:  3 , loss:  2.2827562848333383\n",
      "Batch:  128 , correct:  2 , loss:  2.2828591366613376\n",
      "Batch:  129 , correct:  3 , loss:  2.282961607639099\n",
      "Batch:  130 , correct:  2 , loss:  2.2830645832348884\n",
      "Batch:  131 , correct:  4 , loss:  2.2831668291590135\n",
      "Batch:  132 , correct:  1 , loss:  2.283270848737546\n",
      "Batch:  133 , correct:  6 , loss:  2.2833736904092214\n",
      "Batch:  134 , correct:  4 , loss:  2.283476696656479\n",
      "Batch:  135 , correct:  2 , loss:  2.2835760592916943\n",
      "Batch:  136 , correct:  5 , loss:  2.2836776249422024\n",
      "Batch:  137 , correct:  2 , loss:  2.2837800424418884\n",
      "Batch:  138 , correct:  3 , loss:  2.28388404107682\n",
      "Batch:  139 , correct:  3 , loss:  2.2839865014265994\n",
      "Batch:  140 , correct:  3 , loss:  2.284090564967935\n",
      "Batch:  141 , correct:  3 , loss:  2.2841947066525314\n",
      "Batch:  142 , correct:  1 , loss:  2.284296814821043\n",
      "Batch:  143 , correct:  5 , loss:  2.284397480795278\n",
      "Batch:  144 , correct:  4 , loss:  2.283498457102085\n",
      "Batch:  145 , correct:  0 , loss:  2.2836000414076705\n",
      "Batch:  146 , correct:  7 , loss:  2.2837028187949593\n",
      "Batch:  147 , correct:  1 , loss:  2.2838036024947828\n",
      "Batch:  148 , correct:  5 , loss:  2.2839064448518402\n",
      "Batch:  149 , correct:  8 , loss:  2.28400728204918\n",
      "Total Loss =  2.2820639403859\n",
      "\n",
      "EPOCH:  2\n",
      "Batch:  0 , correct:  2 , loss:  2.284109441258103\n",
      "Batch:  1 , correct:  4 , loss:  2.2842116806344337\n",
      "Batch:  2 , correct:  4 , loss:  2.284313992773311\n",
      "Batch:  3 , correct:  3 , loss:  2.2834149840043425\n",
      "Batch:  4 , correct:  4 , loss:  2.2835174680563477\n",
      "Batch:  5 , correct:  3 , loss:  2.2836190455543868\n",
      "Batch:  6 , correct:  7 , loss:  2.2837182929089437\n",
      "Batch:  7 , correct:  2 , loss:  2.2838208154614814\n",
      "Batch:  8 , correct:  3 , loss:  2.2839236472795315\n",
      "Batch:  9 , correct:  3 , loss:  2.2840262198343044\n",
      "Batch:  10 , correct:  6 , loss:  2.2841254957603394\n",
      "Batch:  11 , correct:  5 , loss:  2.2832265158444485\n",
      "Batch:  12 , correct:  7 , loss:  2.2833293520066515\n",
      "Batch:  13 , correct:  3 , loss:  2.283430218462658\n",
      "Batch:  14 , correct:  2 , loss:  2.283533132524618\n",
      "Batch:  15 , correct:  0 , loss:  2.28363480036\n",
      "Batch:  16 , correct:  2 , loss:  2.283737435914013\n",
      "Batch:  17 , correct:  4 , loss:  2.2838383219452014\n",
      "Batch:  18 , correct:  6 , loss:  2.2839412574979763\n",
      "Batch:  19 , correct:  0 , loss:  2.283042329959034\n",
      "Batch:  20 , correct:  1 , loss:  2.2831454251296543\n",
      "Batch:  21 , correct:  4 , loss:  2.283248269063605\n",
      "Batch:  22 , correct:  2 , loss:  2.2833514285460574\n",
      "Batch:  23 , correct:  7 , loss:  2.283454330585916\n",
      "Batch:  24 , correct:  2 , loss:  2.283555278032151\n",
      "Batch:  25 , correct:  5 , loss:  2.2836579324611983\n",
      "Batch:  26 , correct:  6 , loss:  2.2837595776431217\n",
      "Batch:  27 , correct:  0 , loss:  2.2838634826075324\n",
      "Batch:  28 , correct:  2 , loss:  2.2839648889618975\n",
      "Batch:  29 , correct:  0 , loss:  2.2840670557325367\n",
      "Batch:  30 , correct:  3 , loss:  2.2841697100555334\n",
      "Batch:  31 , correct:  2 , loss:  2.2842727856509644\n",
      "Batch:  32 , correct:  2 , loss:  2.2843744052378745\n",
      "Batch:  33 , correct:  3 , loss:  2.283475385624536\n",
      "Batch:  34 , correct:  4 , loss:  2.2835782640888644\n",
      "Batch:  35 , correct:  2 , loss:  2.2836774280045113\n",
      "Batch:  36 , correct:  4 , loss:  2.2837788891904114\n",
      "Batch:  37 , correct:  5 , loss:  2.2838804190224304\n",
      "Batch:  38 , correct:  1 , loss:  2.283982037930785\n",
      "Batch:  39 , correct:  6 , loss:  2.2840828726111275\n",
      "Batch:  40 , correct:  1 , loss:  2.283183909980588\n",
      "Batch:  41 , correct:  3 , loss:  2.283287831568198\n",
      "Batch:  42 , correct:  4 , loss:  2.282389030796264\n",
      "Batch:  43 , correct:  2 , loss:  2.282492037547842\n",
      "Batch:  44 , correct:  6 , loss:  2.28259480203519\n",
      "Batch:  45 , correct:  2 , loss:  2.281696151036167\n",
      "Batch:  46 , correct:  4 , loss:  2.2818002696779343\n",
      "Batch:  47 , correct:  2 , loss:  2.281899594126117\n",
      "Batch:  48 , correct:  3 , loss:  2.2820013968494512\n",
      "Batch:  49 , correct:  3 , loss:  2.2821007879598056\n",
      "Batch:  50 , correct:  3 , loss:  2.282203851127856\n",
      "Batch:  51 , correct:  5 , loss:  2.2823079708443474\n",
      "Batch:  52 , correct:  4 , loss:  2.2824110883231645\n",
      "Batch:  53 , correct:  2 , loss:  2.281512464036806\n",
      "Batch:  54 , correct:  2 , loss:  2.2816157490235462\n",
      "Batch:  55 , correct:  3 , loss:  2.2817191089436344\n",
      "Batch:  56 , correct:  4 , loss:  2.281821341610863\n",
      "Batch:  57 , correct:  4 , loss:  2.281924484179978\n",
      "Batch:  58 , correct:  2 , loss:  2.2820254510186504\n",
      "Batch:  59 , correct:  6 , loss:  2.2821264915721273\n",
      "Batch:  60 , correct:  3 , loss:  2.2822258579107784\n",
      "Batch:  61 , correct:  2 , loss:  2.2823290271248813\n",
      "Batch:  62 , correct:  0 , loss:  2.281430421751762\n",
      "Batch:  63 , correct:  3 , loss:  2.2815299320763134\n",
      "Batch:  64 , correct:  5 , loss:  2.2806314905911873\n",
      "Batch:  65 , correct:  3 , loss:  2.280734343250367\n",
      "Batch:  66 , correct:  3 , loss:  2.2798360660963612\n",
      "Batch:  67 , correct:  2 , loss:  2.2799395952957435\n",
      "Batch:  68 , correct:  1 , loss:  2.279041542417544\n",
      "Batch:  69 , correct:  3 , loss:  2.279143483729347\n",
      "Batch:  70 , correct:  5 , loss:  2.279245439804565\n",
      "Batch:  71 , correct:  5 , loss:  2.2783475216148963\n",
      "Batch:  72 , correct:  4 , loss:  2.2784519670246497\n",
      "Batch:  73 , correct:  4 , loss:  2.2785540662662336\n",
      "Batch:  74 , correct:  2 , loss:  2.2786571418965202\n",
      "Batch:  75 , correct:  3 , loss:  2.278759292259078\n",
      "Batch:  76 , correct:  3 , loss:  2.2778614765555627\n",
      "Batch:  77 , correct:  6 , loss:  2.277963793843489\n",
      "Batch:  78 , correct:  5 , loss:  2.2770661428165457\n",
      "Batch:  79 , correct:  1 , loss:  2.2771698635787883\n",
      "Batch:  80 , correct:  1 , loss:  2.277273669753982\n",
      "Batch:  81 , correct:  1 , loss:  2.277377416810471\n",
      "Batch:  82 , correct:  1 , loss:  2.277480629568721\n",
      "Batch:  83 , correct:  4 , loss:  2.276583075289011\n",
      "Batch:  84 , correct:  2 , loss:  2.276683032774546\n",
      "Batch:  85 , correct:  2 , loss:  2.2767876034597108\n",
      "Batch:  86 , correct:  3 , loss:  2.2768922645633785\n",
      "Batch:  87 , correct:  2 , loss:  2.27699468229038\n",
      "Batch:  88 , correct:  4 , loss:  2.27709673293339\n",
      "Batch:  89 , correct:  1 , loss:  2.2761992633282513\n",
      "Batch:  90 , correct:  4 , loss:  2.276301470019957\n",
      "Batch:  91 , correct:  0 , loss:  2.276406241779089\n",
      "Batch:  92 , correct:  2 , loss:  2.276511081431279\n",
      "Batch:  93 , correct:  5 , loss:  2.2766135732227992\n",
      "Batch:  94 , correct:  4 , loss:  2.2767148803335533\n",
      "Batch:  95 , correct:  2 , loss:  2.2768186746881938\n",
      "Batch:  96 , correct:  1 , loss:  2.276918588021174\n",
      "Batch:  97 , correct:  5 , loss:  2.277022303960971\n",
      "Batch:  98 , correct:  3 , loss:  2.2771222731361\n",
      "Batch:  99 , correct:  1 , loss:  2.2772235806533874\n",
      "Batch:  100 , correct:  5 , loss:  2.277326726281493\n",
      "Batch:  101 , correct:  5 , loss:  2.277426731605595\n",
      "Batch:  102 , correct:  5 , loss:  2.2775280772827755\n",
      "Batch:  103 , correct:  3 , loss:  2.2776304642975793\n",
      "Batch:  104 , correct:  3 , loss:  2.2777336343009926\n",
      "Batch:  105 , correct:  4 , loss:  2.2778356080384063\n",
      "Batch:  106 , correct:  1 , loss:  2.277935614037448\n",
      "Batch:  107 , correct:  3 , loss:  2.2780380223655774\n",
      "Batch:  108 , correct:  2 , loss:  2.2781426295044507\n",
      "Batch:  109 , correct:  5 , loss:  2.2782462471612805\n",
      "Batch:  110 , correct:  5 , loss:  2.2773485332904895\n",
      "Batch:  111 , correct:  3 , loss:  2.2774486420555133\n",
      "Batch:  112 , correct:  2 , loss:  2.2775511510405964\n",
      "Batch:  113 , correct:  1 , loss:  2.2776543383909664\n",
      "Batch:  114 , correct:  4 , loss:  2.2777569069957844\n",
      "Batch:  115 , correct:  2 , loss:  2.276859302171028\n",
      "Batch:  116 , correct:  2 , loss:  2.276962937119381\n",
      "Batch:  117 , correct:  2 , loss:  2.2770631260412397\n",
      "Batch:  118 , correct:  3 , loss:  2.2771651373532946\n",
      "Batch:  119 , correct:  1 , loss:  2.2772678124162415\n",
      "Batch:  120 , correct:  5 , loss:  2.2763703017038175\n",
      "Batch:  121 , correct:  2 , loss:  2.2764722848653216\n",
      "Batch:  122 , correct:  4 , loss:  2.2765770137852983\n",
      "Batch:  123 , correct:  1 , loss:  2.2766772844261385\n",
      "Batch:  124 , correct:  5 , loss:  2.2767809630962708\n",
      "Batch:  125 , correct:  5 , loss:  2.276881289260843\n",
      "Batch:  126 , correct:  2 , loss:  2.2769840235309946\n",
      "Batch:  127 , correct:  4 , loss:  2.2770860576399095\n",
      "Batch:  128 , correct:  3 , loss:  2.277188007758487\n",
      "Batch:  129 , correct:  4 , loss:  2.2772900958344864\n",
      "Batch:  130 , correct:  4 , loss:  2.2773912797127758\n",
      "Batch:  131 , correct:  6 , loss:  2.2774940123760405\n",
      "Batch:  132 , correct:  4 , loss:  2.2765964540625174\n",
      "Batch:  133 , correct:  5 , loss:  2.2766996380448417\n",
      "Batch:  134 , correct:  5 , loss:  2.276801666086254\n",
      "Batch:  135 , correct:  5 , loss:  2.2769049073516214\n",
      "Batch:  136 , correct:  4 , loss:  2.2770095510340753\n",
      "Batch:  137 , correct:  4 , loss:  2.2771123600169174\n",
      "Batch:  138 , correct:  3 , loss:  2.2772159537365715\n",
      "Batch:  139 , correct:  3 , loss:  2.2773196235699804\n",
      "Batch:  140 , correct:  3 , loss:  2.2774233770072883\n",
      "Batch:  141 , correct:  4 , loss:  2.27752454030482\n",
      "Batch:  142 , correct:  0 , loss:  2.277626574728751\n",
      "Batch:  143 , correct:  4 , loss:  2.2777267416902323\n",
      "Batch:  144 , correct:  3 , loss:  2.2778305115694546\n",
      "Batch:  145 , correct:  0 , loss:  2.2779343630350817\n",
      "Batch:  146 , correct:  3 , loss:  2.278036239188567\n",
      "Batch:  147 , correct:  4 , loss:  2.2781401542954614\n",
      "Batch:  148 , correct:  1 , loss:  2.2782420972098882\n",
      "Batch:  149 , correct:  1 , loss:  2.2783431920193298\n",
      "Total Loss =  2.2799167322072957\n",
      "\n",
      "EPOCH:  3\n",
      "Batch:  0 , correct:  3 , loss:  2.278445840342943\n",
      "Batch:  1 , correct:  1 , loss:  2.2785497697704846\n",
      "Batch:  2 , correct:  5 , loss:  2.2786524726034525\n",
      "Batch:  3 , correct:  4 , loss:  2.278755444222707\n",
      "Batch:  4 , correct:  4 , loss:  2.278858211936856\n",
      "Batch:  5 , correct:  8 , loss:  2.2789600762112867\n",
      "Batch:  6 , correct:  2 , loss:  2.279061134559227\n",
      "Batch:  7 , correct:  1 , loss:  2.27916305706887\n",
      "Batch:  8 , correct:  2 , loss:  2.2792630234952527\n",
      "Batch:  9 , correct:  2 , loss:  2.279366890468329\n",
      "Batch:  10 , correct:  3 , loss:  2.279469827682328\n",
      "Batch:  11 , correct:  2 , loss:  2.2795728378420157\n",
      "Batch:  12 , correct:  5 , loss:  2.279675538917242\n",
      "Batch:  13 , correct:  0 , loss:  2.2797772758137023\n",
      "Batch:  14 , correct:  4 , loss:  2.279881438197998\n",
      "Batch:  15 , correct:  3 , loss:  2.27998135068941\n",
      "Batch:  16 , correct:  3 , loss:  2.27908328608434\n",
      "Batch:  17 , correct:  2 , loss:  2.279185137817013\n",
      "Batch:  18 , correct:  5 , loss:  2.2792882066882925\n",
      "Batch:  19 , correct:  3 , loss:  2.279390123934714\n",
      "Batch:  20 , correct:  4 , loss:  2.2794943687691522\n",
      "Batch:  21 , correct:  2 , loss:  2.2785963932000572\n",
      "Batch:  22 , correct:  1 , loss:  2.2786974341971864\n",
      "Batch:  23 , correct:  3 , loss:  2.278800427582205\n",
      "Batch:  24 , correct:  2 , loss:  2.278904794553775\n",
      "Batch:  25 , correct:  5 , loss:  2.2790048082667327\n",
      "Batch:  26 , correct:  2 , loss:  2.2791067860825756\n",
      "Batch:  27 , correct:  4 , loss:  2.279209801849741\n",
      "Batch:  28 , correct:  6 , loss:  2.2793108186095523\n",
      "Batch:  29 , correct:  3 , loss:  2.2794134765730085\n",
      "Batch:  30 , correct:  1 , loss:  2.2795154698161464\n",
      "Batch:  31 , correct:  3 , loss:  2.279617215309591\n",
      "Batch:  32 , correct:  2 , loss:  2.2797192660857353\n",
      "Batch:  33 , correct:  3 , loss:  2.279820279598584\n",
      "Batch:  34 , correct:  2 , loss:  2.2799220608152977\n",
      "Batch:  35 , correct:  3 , loss:  2.28002198541461\n",
      "Batch:  36 , correct:  4 , loss:  2.279123908904078\n",
      "Batch:  37 , correct:  4 , loss:  2.2792250433986148\n",
      "Batch:  38 , correct:  3 , loss:  2.279329320959811\n",
      "Batch:  39 , correct:  3 , loss:  2.27943143543137\n",
      "Batch:  40 , correct:  2 , loss:  2.2785334831349457\n",
      "Batch:  41 , correct:  4 , loss:  2.278634736696444\n",
      "Batch:  42 , correct:  5 , loss:  2.2787369935906234\n",
      "Batch:  43 , correct:  0 , loss:  2.2788399904307806\n",
      "Batch:  44 , correct:  4 , loss:  2.2789443459335637\n",
      "Batch:  45 , correct:  4 , loss:  2.279046642793395\n",
      "Batch:  46 , correct:  2 , loss:  2.279149015919743\n",
      "Batch:  47 , correct:  3 , loss:  2.2792520310469753\n",
      "Batch:  48 , correct:  3 , loss:  2.2793556016371896\n",
      "Batch:  49 , correct:  4 , loss:  2.2794584464219683\n",
      "Batch:  50 , correct:  2 , loss:  2.2795596182285354\n",
      "Batch:  51 , correct:  2 , loss:  2.2796626491694942\n",
      "Batch:  52 , correct:  4 , loss:  2.2797669419190543\n",
      "Batch:  53 , correct:  6 , loss:  2.279869275811974\n",
      "Batch:  54 , correct:  3 , loss:  2.279971723667351\n",
      "Batch:  55 , correct:  1 , loss:  2.279073652807552\n",
      "Batch:  56 , correct:  0 , loss:  2.279176793460406\n",
      "Batch:  57 , correct:  4 , loss:  2.2792792454360096\n",
      "Batch:  58 , correct:  2 , loss:  2.279382812377829\n",
      "Batch:  59 , correct:  1 , loss:  2.279485981348415\n",
      "Batch:  60 , correct:  2 , loss:  2.279589607896263\n",
      "Batch:  61 , correct:  3 , loss:  2.279692845746636\n",
      "Batch:  62 , correct:  4 , loss:  2.2797939699816108\n",
      "Batch:  63 , correct:  4 , loss:  2.279897266994687\n",
      "Batch:  64 , correct:  4 , loss:  2.2799984471050565\n",
      "Batch:  65 , correct:  3 , loss:  2.2801008389107573\n",
      "Batch:  66 , correct:  5 , loss:  2.279202744348039\n",
      "Batch:  67 , correct:  3 , loss:  2.2793070229053476\n",
      "Batch:  68 , correct:  3 , loss:  2.2794086311233523\n",
      "Batch:  69 , correct:  2 , loss:  2.279512274671547\n",
      "Batch:  70 , correct:  2 , loss:  2.2786143084803094\n",
      "Batch:  71 , correct:  5 , loss:  2.2787156537411764\n",
      "Batch:  72 , correct:  0 , loss:  2.27881906940104\n",
      "Batch:  73 , correct:  4 , loss:  2.2789204710716358\n",
      "Batch:  74 , correct:  4 , loss:  2.2790229146712093\n",
      "Batch:  75 , correct:  3 , loss:  2.27912539525447\n",
      "Batch:  76 , correct:  5 , loss:  2.279226839831174\n",
      "Batch:  77 , correct:  3 , loss:  2.2793283715941066\n",
      "Batch:  78 , correct:  2 , loss:  2.279432629256507\n",
      "Batch:  79 , correct:  6 , loss:  2.279535128940058\n",
      "Batch:  80 , correct:  0 , loss:  2.279636694776424\n",
      "Batch:  81 , correct:  4 , loss:  2.2797392531341374\n",
      "Batch:  82 , correct:  4 , loss:  2.2798435358368287\n",
      "Batch:  83 , correct:  2 , loss:  2.2799460917926537\n",
      "Batch:  84 , correct:  5 , loss:  2.2790480306142284\n",
      "Batch:  85 , correct:  2 , loss:  2.2791507143135754\n",
      "Batch:  86 , correct:  1 , loss:  2.2792523553053523\n",
      "Batch:  87 , correct:  3 , loss:  2.2793538819063186\n",
      "Batch:  88 , correct:  4 , loss:  2.2794571621131463\n",
      "Batch:  89 , correct:  4 , loss:  2.278559204124688\n",
      "Batch:  90 , correct:  1 , loss:  2.2786608688457575\n",
      "Batch:  91 , correct:  2 , loss:  2.2787652660083877\n",
      "Batch:  92 , correct:  0 , loss:  2.278867935388301\n",
      "Batch:  93 , correct:  2 , loss:  2.2789696430260347\n",
      "Batch:  94 , correct:  3 , loss:  2.279073175359338\n",
      "Batch:  95 , correct:  3 , loss:  2.279174931850493\n",
      "Batch:  96 , correct:  4 , loss:  2.27927852677369\n",
      "Batch:  97 , correct:  3 , loss:  2.279381199010263\n",
      "Batch:  98 , correct:  2 , loss:  2.2794838202109373\n",
      "Batch:  99 , correct:  4 , loss:  2.27958707979666\n",
      "Batch:  100 , correct:  6 , loss:  2.2796864889090256\n",
      "Batch:  101 , correct:  5 , loss:  2.278788484007484\n",
      "Batch:  102 , correct:  2 , loss:  2.2788921539232416\n",
      "Batch:  103 , correct:  1 , loss:  2.278995911102948\n",
      "Batch:  104 , correct:  5 , loss:  2.27909927324034\n",
      "Batch:  105 , correct:  5 , loss:  2.279201477483312\n",
      "Batch:  106 , correct:  3 , loss:  2.279304140152388\n",
      "Batch:  107 , correct:  4 , loss:  2.2784062090920107\n",
      "Batch:  108 , correct:  4 , loss:  2.2785078095595885\n",
      "Batch:  109 , correct:  3 , loss:  2.2786106084709834\n",
      "Batch:  110 , correct:  3 , loss:  2.2777128215544673\n",
      "Batch:  111 , correct:  4 , loss:  2.277816717157589\n",
      "Batch:  112 , correct:  5 , loss:  2.2779194854818448\n",
      "Batch:  113 , correct:  5 , loss:  2.278021828669459\n",
      "Batch:  114 , correct:  4 , loss:  2.27812576456131\n",
      "Batch:  115 , correct:  3 , loss:  2.2782292030973217\n",
      "Batch:  116 , correct:  2 , loss:  2.2783327209330277\n",
      "Batch:  117 , correct:  3 , loss:  2.2784350844388945\n",
      "Batch:  118 , correct:  2 , loss:  2.278536682790088\n",
      "Batch:  119 , correct:  2 , loss:  2.2786391127468657\n",
      "Batch:  120 , correct:  4 , loss:  2.2787418855039685\n",
      "Batch:  121 , correct:  3 , loss:  2.278843523151367\n",
      "Batch:  122 , correct:  6 , loss:  2.278947391003448\n",
      "Batch:  123 , correct:  2 , loss:  2.279048987839036\n",
      "Batch:  124 , correct:  2 , loss:  2.2791529175766096\n",
      "Batch:  125 , correct:  2 , loss:  2.279254568853295\n",
      "Batch:  126 , correct:  2 , loss:  2.2793579849999857\n",
      "Batch:  127 , correct:  3 , loss:  2.279457251347498\n",
      "Batch:  128 , correct:  5 , loss:  2.279559598653771\n",
      "Batch:  129 , correct:  8 , loss:  2.279658919949243\n",
      "Batch:  130 , correct:  2 , loss:  2.2787609287304647\n",
      "Batch:  131 , correct:  4 , loss:  2.278862560905647\n",
      "Batch:  132 , correct:  1 , loss:  2.2789620385146607\n",
      "Batch:  133 , correct:  3 , loss:  2.2790637378062395\n",
      "Batch:  134 , correct:  6 , loss:  2.279165417479478\n",
      "Batch:  135 , correct:  1 , loss:  2.279269408169665\n",
      "Batch:  136 , correct:  2 , loss:  2.2793718938979604\n",
      "Batch:  137 , correct:  6 , loss:  2.279473605887545\n",
      "Batch:  138 , correct:  2 , loss:  2.279575398995205\n",
      "Batch:  139 , correct:  3 , loss:  2.2786774172077893\n",
      "Batch:  140 , correct:  4 , loss:  2.2777796184168384\n",
      "Batch:  141 , correct:  2 , loss:  2.27788379525979\n",
      "Batch:  142 , correct:  6 , loss:  2.277988053293901\n",
      "Batch:  143 , correct:  2 , loss:  2.278090697167858\n",
      "Batch:  144 , correct:  3 , loss:  2.278193419920968\n",
      "Batch:  145 , correct:  2 , loss:  2.277295722227379\n",
      "Batch:  146 , correct:  5 , loss:  2.277397561615541\n",
      "Batch:  147 , correct:  4 , loss:  2.2775010580855013\n",
      "Batch:  148 , correct:  4 , loss:  2.2776029583725714\n",
      "Batch:  149 , correct:  1 , loss:  2.2777057851273166\n",
      "Total Loss =  2.2790680393802756\n",
      "\n",
      "EPOCH:  4\n",
      "Batch:  0 , correct:  2 , loss:  2.2778053000670946\n",
      "Batch:  1 , correct:  3 , loss:  2.2779072314392845\n",
      "Batch:  2 , correct:  4 , loss:  2.2780091569224084\n",
      "Batch:  3 , correct:  2 , loss:  2.2771114924741416\n",
      "Batch:  4 , correct:  4 , loss:  2.2772158130996325\n",
      "Batch:  5 , correct:  5 , loss:  2.2763183120289554\n",
      "Batch:  6 , correct:  5 , loss:  2.2754209971352193\n",
      "Batch:  7 , correct:  6 , loss:  2.2755235648336383\n",
      "Batch:  8 , correct:  2 , loss:  2.275625771988728\n",
      "Batch:  9 , correct:  5 , loss:  2.2757294255572305\n",
      "Batch:  10 , correct:  4 , loss:  2.2758339151098594\n",
      "Batch:  11 , correct:  1 , loss:  2.275936664786935\n",
      "Batch:  12 , correct:  1 , loss:  2.2760388896980506\n",
      "Batch:  13 , correct:  5 , loss:  2.2761416975711604\n",
      "Batch:  14 , correct:  2 , loss:  2.27524441443899\n",
      "Batch:  15 , correct:  4 , loss:  2.2753470221685013\n",
      "Batch:  16 , correct:  2 , loss:  2.2744498962019657\n",
      "Batch:  17 , correct:  0 , loss:  2.2745529312445205\n",
      "Batch:  18 , correct:  3 , loss:  2.27465697050758\n",
      "Batch:  19 , correct:  2 , loss:  2.274760063885786\n",
      "Batch:  20 , correct:  0 , loss:  2.274864654764188\n",
      "Batch:  21 , correct:  5 , loss:  2.2749676444836244\n",
      "Batch:  22 , correct:  6 , loss:  2.2750673227403824\n",
      "Batch:  23 , correct:  6 , loss:  2.2741702593539324\n",
      "Batch:  24 , correct:  2 , loss:  2.274270103733681\n",
      "Batch:  25 , correct:  3 , loss:  2.2743732738481226\n",
      "Batch:  26 , correct:  2 , loss:  2.2744756227021097\n",
      "Batch:  27 , correct:  1 , loss:  2.274579307218467\n",
      "Batch:  28 , correct:  4 , loss:  2.274683941736286\n",
      "Batch:  29 , correct:  4 , loss:  2.27478601971394\n",
      "Batch:  30 , correct:  1 , loss:  2.274888616877994\n",
      "Batch:  31 , correct:  1 , loss:  2.2749917604690313\n",
      "Batch:  32 , correct:  3 , loss:  2.2750957219073906\n",
      "Batch:  33 , correct:  1 , loss:  2.2752003465146413\n",
      "Batch:  34 , correct:  3 , loss:  2.2753024225459617\n",
      "Batch:  35 , correct:  4 , loss:  2.275405326378297\n",
      "Batch:  36 , correct:  1 , loss:  2.2745081965471763\n",
      "Batch:  37 , correct:  4 , loss:  2.2746122442595618\n",
      "Batch:  38 , correct:  3 , loss:  2.2747152893272666\n",
      "Batch:  39 , correct:  5 , loss:  2.2748175736391354\n",
      "Batch:  40 , correct:  2 , loss:  2.2749206733486047\n",
      "Batch:  41 , correct:  0 , loss:  2.275024275459602\n",
      "Batch:  42 , correct:  0 , loss:  2.275127433412373\n",
      "Batch:  43 , correct:  2 , loss:  2.275227096054007\n",
      "Batch:  44 , correct:  5 , loss:  2.2753316982663074\n",
      "Batch:  45 , correct:  2 , loss:  2.2754347609499503\n",
      "Batch:  46 , correct:  1 , loss:  2.2755379358118044\n",
      "Batch:  47 , correct:  4 , loss:  2.2756415075220806\n",
      "Batch:  48 , correct:  2 , loss:  2.2757437123444775\n",
      "Batch:  49 , correct:  4 , loss:  2.2758473504887906\n",
      "Batch:  50 , correct:  3 , loss:  2.274950125998115\n",
      "Batch:  51 , correct:  2 , loss:  2.2750547886098254\n",
      "Batch:  52 , correct:  2 , loss:  2.275158562237082\n",
      "Batch:  53 , correct:  4 , loss:  2.2742614855846353\n",
      "Batch:  54 , correct:  7 , loss:  2.2743654892569816\n",
      "Batch:  55 , correct:  1 , loss:  2.2734685755000905\n",
      "Batch:  56 , correct:  1 , loss:  2.273571809989027\n",
      "Batch:  57 , correct:  6 , loss:  2.2736755025201894\n",
      "Batch:  58 , correct:  1 , loss:  2.273775024751533\n",
      "Batch:  59 , correct:  4 , loss:  2.2738770496600536\n",
      "Batch:  60 , correct:  7 , loss:  2.273981092469168\n",
      "Batch:  61 , correct:  2 , loss:  2.273084345296359\n",
      "Batch:  62 , correct:  2 , loss:  2.2731866159374072\n",
      "Batch:  63 , correct:  4 , loss:  2.2722900218579976\n",
      "Batch:  64 , correct:  1 , loss:  2.2723897249154046\n",
      "Batch:  65 , correct:  3 , loss:  2.272491931734953\n",
      "Batch:  66 , correct:  2 , loss:  2.272596160449875\n",
      "Batch:  67 , correct:  3 , loss:  2.2726985384279805\n",
      "Batch:  68 , correct:  5 , loss:  2.2728036697429177\n",
      "Batch:  69 , correct:  2 , loss:  2.2729076567954305\n",
      "Batch:  70 , correct:  3 , loss:  2.272011132042076\n",
      "Batch:  71 , correct:  2 , loss:  2.272114985875173\n",
      "Batch:  72 , correct:  0 , loss:  2.2722192894247613\n",
      "Batch:  73 , correct:  3 , loss:  2.2723190071772414\n",
      "Batch:  74 , correct:  9 , loss:  2.2724233742445943\n",
      "Batch:  75 , correct:  2 , loss:  2.2725256785794934\n",
      "Batch:  76 , correct:  3 , loss:  2.272628731970653\n",
      "Batch:  77 , correct:  3 , loss:  2.2727310970252326\n",
      "Batch:  78 , correct:  5 , loss:  2.271834608427786\n",
      "Batch:  79 , correct:  4 , loss:  2.2719344072368273\n",
      "Batch:  80 , correct:  3 , loss:  2.272037581571955\n",
      "Batch:  81 , correct:  6 , loss:  2.271141228764806\n",
      "Batch:  82 , correct:  4 , loss:  2.27024506663873\n",
      "Batch:  83 , correct:  5 , loss:  2.270347626093786\n",
      "Batch:  84 , correct:  2 , loss:  2.2704502536227293\n",
      "Batch:  85 , correct:  4 , loss:  2.2705525588809263\n",
      "Batch:  86 , correct:  6 , loss:  2.270657828536182\n",
      "Batch:  87 , correct:  3 , loss:  2.27075777127919\n",
      "Batch:  88 , correct:  3 , loss:  2.270861893104927\n",
      "Batch:  89 , correct:  2 , loss:  2.269965785473148\n",
      "Batch:  90 , correct:  1 , loss:  2.270071171530825\n",
      "Batch:  91 , correct:  1 , loss:  2.2701735613311893\n",
      "Batch:  92 , correct:  5 , loss:  2.2702761278660346\n",
      "Batch:  93 , correct:  4 , loss:  2.270381560665347\n",
      "Batch:  94 , correct:  4 , loss:  2.270483990412339\n",
      "Batch:  95 , correct:  5 , loss:  2.270588436120951\n",
      "Batch:  96 , correct:  2 , loss:  2.2706909199431937\n",
      "Batch:  97 , correct:  4 , loss:  2.2707954207209955\n",
      "Batch:  98 , correct:  5 , loss:  2.270897963259624\n",
      "Batch:  99 , correct:  5 , loss:  2.271002055984089\n",
      "Batch:  100 , correct:  4 , loss:  2.27110581420138\n",
      "Batch:  101 , correct:  3 , loss:  2.271211173100864\n",
      "Batch:  102 , correct:  1 , loss:  2.271313638210835\n",
      "Batch:  103 , correct:  4 , loss:  2.271417751385648\n",
      "Batch:  104 , correct:  3 , loss:  2.2715215225916743\n",
      "Batch:  105 , correct:  7 , loss:  2.2716239275167784\n",
      "Batch:  106 , correct:  5 , loss:  2.2717292831974913\n",
      "Batch:  107 , correct:  3 , loss:  2.2708329922108095\n",
      "Batch:  108 , correct:  3 , loss:  2.27093747115872\n",
      "Batch:  109 , correct:  0 , loss:  2.2710420257521244\n",
      "Batch:  110 , correct:  2 , loss:  2.2711445267562502\n",
      "Batch:  111 , correct:  1 , loss:  2.2712471099962506\n",
      "Batch:  112 , correct:  6 , loss:  2.2713495696800576\n",
      "Batch:  113 , correct:  2 , loss:  2.271453359843563\n",
      "Batch:  114 , correct:  2 , loss:  2.2715579089826106\n",
      "Batch:  115 , correct:  2 , loss:  2.2716604118550165\n",
      "Batch:  116 , correct:  3 , loss:  2.2717629885268344\n",
      "Batch:  117 , correct:  1 , loss:  2.2718659120960925\n",
      "Batch:  118 , correct:  2 , loss:  2.271970482255973\n",
      "Batch:  119 , correct:  2 , loss:  2.272075773749205\n",
      "Batch:  120 , correct:  6 , loss:  2.272179523323203\n",
      "Batch:  121 , correct:  3 , loss:  2.2722820931241863\n",
      "Batch:  122 , correct:  1 , loss:  2.2723850079734227\n",
      "Batch:  123 , correct:  4 , loss:  2.2724874473777326\n",
      "Batch:  124 , correct:  3 , loss:  2.2725913171547876\n",
      "Batch:  125 , correct:  2 , loss:  2.2716948494958795\n",
      "Batch:  126 , correct:  4 , loss:  2.271797527508354\n",
      "Batch:  127 , correct:  2 , loss:  2.271897064316508\n",
      "Batch:  128 , correct:  3 , loss:  2.271000741860237\n",
      "Batch:  129 , correct:  4 , loss:  2.271104611735828\n",
      "Batch:  130 , correct:  3 , loss:  2.2712069076209573\n",
      "Batch:  131 , correct:  2 , loss:  2.271311517351572\n",
      "Batch:  132 , correct:  6 , loss:  2.2714142669098383\n",
      "Batch:  133 , correct:  2 , loss:  2.2715189273149203\n",
      "Batch:  134 , correct:  3 , loss:  2.2716212392226978\n",
      "Batch:  135 , correct:  1 , loss:  2.271725190094402\n",
      "Batch:  136 , correct:  4 , loss:  2.2718276599397558\n",
      "Batch:  137 , correct:  2 , loss:  2.2719304186448404\n",
      "Batch:  138 , correct:  4 , loss:  2.2720333166986197\n",
      "Batch:  139 , correct:  3 , loss:  2.272137071999064\n",
      "Batch:  140 , correct:  4 , loss:  2.2722395664721056\n",
      "Batch:  141 , correct:  4 , loss:  2.272344172038884\n",
      "Batch:  142 , correct:  4 , loss:  2.272443604602789\n",
      "Batch:  143 , correct:  2 , loss:  2.272547491805301\n",
      "Batch:  144 , correct:  3 , loss:  2.27265256701084\n",
      "Batch:  145 , correct:  4 , loss:  2.2727563069334655\n",
      "Batch:  146 , correct:  6 , loss:  2.27286022688536\n",
      "Batch:  147 , correct:  4 , loss:  2.2729653447760607\n",
      "Batch:  148 , correct:  5 , loss:  2.2730693261060217\n",
      "Batch:  149 , correct:  2 , loss:  2.2731744965884513\n",
      "Total Loss =  2.273113162367153\n",
      "\n",
      "EPOCH:  5\n",
      "Batch:  0 , correct:  2 , loss:  2.2732772429972337\n",
      "Batch:  1 , correct:  4 , loss:  2.2733765967519894\n",
      "Batch:  2 , correct:  3 , loss:  2.272479966660356\n",
      "Batch:  3 , correct:  7 , loss:  2.271583530992446\n",
      "Batch:  4 , correct:  3 , loss:  2.271685731019988\n",
      "Batch:  5 , correct:  3 , loss:  2.2707894508516824\n",
      "Batch:  6 , correct:  3 , loss:  2.2708918206464843\n",
      "Batch:  7 , correct:  5 , loss:  2.27099437616969\n",
      "Batch:  8 , correct:  3 , loss:  2.2710971015239663\n",
      "Batch:  9 , correct:  5 , loss:  2.271201261977778\n",
      "Batch:  10 , correct:  4 , loss:  2.2703050870721575\n",
      "Batch:  11 , correct:  1 , loss:  2.270409411077768\n",
      "Batch:  12 , correct:  4 , loss:  2.2705124196236595\n",
      "Batch:  13 , correct:  5 , loss:  2.2696163843476826\n",
      "Batch:  14 , correct:  2 , loss:  2.2697192957819095\n",
      "Batch:  15 , correct:  3 , loss:  2.269824008913486\n",
      "Batch:  16 , correct:  2 , loss:  2.2699269779245363\n",
      "Batch:  17 , correct:  2 , loss:  2.2700313791224134\n",
      "Batch:  18 , correct:  4 , loss:  2.2691354405407425\n",
      "Batch:  19 , correct:  3 , loss:  2.2692351404474946\n",
      "Batch:  20 , correct:  4 , loss:  2.2693382318188493\n",
      "Batch:  21 , correct:  1 , loss:  2.2694379891258123\n",
      "Batch:  22 , correct:  6 , loss:  2.2695411023997054\n",
      "Batch:  23 , correct:  3 , loss:  2.2696442329355575\n",
      "Batch:  24 , correct:  1 , loss:  2.2697480884132006\n",
      "Batch:  25 , correct:  3 , loss:  2.2698527339954504\n",
      "Batch:  26 , correct:  2 , loss:  2.269955208926509\n",
      "Batch:  27 , correct:  2 , loss:  2.269059444436822\n",
      "Batch:  28 , correct:  5 , loss:  2.2691593962765495\n",
      "Batch:  29 , correct:  3 , loss:  2.268263798241624\n",
      "Batch:  30 , correct:  5 , loss:  2.268367168323161\n",
      "Batch:  31 , correct:  3 , loss:  2.268472041351453\n",
      "Batch:  32 , correct:  2 , loss:  2.2685767457586072\n",
      "Batch:  33 , correct:  0 , loss:  2.2686816676744472\n",
      "Batch:  34 , correct:  2 , loss:  2.26878575781842\n",
      "Batch:  35 , correct:  2 , loss:  2.2688857586559403\n",
      "Batch:  36 , correct:  2 , loss:  2.2689879371866266\n",
      "Batch:  37 , correct:  5 , loss:  2.269092075270529\n",
      "Batch:  38 , correct:  4 , loss:  2.2691967528029116\n",
      "Batch:  39 , correct:  2 , loss:  2.26830114803261\n",
      "Batch:  40 , correct:  2 , loss:  2.267405732422764\n",
      "Batch:  41 , correct:  4 , loss:  2.267510959342487\n",
      "Batch:  42 , correct:  3 , loss:  2.2676111223861595\n",
      "Batch:  43 , correct:  2 , loss:  2.2677154332751814\n",
      "Batch:  44 , correct:  2 , loss:  2.267820299289205\n",
      "Batch:  45 , correct:  3 , loss:  2.2679246718976187\n",
      "Batch:  46 , correct:  4 , loss:  2.2680295949138682\n",
      "Batch:  47 , correct:  1 , loss:  2.2681297566613097\n",
      "Batch:  48 , correct:  4 , loss:  2.2682330558680124\n",
      "Batch:  49 , correct:  3 , loss:  2.2683355709544815\n",
      "Batch:  50 , correct:  7 , loss:  2.2684404487141485\n",
      "Batch:  51 , correct:  1 , loss:  2.26854327080243\n",
      "Batch:  52 , correct:  4 , loss:  2.268645821218027\n",
      "Batch:  53 , correct:  2 , loss:  2.2687507017340516\n",
      "Batch:  54 , correct:  3 , loss:  2.2688539845536257\n",
      "Batch:  55 , correct:  7 , loss:  2.2689588547129147\n",
      "Batch:  56 , correct:  4 , loss:  2.2690638051015726\n",
      "Batch:  57 , correct:  1 , loss:  2.2681682336704485\n",
      "Batch:  58 , correct:  1 , loss:  2.2682708597487413\n",
      "Batch:  59 , correct:  2 , loss:  2.2683759595002133\n",
      "Batch:  60 , correct:  3 , loss:  2.2684809525826486\n",
      "Batch:  61 , correct:  2 , loss:  2.2685859183455848\n",
      "Batch:  62 , correct:  2 , loss:  2.2676905639104263\n",
      "Batch:  63 , correct:  5 , loss:  2.2677947740044733\n",
      "Batch:  64 , correct:  3 , loss:  2.2678996614428035\n",
      "Batch:  65 , correct:  3 , loss:  2.2680046245697616\n",
      "Batch:  66 , correct:  3 , loss:  2.2681073306598165\n",
      "Batch:  67 , correct:  6 , loss:  2.268209354344681\n",
      "Batch:  68 , correct:  5 , loss:  2.2683095282743277\n",
      "Batch:  69 , correct:  1 , loss:  2.2684127490818433\n",
      "Batch:  70 , correct:  3 , loss:  2.2685129856132473\n",
      "Batch:  71 , correct:  1 , loss:  2.26861794577369\n",
      "Batch:  72 , correct:  6 , loss:  2.268720630903017\n",
      "Batch:  73 , correct:  3 , loss:  2.2688247381522073\n",
      "Batch:  74 , correct:  6 , loss:  2.268928921906247\n",
      "Batch:  75 , correct:  3 , loss:  2.2690337771662064\n",
      "Batch:  76 , correct:  4 , loss:  2.2681383314592436\n",
      "Batch:  77 , correct:  5 , loss:  2.268243354607791\n",
      "Batch:  78 , correct:  3 , loss:  2.2683484525284334\n",
      "Batch:  79 , correct:  3 , loss:  2.2684527383300526\n",
      "Batch:  80 , correct:  4 , loss:  2.268554695661993\n",
      "Batch:  81 , correct:  3 , loss:  2.268657400562559\n",
      "Batch:  82 , correct:  3 , loss:  2.267762030342506\n",
      "Batch:  83 , correct:  2 , loss:  2.2678670521058493\n",
      "Batch:  84 , correct:  3 , loss:  2.267972046666666\n",
      "Batch:  85 , correct:  3 , loss:  2.2680771185945336\n",
      "Batch:  86 , correct:  3 , loss:  2.2681799205162383\n",
      "Batch:  87 , correct:  4 , loss:  2.268280111205377\n",
      "Batch:  88 , correct:  5 , loss:  2.2683803887257956\n",
      "Batch:  89 , correct:  4 , loss:  2.268480737558398\n",
      "Batch:  90 , correct:  3 , loss:  2.268583555297162\n",
      "Batch:  91 , correct:  2 , loss:  2.2686854927449605\n",
      "Batch:  92 , correct:  2 , loss:  2.268788549685029\n",
      "Batch:  93 , correct:  2 , loss:  2.2688914020204853\n",
      "Batch:  94 , correct:  3 , loss:  2.2689943467473124\n",
      "Batch:  95 , correct:  3 , loss:  2.2690993195477946\n",
      "Batch:  96 , correct:  4 , loss:  2.2692023898107974\n",
      "Batch:  97 , correct:  1 , loss:  2.269307223958469\n",
      "Batch:  98 , correct:  2 , loss:  2.269411312917382\n",
      "Batch:  99 , correct:  4 , loss:  2.269516210484125\n",
      "Batch:  100 , correct:  2 , loss:  2.2696192979350487\n",
      "Batch:  101 , correct:  4 , loss:  2.2687237409148766\n",
      "Batch:  102 , correct:  6 , loss:  2.268828772672058\n",
      "Batch:  103 , correct:  1 , loss:  2.267933370909147\n",
      "Batch:  104 , correct:  2 , loss:  2.2680384620902365\n",
      "Batch:  105 , correct:  4 , loss:  2.2681436285163312\n",
      "Batch:  106 , correct:  5 , loss:  2.2682487818037456\n",
      "Batch:  107 , correct:  2 , loss:  2.268351237985186\n",
      "Batch:  108 , correct:  1 , loss:  2.268453130554246\n",
      "Batch:  109 , correct:  3 , loss:  2.2685563369960695\n",
      "Batch:  110 , correct:  4 , loss:  2.268658290592779\n",
      "Batch:  111 , correct:  5 , loss:  2.267762927254483\n",
      "Batch:  112 , correct:  3 , loss:  2.2678681533708818\n",
      "Batch:  113 , correct:  1 , loss:  2.2679733661205383\n",
      "Batch:  114 , correct:  4 , loss:  2.268078656961277\n",
      "Batch:  115 , correct:  4 , loss:  2.268181148765737\n",
      "Batch:  116 , correct:  1 , loss:  2.2682831781376405\n",
      "Batch:  117 , correct:  2 , loss:  2.268387979612149\n",
      "Batch:  118 , correct:  5 , loss:  2.2684920618321494\n",
      "Batch:  119 , correct:  2 , loss:  2.2685972389558757\n",
      "Batch:  120 , correct:  4 , loss:  2.2687000516818143\n",
      "Batch:  121 , correct:  5 , loss:  2.2688020741972936\n",
      "Batch:  122 , correct:  3 , loss:  2.2689052152772295\n",
      "Batch:  123 , correct:  1 , loss:  2.2690084402895336\n",
      "Batch:  124 , correct:  2 , loss:  2.2691112661815747\n",
      "Batch:  125 , correct:  2 , loss:  2.2692141722199306\n",
      "Batch:  126 , correct:  1 , loss:  2.2693193193586456\n",
      "Batch:  127 , correct:  3 , loss:  2.2694222443511363\n",
      "Batch:  128 , correct:  1 , loss:  2.269525139396664\n",
      "Batch:  129 , correct:  1 , loss:  2.2696301124931035\n",
      "Batch:  130 , correct:  4 , loss:  2.2697301945686217\n",
      "Batch:  131 , correct:  6 , loss:  2.26983244470687\n",
      "Batch:  132 , correct:  2 , loss:  2.269935357148206\n",
      "Batch:  133 , correct:  3 , loss:  2.270038200495322\n",
      "Batch:  134 , correct:  3 , loss:  2.2701411699541576\n",
      "Batch:  135 , correct:  5 , loss:  2.2692457669720114\n",
      "Batch:  136 , correct:  3 , loss:  2.2693459333872323\n",
      "Batch:  137 , correct:  3 , loss:  2.2684506845814125\n",
      "Batch:  138 , correct:  2 , loss:  2.268555060394515\n",
      "Batch:  139 , correct:  3 , loss:  2.268657276757633\n",
      "Batch:  140 , correct:  1 , loss:  2.2687604345961443\n",
      "Batch:  141 , correct:  5 , loss:  2.2688634276729105\n",
      "Batch:  142 , correct:  1 , loss:  2.2689666368884884\n",
      "Batch:  143 , correct:  7 , loss:  2.269071632124666\n",
      "Batch:  144 , correct:  6 , loss:  2.2691718306884217\n",
      "Batch:  145 , correct:  4 , loss:  2.269274031878891\n",
      "Batch:  146 , correct:  6 , loss:  2.268378807821203\n",
      "Batch:  147 , correct:  2 , loss:  2.2684821178327352\n",
      "Batch:  148 , correct:  1 , loss:  2.2685871520477208\n",
      "Batch:  149 , correct:  3 , loss:  2.268690523874257\n",
      "Total Loss =  2.268954887741895\n",
      "\n",
      "EPOCH:  6\n",
      "Batch:  0 , correct:  2 , loss:  2.267795419225581\n",
      "Batch:  1 , correct:  3 , loss:  2.2678985108971657\n",
      "Batch:  2 , correct:  3 , loss:  2.268000875829881\n",
      "Batch:  3 , correct:  2 , loss:  2.268104061600684\n",
      "Batch:  4 , correct:  3 , loss:  2.2682062835843624\n",
      "Batch:  5 , correct:  3 , loss:  2.2683085463769896\n",
      "Batch:  6 , correct:  1 , loss:  2.2674138248275177\n",
      "Batch:  7 , correct:  1 , loss:  2.2675161896640055\n",
      "Batch:  8 , correct:  3 , loss:  2.26761859638459\n",
      "Batch:  9 , correct:  5 , loss:  2.26772108110281\n",
      "Batch:  10 , correct:  2 , loss:  2.267825903438768\n",
      "Batch:  11 , correct:  3 , loss:  2.267929241169999\n",
      "Batch:  12 , correct:  3 , loss:  2.2680328885866925\n",
      "Batch:  13 , correct:  3 , loss:  2.2681367644464943\n",
      "Batch:  14 , correct:  2 , loss:  2.2682401293722365\n",
      "Batch:  15 , correct:  3 , loss:  2.268342432990544\n",
      "Batch:  16 , correct:  0 , loss:  2.268447229967333\n",
      "Batch:  17 , correct:  3 , loss:  2.268550642184801\n",
      "Batch:  18 , correct:  4 , loss:  2.2686509357219236\n",
      "Batch:  19 , correct:  7 , loss:  2.268751314218122\n",
      "Batch:  20 , correct:  4 , loss:  2.268855146661212\n",
      "Batch:  21 , correct:  4 , loss:  2.2689574883634775\n",
      "Batch:  22 , correct:  3 , loss:  2.269057904233767\n",
      "Batch:  23 , correct:  3 , loss:  2.2691617766476297\n",
      "Batch:  24 , correct:  2 , loss:  2.2692652807603424\n",
      "Batch:  25 , correct:  4 , loss:  2.269367478886619\n",
      "Batch:  26 , correct:  3 , loss:  2.269467912549069\n",
      "Batch:  27 , correct:  1 , loss:  2.2695712187522075\n",
      "Batch:  28 , correct:  1 , loss:  2.269675776969418\n",
      "Batch:  29 , correct:  3 , loss:  2.2697779943284497\n",
      "Batch:  30 , correct:  6 , loss:  2.2688829673585387\n",
      "Batch:  31 , correct:  2 , loss:  2.2689865356721035\n",
      "Batch:  32 , correct:  1 , loss:  2.2690901773275445\n",
      "Batch:  33 , correct:  2 , loss:  2.2691948278713654\n",
      "Batch:  34 , correct:  0 , loss:  2.2692986730389078\n",
      "Batch:  35 , correct:  4 , loss:  2.269402362130338\n",
      "Batch:  36 , correct:  4 , loss:  2.2695062665893766\n",
      "Batch:  37 , correct:  6 , loss:  2.2696066797385566\n",
      "Batch:  38 , correct:  6 , loss:  2.2697071670933813\n",
      "Batch:  39 , correct:  4 , loss:  2.269809383834514\n",
      "Batch:  40 , correct:  5 , loss:  2.2699139945079607\n",
      "Batch:  41 , correct:  4 , loss:  2.2700172153397125\n",
      "Batch:  42 , correct:  3 , loss:  2.2701194681782058\n",
      "Batch:  43 , correct:  1 , loss:  2.2702227499382572\n",
      "Batch:  44 , correct:  1 , loss:  2.270326331114265\n",
      "Batch:  45 , correct:  4 , loss:  2.2704301533297717\n",
      "Batch:  46 , correct:  3 , loss:  2.2695349935594784\n",
      "Batch:  47 , correct:  4 , loss:  2.2686400227605934\n",
      "Batch:  48 , correct:  2 , loss:  2.2677452334078825\n",
      "Batch:  49 , correct:  1 , loss:  2.2678500579205\n",
      "Batch:  50 , correct:  3 , loss:  2.2679507163889947\n",
      "Batch:  51 , correct:  2 , loss:  2.2680548186972156\n",
      "Batch:  52 , correct:  2 , loss:  2.2681585677009912\n",
      "Batch:  53 , correct:  4 , loss:  2.2682620426542286\n",
      "Batch:  54 , correct:  2 , loss:  2.268365824810062\n",
      "Batch:  55 , correct:  5 , loss:  2.2684706301075384\n",
      "Batch:  56 , correct:  3 , loss:  2.2685712592535157\n",
      "Batch:  57 , correct:  3 , loss:  2.267676483437365\n",
      "Batch:  58 , correct:  6 , loss:  2.2677806484277427\n",
      "Batch:  59 , correct:  1 , loss:  2.266886034916446\n",
      "Batch:  60 , correct:  4 , loss:  2.2669882953865215\n",
      "Batch:  61 , correct:  6 , loss:  2.267090645548804\n",
      "Batch:  62 , correct:  4 , loss:  2.2671942404704866\n",
      "Batch:  63 , correct:  1 , loss:  2.2672991833583054\n",
      "Batch:  64 , correct:  9 , loss:  2.267403065449852\n",
      "Batch:  65 , correct:  3 , loss:  2.2665085319498925\n",
      "Batch:  66 , correct:  5 , loss:  2.266613120560174\n",
      "Batch:  67 , correct:  6 , loss:  2.266716819201403\n",
      "Batch:  68 , correct:  6 , loss:  2.2668205963472796\n",
      "Batch:  69 , correct:  2 , loss:  2.266924832688979\n",
      "Batch:  70 , correct:  3 , loss:  2.267029155323293\n",
      "Batch:  71 , correct:  3 , loss:  2.2671298850961428\n",
      "Batch:  72 , correct:  5 , loss:  2.267233800309881\n",
      "Batch:  73 , correct:  5 , loss:  2.2673387400603913\n",
      "Batch:  74 , correct:  4 , loss:  2.266444218943261\n",
      "Batch:  75 , correct:  0 , loss:  2.2665482754180473\n",
      "Batch:  76 , correct:  1 , loss:  2.2666506505979296\n",
      "Batch:  77 , correct:  1 , loss:  2.2667529680599983\n",
      "Batch:  78 , correct:  3 , loss:  2.2668567420425263\n",
      "Batch:  79 , correct:  3 , loss:  2.266960596672043\n",
      "Batch:  80 , correct:  1 , loss:  2.2670649116438435\n",
      "Batch:  81 , correct:  6 , loss:  2.267169386650877\n",
      "Batch:  82 , correct:  2 , loss:  2.267273277183387\n",
      "Batch:  83 , correct:  8 , loss:  2.267377815466572\n",
      "Batch:  84 , correct:  5 , loss:  2.267480087326542\n",
      "Batch:  85 , correct:  3 , loss:  2.2675849774067802\n",
      "Batch:  86 , correct:  1 , loss:  2.267688883630516\n",
      "Batch:  87 , correct:  1 , loss:  2.267793831665916\n",
      "Batch:  88 , correct:  5 , loss:  2.2678980825234087\n",
      "Batch:  89 , correct:  3 , loss:  2.268003089771949\n",
      "Batch:  90 , correct:  5 , loss:  2.2681070145699653\n",
      "Batch:  91 , correct:  2 , loss:  2.2682120789560107\n",
      "Batch:  92 , correct:  0 , loss:  2.2683142846592785\n",
      "Batch:  93 , correct:  1 , loss:  2.268414767369512\n",
      "Batch:  94 , correct:  5 , loss:  2.268519175132362\n",
      "Batch:  95 , correct:  3 , loss:  2.2686212570096833\n",
      "Batch:  96 , correct:  2 , loss:  2.267726464747124\n",
      "Batch:  97 , correct:  3 , loss:  2.267830732012546\n",
      "Batch:  98 , correct:  2 , loss:  2.267933015877325\n",
      "Batch:  99 , correct:  4 , loss:  2.2680369613274385\n",
      "Batch:  100 , correct:  6 , loss:  2.2681412594493677\n",
      "Batch:  101 , correct:  2 , loss:  2.268245264170745\n",
      "Batch:  102 , correct:  2 , loss:  2.2683497029069697\n",
      "Batch:  103 , correct:  1 , loss:  2.26845198851658\n",
      "Batch:  104 , correct:  4 , loss:  2.2685540906410218\n",
      "Batch:  105 , correct:  5 , loss:  2.267659314147709\n",
      "Batch:  106 , correct:  3 , loss:  2.2677630248330507\n",
      "Batch:  107 , correct:  3 , loss:  2.267867107579145\n",
      "Batch:  108 , correct:  1 , loss:  2.2679694864459674\n",
      "Batch:  109 , correct:  3 , loss:  2.2680699546123955\n",
      "Batch:  110 , correct:  2 , loss:  2.268174928747657\n",
      "Batch:  111 , correct:  3 , loss:  2.26827864360236\n",
      "Batch:  112 , correct:  1 , loss:  2.2683827193372976\n",
      "Batch:  113 , correct:  5 , loss:  2.2684869510878856\n",
      "Batch:  114 , correct:  2 , loss:  2.268591945145654\n",
      "Batch:  115 , correct:  5 , loss:  2.268697019107595\n",
      "Batch:  116 , correct:  2 , loss:  2.268799333317532\n",
      "Batch:  117 , correct:  5 , loss:  2.2689044676434778\n",
      "Batch:  118 , correct:  2 , loss:  2.2690064814487405\n",
      "Batch:  119 , correct:  3 , loss:  2.2691116721117726\n",
      "Batch:  120 , correct:  4 , loss:  2.2692120123102875\n",
      "Batch:  121 , correct:  1 , loss:  2.2683170969013893\n",
      "Batch:  122 , correct:  4 , loss:  2.2684202273306173\n",
      "Batch:  123 , correct:  2 , loss:  2.268524456952534\n",
      "Batch:  124 , correct:  1 , loss:  2.2686284873536713\n",
      "Batch:  125 , correct:  4 , loss:  2.268730556859153\n",
      "Batch:  126 , correct:  5 , loss:  2.2688348252218544\n",
      "Batch:  127 , correct:  4 , loss:  2.2689391769610356\n",
      "Batch:  128 , correct:  2 , loss:  2.2690436042349327\n",
      "Batch:  129 , correct:  4 , loss:  2.269145691216925\n",
      "Batch:  130 , correct:  2 , loss:  2.2692508586824176\n",
      "Batch:  131 , correct:  1 , loss:  2.269354838777112\n",
      "Batch:  132 , correct:  4 , loss:  2.2694570295959\n",
      "Batch:  133 , correct:  2 , loss:  2.2685620660287498\n",
      "Batch:  134 , correct:  2 , loss:  2.268664257686219\n",
      "Batch:  135 , correct:  4 , loss:  2.2687683579118776\n",
      "Batch:  136 , correct:  2 , loss:  2.2688706689408757\n",
      "Batch:  137 , correct:  7 , loss:  2.2689758919138945\n",
      "Batch:  138 , correct:  1 , loss:  2.2690800354935763\n",
      "Batch:  139 , correct:  3 , loss:  2.269184258213762\n",
      "Batch:  140 , correct:  1 , loss:  2.2692895181131343\n",
      "Batch:  141 , correct:  3 , loss:  2.2693938735527506\n",
      "Batch:  142 , correct:  7 , loss:  2.2694978936336407\n",
      "Batch:  143 , correct:  4 , loss:  2.2695999959694997\n",
      "Batch:  144 , correct:  4 , loss:  2.2697033224456282\n",
      "Batch:  145 , correct:  7 , loss:  2.268808307797023\n",
      "Batch:  146 , correct:  2 , loss:  2.268908520745553\n",
      "Batch:  147 , correct:  4 , loss:  2.269011993118756\n",
      "Batch:  148 , correct:  3 , loss:  2.2691162353596908\n",
      "Batch:  149 , correct:  1 , loss:  2.2692215083322735\n",
      "Total Loss =  2.26837791011848\n",
      "\n",
      "EPOCH:  7\n",
      "Batch:  0 , correct:  2 , loss:  2.269325560332858\n",
      "Batch:  1 , correct:  7 , loss:  2.269430896525186\n",
      "Batch:  2 , correct:  3 , loss:  2.269535152540244\n",
      "Batch:  3 , correct:  3 , loss:  2.269640542433202\n",
      "Batch:  4 , correct:  3 , loss:  2.2697426822021063\n",
      "Batch:  5 , correct:  3 , loss:  2.2698447376748767\n",
      "Batch:  6 , correct:  5 , loss:  2.2699490111819123\n",
      "Batch:  7 , correct:  6 , loss:  2.270054419744786\n",
      "Batch:  8 , correct:  4 , loss:  2.270158752754653\n",
      "Batch:  9 , correct:  5 , loss:  2.2702642198463336\n",
      "Batch:  10 , correct:  3 , loss:  2.270368168146493\n",
      "Batch:  11 , correct:  0 , loss:  2.2704681728337444\n",
      "Batch:  12 , correct:  4 , loss:  2.2705725206170233\n",
      "Batch:  13 , correct:  2 , loss:  2.269677320003981\n",
      "Batch:  14 , correct:  2 , loss:  2.269780636067596\n",
      "Batch:  15 , correct:  2 , loss:  2.26988076546594\n",
      "Batch:  16 , correct:  8 , loss:  2.269982863862539\n",
      "Batch:  17 , correct:  2 , loss:  2.2690877842439994\n",
      "Batch:  18 , correct:  2 , loss:  2.2691900350285703\n",
      "Batch:  19 , correct:  2 , loss:  2.2692941065414716\n",
      "Batch:  20 , correct:  5 , loss:  2.2693982676060216\n",
      "Batch:  21 , correct:  4 , loss:  2.2695025074157997\n",
      "Batch:  22 , correct:  4 , loss:  2.269606828510636\n",
      "Batch:  23 , correct:  2 , loss:  2.2697112548094642\n",
      "Batch:  24 , correct:  1 , loss:  2.2688162313007023\n",
      "Batch:  25 , correct:  2 , loss:  2.268916455720102\n",
      "Batch:  26 , correct:  6 , loss:  2.2690190955822485\n",
      "Batch:  27 , correct:  3 , loss:  2.2691236413280307\n",
      "Batch:  28 , correct:  4 , loss:  2.2692280403636467\n",
      "Batch:  29 , correct:  3 , loss:  2.2693313526706764\n",
      "Batch:  30 , correct:  3 , loss:  2.2694315775185574\n",
      "Batch:  31 , correct:  7 , loss:  2.2695361428291103\n",
      "Batch:  32 , correct:  5 , loss:  2.269639495424489\n",
      "Batch:  33 , correct:  3 , loss:  2.2697448694340245\n",
      "Batch:  34 , correct:  6 , loss:  2.269847442311171\n",
      "Batch:  35 , correct:  0 , loss:  2.2689523896008335\n",
      "Batch:  36 , correct:  0 , loss:  2.269057053020432\n",
      "Batch:  37 , correct:  3 , loss:  2.2691592259245827\n",
      "Batch:  38 , correct:  2 , loss:  2.2692646993651704\n",
      "Batch:  39 , correct:  1 , loss:  2.269369070859525\n",
      "Batch:  40 , correct:  4 , loss:  2.269473520874777\n",
      "Batch:  41 , correct:  4 , loss:  2.2695768870347974\n",
      "Batch:  42 , correct:  3 , loss:  2.269682374827053\n",
      "Batch:  43 , correct:  0 , loss:  2.2697849658539555\n",
      "Batch:  44 , correct:  2 , loss:  2.269887632082516\n",
      "Batch:  45 , correct:  5 , loss:  2.269992088757478\n",
      "Batch:  46 , correct:  3 , loss:  2.2700921851892444\n",
      "Batch:  47 , correct:  1 , loss:  2.270194893350213\n",
      "Batch:  48 , correct:  3 , loss:  2.270295056452206\n",
      "Batch:  49 , correct:  4 , loss:  2.270399523385139\n",
      "Batch:  50 , correct:  2 , loss:  2.270502804399456\n",
      "Batch:  51 , correct:  4 , loss:  2.270606164790297\n",
      "Batch:  52 , correct:  4 , loss:  2.2707077728157565\n",
      "Batch:  53 , correct:  2 , loss:  2.269812546101162\n",
      "Batch:  54 , correct:  0 , loss:  2.2699179882530887\n",
      "Batch:  55 , correct:  2 , loss:  2.2700225358711137\n",
      "Batch:  56 , correct:  0 , loss:  2.270126980792832\n",
      "Batch:  57 , correct:  5 , loss:  2.2702324656291983\n",
      "Batch:  58 , correct:  2 , loss:  2.2703344057084713\n",
      "Batch:  59 , correct:  5 , loss:  2.2704364300958333\n",
      "Batch:  60 , correct:  4 , loss:  2.2705400514837644\n",
      "Batch:  61 , correct:  3 , loss:  2.270644494925662\n",
      "Batch:  62 , correct:  3 , loss:  2.2707470992589545\n",
      "Batch:  63 , correct:  4 , loss:  2.2708486831351955\n",
      "Batch:  64 , correct:  5 , loss:  2.2699534228906493\n",
      "Batch:  65 , correct:  2 , loss:  2.2700568018390714\n",
      "Batch:  66 , correct:  3 , loss:  2.270162297892448\n",
      "Batch:  67 , correct:  4 , loss:  2.27026681669286\n",
      "Batch:  68 , correct:  2 , loss:  2.2703688563965008\n",
      "Batch:  69 , correct:  4 , loss:  2.2704743911668954\n",
      "Batch:  70 , correct:  0 , loss:  2.269579210247008\n",
      "Batch:  71 , correct:  1 , loss:  2.2696838497425214\n",
      "Batch:  72 , correct:  5 , loss:  2.269788344399483\n",
      "Batch:  73 , correct:  2 , loss:  2.2698904806155444\n",
      "Batch:  74 , correct:  3 , loss:  2.2699905165085084\n",
      "Batch:  75 , correct:  3 , loss:  2.270096132836825\n",
      "Batch:  76 , correct:  2 , loss:  2.2701987545899365\n",
      "Batch:  77 , correct:  4 , loss:  2.270301457610279\n",
      "Batch:  78 , correct:  3 , loss:  2.2704047655600026\n",
      "Batch:  79 , correct:  1 , loss:  2.2705103971445846\n",
      "Batch:  80 , correct:  3 , loss:  2.27061249411634\n",
      "Batch:  81 , correct:  2 , loss:  2.270718182019502\n",
      "Batch:  82 , correct:  2 , loss:  2.270820334446063\n",
      "Batch:  83 , correct:  2 , loss:  2.27092029063051\n",
      "Batch:  84 , correct:  3 , loss:  2.2710246351446353\n",
      "Batch:  85 , correct:  3 , loss:  2.2711290522201897\n",
      "Batch:  86 , correct:  2 , loss:  2.2712316713984047\n",
      "Batch:  87 , correct:  4 , loss:  2.2713361186580143\n",
      "Batch:  88 , correct:  0 , loss:  2.2714406412401997\n",
      "Batch:  89 , correct:  2 , loss:  2.2715439868808636\n",
      "Batch:  90 , correct:  3 , loss:  2.2716485743798502\n",
      "Batch:  91 , correct:  3 , loss:  2.2717541159405217\n",
      "Batch:  92 , correct:  1 , loss:  2.2718597406736447\n",
      "Batch:  93 , correct:  1 , loss:  2.271961014451318\n",
      "Batch:  94 , correct:  1 , loss:  2.271065551726958\n",
      "Batch:  95 , correct:  3 , loss:  2.2701702665029853\n",
      "Batch:  96 , correct:  4 , loss:  2.2702729807397506\n",
      "Batch:  97 , correct:  5 , loss:  2.2703729923083404\n",
      "Batch:  98 , correct:  4 , loss:  2.2704744711640314\n",
      "Batch:  99 , correct:  5 , loss:  2.270580263038739\n",
      "Batch:  100 , correct:  5 , loss:  2.2706817986583703\n",
      "Batch:  101 , correct:  2 , loss:  2.270784515543002\n",
      "Batch:  102 , correct:  2 , loss:  2.2708887544305028\n",
      "Batch:  103 , correct:  7 , loss:  2.270992122714028\n",
      "Batch:  104 , correct:  4 , loss:  2.2710920840205513\n",
      "Batch:  105 , correct:  2 , loss:  2.2711978461226523\n",
      "Batch:  106 , correct:  4 , loss:  2.2712993664418226\n",
      "Batch:  107 , correct:  4 , loss:  2.271403604107257\n",
      "Batch:  108 , correct:  3 , loss:  2.2715055277759784\n",
      "Batch:  109 , correct:  2 , loss:  2.2716075337511263\n",
      "Batch:  110 , correct:  4 , loss:  2.2717096181999694\n",
      "Batch:  111 , correct:  3 , loss:  2.2718126035330295\n",
      "Batch:  112 , correct:  5 , loss:  2.271918324196519\n",
      "Batch:  113 , correct:  5 , loss:  2.271022863888181\n",
      "Batch:  114 , correct:  7 , loss:  2.271125993599224\n",
      "Batch:  115 , correct:  2 , loss:  2.270230696125979\n",
      "Batch:  116 , correct:  5 , loss:  2.270336635378599\n",
      "Batch:  117 , correct:  3 , loss:  2.269441498513017\n",
      "Batch:  118 , correct:  3 , loss:  2.2695438254969047\n",
      "Batch:  119 , correct:  4 , loss:  2.2696471568864287\n",
      "Batch:  120 , correct:  5 , loss:  2.269747205257739\n",
      "Batch:  121 , correct:  3 , loss:  2.268852190759935\n",
      "Batch:  122 , correct:  2 , loss:  2.268955678605357\n",
      "Batch:  123 , correct:  4 , loss:  2.269061772311751\n",
      "Batch:  124 , correct:  1 , loss:  2.269165315262123\n",
      "Batch:  125 , correct:  2 , loss:  2.2692714697185856\n",
      "Batch:  126 , correct:  4 , loss:  2.2693738353433774\n",
      "Batch:  127 , correct:  8 , loss:  2.2694800523795546\n",
      "Batch:  128 , correct:  2 , loss:  2.2695845527945897\n",
      "Batch:  129 , correct:  3 , loss:  2.2696872084455006\n",
      "Batch:  130 , correct:  0 , loss:  2.2697934632185137\n",
      "Batch:  131 , correct:  4 , loss:  2.26989348634223\n",
      "Batch:  132 , correct:  4 , loss:  2.2699977123332715\n",
      "Batch:  133 , correct:  4 , loss:  2.2700991859897353\n",
      "Batch:  134 , correct:  5 , loss:  2.270199252175492\n",
      "Batch:  135 , correct:  2 , loss:  2.2702993953258686\n",
      "Batch:  136 , correct:  2 , loss:  2.2704009114524455\n",
      "Batch:  137 , correct:  3 , loss:  2.2695057669555676\n",
      "Batch:  138 , correct:  1 , loss:  2.269609220925603\n",
      "Batch:  139 , correct:  2 , loss:  2.2697135051755537\n",
      "Batch:  140 , correct:  2 , loss:  2.2698170257855486\n",
      "Batch:  141 , correct:  5 , loss:  2.269923253115756\n",
      "Batch:  142 , correct:  4 , loss:  2.269028205945327\n",
      "Batch:  143 , correct:  2 , loss:  2.268133340100725\n",
      "Batch:  144 , correct:  2 , loss:  2.267238661280314\n",
      "Batch:  145 , correct:  7 , loss:  2.267345212084336\n",
      "Batch:  146 , correct:  0 , loss:  2.267449008875656\n",
      "Batch:  147 , correct:  3 , loss:  2.2675556261621987\n",
      "Batch:  148 , correct:  7 , loss:  2.2676574001268\n",
      "Batch:  149 , correct:  2 , loss:  2.2677640721831613\n",
      "Total Loss =  2.2700218184689374\n",
      "\n",
      "EPOCH:  8\n",
      "Batch:  0 , correct:  4 , loss:  2.267867885527682\n",
      "Batch:  1 , correct:  1 , loss:  2.2679696950883494\n",
      "Batch:  2 , correct:  3 , loss:  2.2680720394930485\n",
      "Batch:  3 , correct:  8 , loss:  2.2671773680318164\n",
      "Batch:  4 , correct:  5 , loss:  2.266282880347526\n",
      "Batch:  5 , correct:  4 , loss:  2.2653885833438085\n",
      "Batch:  6 , correct:  7 , loss:  2.264494472182592\n",
      "Batch:  7 , correct:  4 , loss:  2.264595087572669\n",
      "Batch:  8 , correct:  0 , loss:  2.26469979230373\n",
      "Batch:  9 , correct:  2 , loss:  2.264804563534335\n",
      "Batch:  10 , correct:  3 , loss:  2.2649093282153165\n",
      "Batch:  11 , correct:  0 , loss:  2.265013426327501\n",
      "Batch:  12 , correct:  4 , loss:  2.2651140383081207\n",
      "Batch:  13 , correct:  4 , loss:  2.2652147248722567\n",
      "Batch:  14 , correct:  3 , loss:  2.2643206558153173\n",
      "Batch:  15 , correct:  2 , loss:  2.264425514888589\n",
      "Batch:  16 , correct:  5 , loss:  2.2645284443681106\n",
      "Batch:  17 , correct:  3 , loss:  2.264631447234219\n",
      "Batch:  18 , correct:  3 , loss:  2.2647348863310888\n",
      "Batch:  19 , correct:  2 , loss:  2.2638409074688832\n",
      "Batch:  20 , correct:  4 , loss:  2.2639458707886457\n",
      "Batch:  21 , correct:  6 , loss:  2.2630520560571634\n",
      "Batch:  22 , correct:  2 , loss:  2.2631548054272894\n",
      "Batch:  23 , correct:  3 , loss:  2.2632579924079272\n",
      "Batch:  24 , correct:  2 , loss:  2.262364321097733\n",
      "Batch:  25 , correct:  2 , loss:  2.2624692755008997\n",
      "Batch:  26 , correct:  1 , loss:  2.2625714977420675\n",
      "Batch:  27 , correct:  5 , loss:  2.2626724197495647\n",
      "Batch:  28 , correct:  3 , loss:  2.262775708384371\n",
      "Batch:  29 , correct:  0 , loss:  2.262880676340218\n",
      "Batch:  30 , correct:  3 , loss:  2.262987617813438\n",
      "Batch:  31 , correct:  4 , loss:  2.2630909423079064\n",
      "Batch:  32 , correct:  1 , loss:  2.263197941119838\n",
      "Batch:  33 , correct:  3 , loss:  2.2633006669859443\n",
      "Batch:  34 , correct:  4 , loss:  2.263405634451236\n",
      "Batch:  35 , correct:  5 , loss:  2.263509129307111\n",
      "Batch:  36 , correct:  1 , loss:  2.263612449731415\n",
      "Batch:  37 , correct:  3 , loss:  2.263715845494206\n",
      "Batch:  38 , correct:  5 , loss:  2.2638208288192696\n",
      "Batch:  39 , correct:  4 , loss:  2.262927044278656\n",
      "Batch:  40 , correct:  4 , loss:  2.2630291853476034\n",
      "Batch:  41 , correct:  2 , loss:  2.26313002077438\n",
      "Batch:  42 , correct:  5 , loss:  2.2632309386876344\n",
      "Batch:  43 , correct:  5 , loss:  2.2633379109538296\n",
      "Batch:  44 , correct:  6 , loss:  2.2624442215129403\n",
      "Batch:  45 , correct:  4 , loss:  2.2625483356585736\n",
      "Batch:  46 , correct:  1 , loss:  2.262651911755358\n",
      "Batch:  47 , correct:  1 , loss:  2.2627555639323997\n",
      "Batch:  48 , correct:  4 , loss:  2.2628577491053923\n",
      "Batch:  49 , correct:  3 , loss:  2.2629626188457816\n",
      "Batch:  50 , correct:  5 , loss:  2.2630653045924545\n",
      "Batch:  51 , correct:  1 , loss:  2.2631702391620543\n",
      "Batch:  52 , correct:  1 , loss:  2.2632729784607815\n",
      "Batch:  53 , correct:  4 , loss:  2.263377951933884\n",
      "Batch:  54 , correct:  1 , loss:  2.2634788249201474\n",
      "Batch:  55 , correct:  3 , loss:  2.263582164002535\n",
      "Batch:  56 , correct:  4 , loss:  2.2636890695580814\n",
      "Batch:  57 , correct:  5 , loss:  2.263796053018444\n",
      "Batch:  58 , correct:  3 , loss:  2.2639009362632576\n",
      "Batch:  59 , correct:  1 , loss:  2.264004287005748\n",
      "Batch:  60 , correct:  3 , loss:  2.2641092190095575\n",
      "Batch:  61 , correct:  4 , loss:  2.2642118792355417\n",
      "Batch:  62 , correct:  4 , loss:  2.2643152666481834\n",
      "Batch:  63 , correct:  8 , loss:  2.264420234254043\n",
      "Batch:  64 , correct:  5 , loss:  2.2635263202996447\n",
      "Batch:  65 , correct:  3 , loss:  2.2636333407783487\n",
      "Batch:  66 , correct:  4 , loss:  2.2627395878907324\n",
      "Batch:  67 , correct:  2 , loss:  2.2628404946653156\n",
      "Batch:  68 , correct:  2 , loss:  2.2629425761488386\n",
      "Batch:  69 , correct:  0 , loss:  2.263045373973868\n",
      "Batch:  70 , correct:  4 , loss:  2.263148907662405\n",
      "Batch:  71 , correct:  5 , loss:  2.2632540166663517\n",
      "Batch:  72 , correct:  1 , loss:  2.2633576031360167\n",
      "Batch:  73 , correct:  5 , loss:  2.263464659128478\n",
      "Batch:  74 , correct:  0 , loss:  2.2635695140219148\n",
      "Batch:  75 , correct:  4 , loss:  2.263673134987121\n",
      "Batch:  76 , correct:  3 , loss:  2.2637739645358836\n",
      "Batch:  77 , correct:  2 , loss:  2.2638759626034477\n",
      "Batch:  78 , correct:  4 , loss:  2.2629821564786754\n",
      "Batch:  79 , correct:  2 , loss:  2.263087116349477\n",
      "Batch:  80 , correct:  5 , loss:  2.2631942255613926\n",
      "Batch:  81 , correct:  4 , loss:  2.263299326776022\n",
      "Batch:  82 , correct:  2 , loss:  2.2634043156391\n",
      "Batch:  83 , correct:  4 , loss:  2.2635076340356073\n",
      "Batch:  84 , correct:  5 , loss:  2.262613913888058\n",
      "Batch:  85 , correct:  2 , loss:  2.262721117482906\n",
      "Batch:  86 , correct:  3 , loss:  2.262826224387629\n",
      "Batch:  87 , correct:  7 , loss:  2.262927130505842\n",
      "Batch:  88 , correct:  1 , loss:  2.2630305472761303\n",
      "Batch:  89 , correct:  3 , loss:  2.26313419027757\n",
      "Batch:  90 , correct:  9 , loss:  2.2622405397333853\n",
      "Batch:  91 , correct:  5 , loss:  2.2623441292630155\n",
      "Batch:  92 , correct:  3 , loss:  2.262446118097952\n",
      "Batch:  93 , correct:  3 , loss:  2.2625487981077743\n",
      "Batch:  94 , correct:  5 , loss:  2.2626522487230027\n",
      "Batch:  95 , correct:  2 , loss:  2.2627557826067313\n",
      "Batch:  96 , correct:  5 , loss:  2.2628569858699876\n",
      "Batch:  97 , correct:  3 , loss:  2.262961983802618\n",
      "Batch:  98 , correct:  0 , loss:  2.2630656656195827\n",
      "Batch:  99 , correct:  0 , loss:  2.263168321427504\n",
      "Batch:  100 , correct:  3 , loss:  2.2632733582511118\n",
      "Batch:  101 , correct:  1 , loss:  2.2633745551472324\n",
      "Batch:  102 , correct:  1 , loss:  2.263481640171567\n",
      "Batch:  103 , correct:  4 , loss:  2.2635835059704412\n",
      "Batch:  104 , correct:  3 , loss:  2.2636906478510403\n",
      "Batch:  105 , correct:  4 , loss:  2.263793279377961\n",
      "Batch:  106 , correct:  1 , loss:  2.2638944729903248\n",
      "Batch:  107 , correct:  6 , loss:  2.263998072753706\n",
      "Batch:  108 , correct:  2 , loss:  2.2641030469482346\n",
      "Batch:  109 , correct:  3 , loss:  2.264205696200451\n",
      "Batch:  110 , correct:  1 , loss:  2.2643128136332225\n",
      "Batch:  111 , correct:  1 , loss:  2.26441749083781\n",
      "Batch:  112 , correct:  3 , loss:  2.2645207739489446\n",
      "Batch:  113 , correct:  3 , loss:  2.264624351727096\n",
      "Batch:  114 , correct:  0 , loss:  2.2647269924530558\n",
      "Batch:  115 , correct:  1 , loss:  2.264829717035802\n",
      "Batch:  116 , correct:  3 , loss:  2.264936807658194\n",
      "Batch:  117 , correct:  1 , loss:  2.265043981079431\n",
      "Batch:  118 , correct:  4 , loss:  2.2651486137911405\n",
      "Batch:  119 , correct:  3 , loss:  2.265251837343929\n",
      "Batch:  120 , correct:  5 , loss:  2.2653550507146742\n",
      "Batch:  121 , correct:  0 , loss:  2.2654622391327752\n",
      "Batch:  122 , correct:  1 , loss:  2.2655670155890175\n",
      "Batch:  123 , correct:  7 , loss:  2.265670484269768\n",
      "Batch:  124 , correct:  2 , loss:  2.265773696473229\n",
      "Batch:  125 , correct:  2 , loss:  2.265876911401879\n",
      "Batch:  126 , correct:  2 , loss:  2.265980191513776\n",
      "Batch:  127 , correct:  4 , loss:  2.2660816604130747\n",
      "Batch:  128 , correct:  3 , loss:  2.2661888091792215\n",
      "Batch:  129 , correct:  1 , loss:  2.2662920363718353\n",
      "Batch:  130 , correct:  5 , loss:  2.266396515377122\n",
      "Batch:  131 , correct:  3 , loss:  2.2664998036584385\n",
      "Batch:  132 , correct:  5 , loss:  2.266603055389613\n",
      "Batch:  133 , correct:  3 , loss:  2.2657088069797027\n",
      "Batch:  134 , correct:  5 , loss:  2.2658160266085585\n",
      "Batch:  135 , correct:  5 , loss:  2.2659175380715366\n",
      "Batch:  136 , correct:  4 , loss:  2.2660200294804818\n",
      "Batch:  137 , correct:  3 , loss:  2.2651258987000698\n",
      "Batch:  138 , correct:  2 , loss:  2.2652267457889694\n",
      "Batch:  139 , correct:  3 , loss:  2.26533147359738\n",
      "Batch:  140 , correct:  3 , loss:  2.265436280152799\n",
      "Batch:  141 , correct:  5 , loss:  2.26554355207536\n",
      "Batch:  142 , correct:  3 , loss:  2.2656480866128783\n",
      "Batch:  143 , correct:  2 , loss:  2.265752929240385\n",
      "Batch:  144 , correct:  0 , loss:  2.2658537488998505\n",
      "Batch:  145 , correct:  1 , loss:  2.265957071530269\n",
      "Batch:  146 , correct:  4 , loss:  2.266064340774843\n",
      "Batch:  147 , correct:  7 , loss:  2.2661676038619913\n",
      "Batch:  148 , correct:  2 , loss:  2.265273445136054\n",
      "Batch:  149 , correct:  2 , loss:  2.264379467786592\n",
      "Total Loss =  2.264139582853733\n",
      "\n",
      "EPOCH:  9\n",
      "Batch:  0 , correct:  3 , loss:  2.2644810783790716\n",
      "Batch:  1 , correct:  3 , loss:  2.2645844456803994\n",
      "Batch:  2 , correct:  2 , loss:  2.2646878919927063\n",
      "Batch:  3 , correct:  1 , loss:  2.264791417533356\n",
      "Batch:  4 , correct:  4 , loss:  2.2648950224834126\n",
      "Batch:  5 , correct:  2 , loss:  2.2649959257386874\n",
      "Batch:  6 , correct:  5 , loss:  2.2651004729373323\n",
      "Batch:  7 , correct:  6 , loss:  2.2652050990050987\n",
      "Batch:  8 , correct:  2 , loss:  2.265307546207517\n",
      "Batch:  9 , correct:  2 , loss:  2.264413563446033\n",
      "Batch:  10 , correct:  1 , loss:  2.2645161732623365\n",
      "Batch:  11 , correct:  5 , loss:  2.264620921492019\n",
      "Batch:  12 , correct:  4 , loss:  2.2647243031090305\n",
      "Batch:  13 , correct:  4 , loss:  2.2648276442240767\n",
      "Batch:  14 , correct:  3 , loss:  2.264930269590672\n",
      "Batch:  15 , correct:  1 , loss:  2.2650336688242025\n",
      "Batch:  16 , correct:  4 , loss:  2.2651345531498137\n",
      "Batch:  17 , correct:  4 , loss:  2.2652393032408047\n",
      "Batch:  18 , correct:  2 , loss:  2.265342824342881\n",
      "Batch:  19 , correct:  0 , loss:  2.2654461622210813\n",
      "Batch:  20 , correct:  1 , loss:  2.2655533343189203\n",
      "Batch:  21 , correct:  6 , loss:  2.2656581106164078\n",
      "Batch:  22 , correct:  2 , loss:  2.2657627265882985\n",
      "Batch:  23 , correct:  3 , loss:  2.265865370525661\n",
      "Batch:  24 , correct:  5 , loss:  2.2659681207062374\n",
      "Batch:  25 , correct:  2 , loss:  2.266071688697224\n",
      "Batch:  26 , correct:  3 , loss:  2.2661722879733657\n",
      "Batch:  27 , correct:  2 , loss:  2.2662736213340917\n",
      "Batch:  28 , correct:  3 , loss:  2.2653791431827957\n",
      "Batch:  29 , correct:  4 , loss:  2.2654864486626476\n",
      "Batch:  30 , correct:  1 , loss:  2.2655892864981904\n",
      "Batch:  31 , correct:  2 , loss:  2.2656899899243395\n",
      "Batch:  32 , correct:  3 , loss:  2.265797347261261\n",
      "Batch:  33 , correct:  4 , loss:  2.2659002156337835\n",
      "Batch:  34 , correct:  3 , loss:  2.266001609200627\n",
      "Batch:  35 , correct:  4 , loss:  2.266104540086303\n",
      "Batch:  36 , correct:  8 , loss:  2.2662091657650594\n",
      "Batch:  37 , correct:  4 , loss:  2.266311874389968\n",
      "Batch:  38 , correct:  3 , loss:  2.266415327304819\n",
      "Batch:  39 , correct:  3 , loss:  2.2665182720051646\n",
      "Batch:  40 , correct:  3 , loss:  2.266621300643794\n",
      "Batch:  41 , correct:  4 , loss:  2.266725970745216\n",
      "Batch:  42 , correct:  5 , loss:  2.2668286697523192\n",
      "Batch:  43 , correct:  4 , loss:  2.2669321207632347\n",
      "Batch:  44 , correct:  3 , loss:  2.2670366836662277\n",
      "Batch:  45 , correct:  3 , loss:  2.267140039070828\n",
      "Batch:  46 , correct:  5 , loss:  2.2672435253340844\n",
      "Batch:  47 , correct:  4 , loss:  2.26734651322021\n",
      "Batch:  48 , correct:  5 , loss:  2.2664518020517574\n",
      "Batch:  49 , correct:  2 , loss:  2.266556513877169\n",
      "Batch:  50 , correct:  6 , loss:  2.266661153234965\n",
      "Batch:  51 , correct:  6 , loss:  2.266762402780119\n",
      "Batch:  52 , correct:  4 , loss:  2.2668654898763334\n",
      "Batch:  53 , correct:  7 , loss:  2.266969036921004\n",
      "Batch:  54 , correct:  2 , loss:  2.2670724081797458\n",
      "Batch:  55 , correct:  2 , loss:  2.267172829225083\n",
      "Batch:  56 , correct:  2 , loss:  2.2672754449124835\n",
      "Batch:  57 , correct:  3 , loss:  2.267378860168188\n",
      "Batch:  58 , correct:  10 , loss:  2.2674793169630596\n",
      "Batch:  59 , correct:  5 , loss:  2.2675798475217475\n",
      "Batch:  60 , correct:  2 , loss:  2.2666851039416454\n",
      "Batch:  61 , correct:  3 , loss:  2.2667886905264285\n",
      "Batch:  62 , correct:  2 , loss:  2.265894095453597\n",
      "Batch:  63 , correct:  3 , loss:  2.2659972688586802\n",
      "Batch:  64 , correct:  5 , loss:  2.266100515325079\n",
      "Batch:  65 , correct:  4 , loss:  2.266205196399293\n",
      "Batch:  66 , correct:  3 , loss:  2.2663084924815533\n",
      "Batch:  67 , correct:  3 , loss:  2.266409178961093\n",
      "Batch:  68 , correct:  3 , loss:  2.265514670657658\n",
      "Batch:  69 , correct:  1 , loss:  2.2656193262185553\n",
      "Batch:  70 , correct:  2 , loss:  2.2657201405374328\n",
      "Batch:  71 , correct:  4 , loss:  2.2658271827403698\n",
      "Batch:  72 , correct:  1 , loss:  2.265930566389733\n",
      "Batch:  73 , correct:  2 , loss:  2.266035240808935\n",
      "Batch:  74 , correct:  4 , loss:  2.266136076408797\n",
      "Batch:  75 , correct:  2 , loss:  2.266239529001444\n",
      "Batch:  76 , correct:  1 , loss:  2.2663441841850407\n",
      "Batch:  77 , correct:  4 , loss:  2.2664467852581667\n",
      "Batch:  78 , correct:  3 , loss:  2.2665502778774043\n",
      "Batch:  79 , correct:  6 , loss:  2.2666572582032947\n",
      "Batch:  80 , correct:  2 , loss:  2.2667580657582582\n",
      "Batch:  81 , correct:  1 , loss:  2.2668613643891318\n",
      "Batch:  82 , correct:  6 , loss:  2.2669639623589393\n",
      "Batch:  83 , correct:  4 , loss:  2.2670674489742235\n",
      "Batch:  84 , correct:  4 , loss:  2.267168446322501\n",
      "Batch:  85 , correct:  6 , loss:  2.2672692583338705\n",
      "Batch:  86 , correct:  2 , loss:  2.2663745784074747\n",
      "Batch:  87 , correct:  5 , loss:  2.266477519031465\n",
      "Batch:  88 , correct:  4 , loss:  2.2655833010681405\n",
      "Batch:  89 , correct:  6 , loss:  2.2656866792072035\n",
      "Batch:  90 , correct:  4 , loss:  2.2657901412830928\n",
      "Batch:  91 , correct:  0 , loss:  2.2658946825714765\n",
      "Batch:  92 , correct:  1 , loss:  2.266001565375248\n",
      "Batch:  93 , correct:  4 , loss:  2.2661050630039\n",
      "Batch:  94 , correct:  4 , loss:  2.2662060932779755\n",
      "Batch:  95 , correct:  3 , loss:  2.266307200506727\n",
      "Batch:  96 , correct:  2 , loss:  2.266410463963394\n",
      "Batch:  97 , correct:  3 , loss:  2.266513638703725\n",
      "Batch:  98 , correct:  2 , loss:  2.2666180018989666\n",
      "Batch:  99 , correct:  2 , loss:  2.2667214768818913\n",
      "Batch:  100 , correct:  4 , loss:  2.2668250352904273\n",
      "Batch:  101 , correct:  5 , loss:  2.2669294671987696\n",
      "Batch:  102 , correct:  0 , loss:  2.2670304122800053\n",
      "Batch:  103 , correct:  4 , loss:  2.2671334334134485\n",
      "Batch:  104 , correct:  2 , loss:  2.2672379063639005\n",
      "Batch:  105 , correct:  0 , loss:  2.267340982233828\n",
      "Batch:  106 , correct:  4 , loss:  2.2674476713796876\n",
      "Batch:  107 , correct:  2 , loss:  2.2665532536538677\n",
      "Batch:  108 , correct:  2 , loss:  2.2666564059088685\n",
      "Batch:  109 , correct:  3 , loss:  2.266760741939598\n",
      "Batch:  110 , correct:  3 , loss:  2.266867551601285\n",
      "Batch:  111 , correct:  6 , loss:  2.266971946914538\n",
      "Batch:  112 , correct:  3 , loss:  2.266077625980202\n",
      "Batch:  113 , correct:  2 , loss:  2.2651834856598345\n",
      "Batch:  114 , correct:  2 , loss:  2.265284615310883\n",
      "Batch:  115 , correct:  3 , loss:  2.2653916259792584\n",
      "Batch:  116 , correct:  1 , loss:  2.2654948791888794\n",
      "Batch:  117 , correct:  5 , loss:  2.265595926089146\n",
      "Batch:  118 , correct:  1 , loss:  2.265699173570792\n",
      "Batch:  119 , correct:  2 , loss:  2.265802507534915\n",
      "Batch:  120 , correct:  2 , loss:  2.2659095096232167\n",
      "Batch:  121 , correct:  3 , loss:  2.266010597526978\n",
      "Batch:  122 , correct:  4 , loss:  2.266113831296664\n",
      "Batch:  123 , correct:  2 , loss:  2.2662169411483744\n",
      "Batch:  124 , correct:  4 , loss:  2.2663202688842916\n",
      "Batch:  125 , correct:  2 , loss:  2.2664237139910854\n",
      "Batch:  126 , correct:  3 , loss:  2.266528104014342\n",
      "Batch:  127 , correct:  1 , loss:  2.2666313355168137\n",
      "Batch:  128 , correct:  7 , loss:  2.2667344391828035\n",
      "Batch:  129 , correct:  3 , loss:  2.266835467541229\n",
      "Batch:  130 , correct:  3 , loss:  2.2669386341984175\n",
      "Batch:  131 , correct:  4 , loss:  2.26704205594158\n",
      "Batch:  132 , correct:  2 , loss:  2.2671452808858796\n",
      "Batch:  133 , correct:  0 , loss:  2.2672495614540833\n",
      "Batch:  134 , correct:  0 , loss:  2.267352745624582\n",
      "Batch:  135 , correct:  1 , loss:  2.267456013042858\n",
      "Batch:  136 , correct:  3 , loss:  2.2675591997455755\n",
      "Batch:  137 , correct:  1 , loss:  2.2676599370027253\n",
      "Batch:  138 , correct:  1 , loss:  2.26776087863915\n",
      "Batch:  139 , correct:  1 , loss:  2.2678642363827324\n",
      "Batch:  140 , correct:  4 , loss:  2.267967443638821\n",
      "Batch:  141 , correct:  3 , loss:  2.2680741069310204\n",
      "Batch:  142 , correct:  5 , loss:  2.268177370554505\n",
      "Batch:  143 , correct:  5 , loss:  2.2682815110209185\n",
      "Batch:  144 , correct:  3 , loss:  2.2673869210913815\n",
      "Batch:  145 , correct:  4 , loss:  2.2674876990445703\n",
      "Batch:  146 , correct:  3 , loss:  2.267590842993201\n",
      "Batch:  147 , correct:  3 , loss:  2.267694069342017\n",
      "Batch:  148 , correct:  3 , loss:  2.2677974114468777\n",
      "Batch:  149 , correct:  3 , loss:  2.2679005910933316\n",
      "Total Loss =  2.2663579386285813\n",
      "\n"
     ]
    }
   ],
   "source": [
    "per_epoch_parameters = train(x_me, y_me, 10, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7546abf-9019-4e13-8d53-539f0bdb4721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ead95e4-5f9e-449c-8b22-086e1dc9eba5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be516b51-d930-4ccc-b705-55a82670ff32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
