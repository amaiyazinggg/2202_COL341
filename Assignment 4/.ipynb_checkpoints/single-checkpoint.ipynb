{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b47422ed-0f4b-4c4e-b75e-2f4ce1751337",
   "metadata": {},
   "source": [
    "<center>\n",
    "    \n",
    "# COL341 Spring 2023 <br> Assignment 4 : CNN  \n",
    "## Part 1\n",
    "### Amaiya Singhal\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0a596da8-027d-4479-95cb-006662857d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd265432-b870-4f46-80e2-e38f9120634f",
   "metadata": {},
   "source": [
    "## Defining some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd2a6121-4709-498a-9067-bcd297bb2085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(v):\n",
    "    exp_array = np.exp(v - np.max(v))\n",
    "    return exp_array / np.sum(exp_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e13dfe0-2ec6-4a9a-8a85-f9f48f6f54ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(map):    \n",
    "    output_tensor = np.maximum(map, 0)\n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82c7aef5-4095-4f08-8619-e58a44ca6a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooling(map, dim):\n",
    "    n_channels = map.shape[0]\n",
    "    map_size = map.shape[1]\n",
    "    output_size = map.shape[1]//dim\n",
    "    return map.reshape(n_channels, output_size, dim, output_size, dim).max(axis=(2,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b4017ce-ec8a-4825-96a4-273b591dca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution(sample, kernel):\n",
    "    kernel_size = kernel.shape[1]\n",
    "    pad = kernel_size//2\n",
    "    n, h, w = sample.shape\n",
    "    \n",
    "    out_sample = np.zeros((n, h+2*pad, w+2*pad))\n",
    "    for i in range(n):\n",
    "        out_sample[i] = np.pad(sample[i], (pad,), 'constant', constant_values = 0)\n",
    "    sample = out_sample\n",
    "    \n",
    "    size_feature_map = h\n",
    "    n_out_channels = kernel.shape[0]\n",
    "\n",
    "    output_tensor = np.zeros((n_out_channels, size_feature_map, size_feature_map))\n",
    "\n",
    "    for i in range(n_out_channels):\n",
    "        current_kernel = kernel[i]\n",
    "\n",
    "        for r in range(size_feature_map):\n",
    "            for c in range(size_feature_map):\n",
    "                window = sample[:, r : r + kernel_size, c : c + kernel_size]\n",
    "                value = np.sum(window*current_kernel, axis = None)\n",
    "                output_tensor[i, r, c] = value\n",
    "\n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5448a29e-bec6-46bf-9908-d02a34e94447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def other_convolution(sample, kernel, pad):\n",
    "    kernel_size = kernel.shape[1]\n",
    "    n, h, w = sample.shape\n",
    "    \n",
    "    out_sample = np.zeros((n, h+2*pad, w+2*pad))\n",
    "    for i in range(n):\n",
    "        out_sample[i] = np.pad(sample[i], (pad,), 'constant', constant_values = 0)\n",
    "    sample = out_sample\n",
    "    \n",
    "    size_feature_map = h + 2*pad - kernel_size + 1\n",
    "    n_out_channels = kernel.shape[0]\n",
    "\n",
    "    output_tensor = np.zeros((n_out_channels, size_feature_map, size_feature_map))\n",
    "\n",
    "    for i in range(n_out_channels):\n",
    "        current_kernel = kernel[i]\n",
    "\n",
    "        for r in range(size_feature_map):\n",
    "            for c in range(size_feature_map):\n",
    "                window = sample[:, r : r + kernel_size, c : c + kernel_size] \n",
    "                value = np.sum(window*current_kernel, axis = None)\n",
    "                output_tensor[i, r, c] = value\n",
    "\n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17752509-813b-40f1-a0cb-27b85d5f8c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve2d(sample, kernel):\n",
    "    size_feature_map = sample.shape[0]\n",
    "    kernel_size = kernel.shape[0]\n",
    "    pad = kernel_size//2\n",
    "    \n",
    "    sample = np.pad(sample, (pad,), 'constant', constant_values = 0)\n",
    "    \n",
    "    output_tensor = np.zeros((size_feature_map, size_feature_map))\n",
    "    for r in range(size_feature_map):\n",
    "        for c in range(size_feature_map):\n",
    "            window = sample[r : r + kernel_size, c : c + kernel_size]\n",
    "            value = np.sum(window*kernel, axis = None)\n",
    "            output_tensor[r, c] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22978e33-dd8e-41eb-9a0c-8e736abced05",
   "metadata": {},
   "source": [
    "## CONV2D Class for the Convolution Layers\n",
    "Forward and Backward Pass has been implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13f8f060-98b8-4275-a4f7-8feb6f6c015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv2d:\n",
    "    def __init__(self, num, size):\n",
    "        self.kernel = np.random.randn(num, size, size)/4096 # Xavier Initialisation\n",
    "        self.bias = np.random.rand(num, size, size)/4096\n",
    "        self.layer_input = None\n",
    "        self.layer_activated = None\n",
    "        self.kernel_grad = None\n",
    "        self.size = size\n",
    "    \n",
    "    def forward_pass(self, sample):\n",
    "        self.layer_input = sample\n",
    "        output_tensor = convolution(sample, self.kernel)\n",
    "        self.layer_activated = relu(output_tensor)\n",
    "        return self.layer_activated\n",
    "        \n",
    "    def backward_pass(self, inp_grad):\n",
    "        n, h, w = self.layer_input.shape\n",
    "        pass_grad = np.zeros((n,h,w))\n",
    "        relu_mat = self.layer_activated\n",
    "        relu_mat[np.nonzero(relu_mat)] = 1\n",
    "        inp_grad = inp_grad * relu_mat\n",
    "        \n",
    "        kernel_grad = other_convolution(self.layer_input, inp_grad, self.size//2)\n",
    "        # print(\"here: \", inp_grad.shape, self.layer_input.shape, kernel_grad.shape)\n",
    "        # check this kernel gradient once, need to perform full convolution\n",
    "        self.kernel_grad = kernel_grad\n",
    "        \n",
    "        not_final = np.zeros((inp_grad.shape[0], h, w))\n",
    "        for i in range(inp_grad.shape[0]):\n",
    "            curr_grad = inp_grad[i]\n",
    "            curr_kernel = np.flip(self.kernel[i], axis = (0,1))\n",
    "            not_final[i] = convolve2d(curr_grad, curr_kernel)\n",
    "        still_not_final = np.sum(pass_grad, axis = 0)\n",
    "        \n",
    "        for i in range(n):\n",
    "            pass_grad[i] = still_not_final\n",
    "        return pass_grad\n",
    "        \n",
    "        # flip_kernel = np.flip(kernel_grad, axis=(0,1))\n",
    "        # not_final = convolution(inp_grad, flip_kernel)\n",
    "        # still_not_final = np.sum(not_final, axis = 0)\n",
    "        \n",
    "    def update(self):\n",
    "        self.kernel -= 0.001*self.kernel_grad\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f95e8d3-a8ca-4881-9465-3844416932e2",
   "metadata": {},
   "source": [
    "## MAXPOOL2D Class for the Pooling Layers\n",
    "Forward and Backward Pass has been implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c90d2bd6-82bd-4a9b-8cf7-e24433d0e556",
   "metadata": {},
   "outputs": [],
   "source": [
    "class maxpool2d:\n",
    "    def __init__(self, dim):\n",
    "        self.dim = dim\n",
    "        self.layer_input = None\n",
    "        self.layer_output = None\n",
    "        \n",
    "    def forward_pass(self , sample):\n",
    "        self.layer_input = sample\n",
    "        self.layer_output = pooling(sample, 2)\n",
    "        return self.layer_output\n",
    "    \n",
    "    def backward_pass(self, inp_grad):\n",
    "        n, h, w = self.layer_input.shape\n",
    "        x = self.layer_input\n",
    "        dim = self.dim\n",
    "        \n",
    "        pass_mat = np.zeros((n,h,w))\n",
    "\n",
    "        for i in range(n):\n",
    "            for r in range(0, h-1, dim):\n",
    "                for c in range(0, w-1, dim):\n",
    "                    window = x[i, r:r+dim, c:c+dim]\n",
    "                    max_ind = np.unravel_index(window.argmax(), window.shape)\n",
    "                    pass_mat[i, r:r+dim, c:c+dim][max_ind] = inp_grad[i, r//2, c//2]\n",
    "            \n",
    "        return pass_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb725ef-ccff-4c40-a451-92294ed14f5c",
   "metadata": {},
   "source": [
    "## FC_1 Class for the First Fully Connected Layer\n",
    "Forward and Backward Pass has been implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f11c194-63c0-4de5-889d-94710e61f1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fc_1:\n",
    "    def __init__(self, size, next_size):\n",
    "        self.weights = np.random.randn(next_size, size)/4096\n",
    "        self.bias = np.random.randn(1, next_size)/4096\n",
    "        self.weights_grad = None\n",
    "        self.bias_grad = None\n",
    "        self.layer_input = None\n",
    "        self.layer_output = None\n",
    "        self.layer_output_active = None\n",
    "        \n",
    "    def forward_pass(self, sample):\n",
    "        self.layer_input = sample\n",
    "        output = sample @ self.weights.T + self.bias\n",
    "        self.layer_output = output\n",
    "        self.layer_output_active = relu(output)\n",
    "        return self.layer_output_active\n",
    "    \n",
    "    def backward_pass(self, inp_grad):\n",
    "        relu_mat = self.layer_output_active\n",
    "        relu_mat[np.nonzero(relu_mat)] = 1\n",
    "        relued_grad = inp_grad * relu_mat\n",
    "        pass_grad = relued_grad.reshape(1,-1) @ self.weights\n",
    "        self.weights_grad = relued_grad.reshape(-1,1) @ self.layer_input.reshape(1,-1)\n",
    "        self.bias_grad = relued_grad.reshape(1,-1)\n",
    "        return pass_grad\n",
    "    \n",
    "    def update(self):\n",
    "        self.weights -= 0.001*self.weights_grad\n",
    "        self.bias -= 0.001*self.bias_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5de94d9-98f2-48fd-938f-acb26851f66b",
   "metadata": {},
   "source": [
    "## FC_2 Class for the Second Fully Connected Layer\n",
    "Forward and Backward Pass has been implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12c47614-e2f5-4ff4-963c-a259cf130fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fc_2:\n",
    "    def __init__(self, size, next_size):\n",
    "        self.weights = np.random.randn(next_size, size)/64\n",
    "        self.bias = np.random.randn(1, next_size)/64\n",
    "        self.weights_grad = None\n",
    "        self.bias_grad = None\n",
    "        self.layer_input = None\n",
    "        self.layer_output = None\n",
    "        \n",
    "    def forward_pass(self, sample):\n",
    "        self.layer_input = sample\n",
    "        output = sample @ self.weights.T + self.bias\n",
    "        self.layer_output = output\n",
    "        return self.layer_output\n",
    "    \n",
    "    def backward_pass(self, inp_grad):\n",
    "        pass_grad = inp_grad.reshape(1,-1) @ self.weights\n",
    "        self.weights_grad = inp_grad.reshape(-1,1) @ self.layer_input.reshape(1,-1)\n",
    "        # print(inp_grad.shape)\n",
    "        self.bias_grad = inp_grad.reshape(1,-1)\n",
    "        return pass_grad\n",
    "    \n",
    "    def update(self):\n",
    "        self.weights -= 0.001*self.weights_grad\n",
    "        self.bias -= 0.001*self.bias_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c24d83d-ef04-475c-b9ca-f2edfbf14f87",
   "metadata": {},
   "source": [
    "## Loading the CIFAR-10 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c7ad364-0eea-4c61-a1ce-139db454ccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6638e951-9124-4430-a40a-ca082ddfadac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 3072), (50000,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set1 = unpickle(\"./content/data_batch_1\")\n",
    "set2 = unpickle(\"./content/data_batch_2\")\n",
    "set3 = unpickle(\"./content/data_batch_3\")\n",
    "set4 = unpickle(\"./content/data_batch_4\")\n",
    "set5 = unpickle(\"./content/data_batch_5\")\n",
    "x_train = np.vstack((set1[b'data'], set2[b'data'], set3[b'data'], set4[b'data'], set5[b'data']))\n",
    "y_train = np.hstack((np.array(set1[b'labels']), np.array(set2[b'labels']), np.array(set3[b'labels']), np.array(set4[b'labels']) ,np.array(set5[b'labels']) ))\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bb5da4c-255d-424a-b9bd-4b7d34face3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trial = []\n",
    "y_trial = y_train[0:1000]\n",
    "for i in range(1000):\n",
    "    x_trial.append(x_train[i].reshape(3,32,32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970963f3-0d64-403e-9ed2-e1d4ebce71fc",
   "metadata": {},
   "source": [
    "## Defining the Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d4ef364-c202-4b7d-9db5-646d4e283edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, y, epochs, batch_size):\n",
    "    conv1 = conv2d(32, 3)\n",
    "    pool1 = maxpool2d(2)\n",
    "    conv2 = conv2d(64, 5)\n",
    "    pool2 = maxpool2d(2)\n",
    "    conv3 = conv2d(64, 3)\n",
    "    fc1 = fc_1(4096, 64)\n",
    "    fc2 = fc_2(64, 10)\n",
    "    num_batch = 150\n",
    "    parameters = []\n",
    "    for i in range(epochs):\n",
    "        perm = np.random.permutation(x.shape[0])\n",
    "        x = x[perm]\n",
    "        y = y[perm]\n",
    "        print(\"EPOCH: \", i)\n",
    "        total_loss = 0\n",
    "        for j in range(num_batch):\n",
    "            update_fc2_w = []\n",
    "            update_fc1_w = []\n",
    "            update_fc2_b = []\n",
    "            update_fc1_b = []\n",
    "            update_conv3 = []\n",
    "            update_conv2 = []\n",
    "            update_conv1 = []\n",
    "            count = 0\n",
    "            loss = 0\n",
    "            for k in range(batch_size):\n",
    "                a1 = conv1.forward_pass(x[j*32 + k])\n",
    "                a2 = pool1.forward_pass(a1)\n",
    "                a3 = conv2.forward_pass(a2)\n",
    "                a4 = pool2.forward_pass(a3)\n",
    "                a5 = conv3.forward_pass(a4).flatten()\n",
    "                a6 = fc1.forward_pass(a5)\n",
    "                a7 = fc2.forward_pass(a6)\n",
    "                out = softmax(a7)\n",
    "                onehot = np.zeros(10)\n",
    "                onehot[y[j*32 + k]] = 1\n",
    "                grad_fc2 = out - onehot\n",
    "                grad_fc1 = fc2.backward_pass(grad_fc2.reshape(1,-1))\n",
    "                grad_conv3 = fc1.backward_pass(grad_fc1).reshape(64, 8, 8)\n",
    "                grad_pool2 = conv3.backward_pass(grad_conv3)\n",
    "                grad_conv2 = pool2.backward_pass(grad_pool2)\n",
    "                grad_pool1 = conv2.backward_pass(grad_conv2)\n",
    "                grad_conv1 = pool1.backward_pass(grad_pool1)\n",
    "                init_grad = conv1.backward_pass(grad_conv1)\n",
    "                update_fc2_w += [fc2.weights]\n",
    "                update_fc1_w += [fc1.weights]\n",
    "                update_fc2_b += [fc2.bias] \n",
    "                update_fc1_b += [fc1.bias]\n",
    "                update_conv3 += [conv3.kernel]\n",
    "                update_conv2 += [conv2.kernel]\n",
    "                update_conv1 += [conv1.kernel]\n",
    "                if out.argmax() == y[j*32 + k]:\n",
    "                    count += 1\n",
    "                loss -= np.log(np.max(out))\n",
    "            print(\"Batch: \", j, \", correct: \", count, \", loss: \", loss/32)\n",
    "            total_loss += loss/32\n",
    "            fc2.weights = np.sum(update_fc2_w, axis = 0)/32 \n",
    "            fc1.weights = np.sum(update_fc1_w, axis = 0)/32\n",
    "            fc2.bias = np.sum(update_fc2_b, axis = 0)/32\n",
    "            fc1.bias = np.sum(update_fc1_b, axis = 0)/32\n",
    "            conv3.kernel = np.sum(update_conv3, axis = 0)/32\n",
    "            conv2.kernel = np.sum(update_conv2, axis = 0)/32\n",
    "            conv1.kernel = np.sum(update_conv1, axis = 0)/32   \n",
    "            fc2.update()\n",
    "            fc1.update()\n",
    "            conv3.update()\n",
    "            conv2.update()\n",
    "            conv1.update()\n",
    "        parameters += [[fc2.weights, fc1.weights, fc2.bias, fc1.bias, conv3.kernel, conv2.kernel, conv1.kernel]]\n",
    "        print(\"Total Loss = \", total_loss/150)\n",
    "        print()\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4619a36c-a20c-4b27-a06d-26a37311171e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_me = []\n",
    "y_me = []\n",
    "for i in range(10):\n",
    "    for j in range(480):\n",
    "        x_me.append(x_train[5000*i+j].reshape(3,32,32))\n",
    "        y_me.append(y_train[5000*i+j])\n",
    "x_me = np.array(x_me)\n",
    "y_me = np.array(y_me)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b564a132-ef2c-48cf-989d-b8798143970b",
   "metadata": {},
   "source": [
    "## Training the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74f27dbe-80f2-4751-bb7c-9b9181c124ed",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:  0\n",
      "Batch:  0 , correct:  3 , loss:  2.2821732033037847\n",
      "Batch:  1 , correct:  1 , loss:  2.282276138681037\n",
      "Batch:  2 , correct:  4 , loss:  2.2823799926750468\n",
      "Batch:  3 , correct:  2 , loss:  2.2824824413988916\n",
      "Batch:  4 , correct:  5 , loss:  2.2825863539235938\n",
      "Batch:  5 , correct:  4 , loss:  2.282690347989605\n",
      "Batch:  6 , correct:  3 , loss:  2.282794854552202\n",
      "Batch:  7 , correct:  3 , loss:  2.282894971897414\n",
      "Batch:  8 , correct:  4 , loss:  2.2829989895167575\n",
      "Batch:  9 , correct:  1 , loss:  2.2830991785681376\n",
      "Batch:  10 , correct:  6 , loss:  2.283199437021148\n",
      "Batch:  11 , correct:  4 , loss:  2.283301823903812\n",
      "Batch:  12 , correct:  6 , loss:  2.2834018710430435\n",
      "Batch:  13 , correct:  5 , loss:  2.283503698684125\n",
      "Batch:  14 , correct:  2 , loss:  2.2836056337213777\n",
      "Batch:  15 , correct:  2 , loss:  2.283708794722919\n",
      "Batch:  16 , correct:  5 , loss:  2.283808857443812\n",
      "Batch:  17 , correct:  4 , loss:  2.2839090179104624\n",
      "Batch:  18 , correct:  2 , loss:  2.2840109746603603\n",
      "Batch:  19 , correct:  4 , loss:  2.2841144147482875\n",
      "Batch:  20 , correct:  2 , loss:  2.2842145968720406\n",
      "Batch:  21 , correct:  2 , loss:  2.284317754708956\n",
      "Batch:  22 , correct:  3 , loss:  2.2844179924432972\n",
      "Batch:  23 , correct:  4 , loss:  2.2835127923025422\n",
      "Batch:  24 , correct:  5 , loss:  2.283616340875274\n",
      "Batch:  25 , correct:  5 , loss:  2.283718239183312\n",
      "Batch:  26 , correct:  4 , loss:  2.2838202098632827\n",
      "Batch:  27 , correct:  3 , loss:  2.2839222595493074\n",
      "Batch:  28 , correct:  4 , loss:  2.284024218050692\n",
      "Batch:  29 , correct:  4 , loss:  2.284128018032906\n",
      "Batch:  30 , correct:  1 , loss:  2.2842298875567884\n",
      "Batch:  31 , correct:  1 , loss:  2.2843318764619163\n",
      "Batch:  32 , correct:  8 , loss:  2.2844352991214225\n",
      "Batch:  33 , correct:  3 , loss:  2.283530120396328\n",
      "Batch:  34 , correct:  3 , loss:  2.2826251379099016\n",
      "Batch:  35 , correct:  4 , loss:  2.282727326510096\n",
      "Batch:  36 , correct:  4 , loss:  2.2828294226931303\n",
      "Batch:  37 , correct:  4 , loss:  2.282929766813176\n",
      "Batch:  38 , correct:  4 , loss:  2.2830329971543364\n",
      "Batch:  39 , correct:  8 , loss:  2.283135220963549\n",
      "Batch:  40 , correct:  1 , loss:  2.282230309898691\n",
      "Batch:  41 , correct:  5 , loss:  2.28233076312414\n",
      "Batch:  42 , correct:  4 , loss:  2.2824341217687145\n",
      "Batch:  43 , correct:  3 , loss:  2.282536289418615\n",
      "Batch:  44 , correct:  4 , loss:  2.282638602743348\n",
      "Batch:  45 , correct:  6 , loss:  2.2827390695389895\n",
      "Batch:  46 , correct:  1 , loss:  2.2828431790160497\n",
      "Batch:  47 , correct:  3 , loss:  2.282945331009346\n",
      "Batch:  48 , correct:  2 , loss:  2.2830476627621255\n",
      "Batch:  49 , correct:  1 , loss:  2.283151465045113\n",
      "Batch:  50 , correct:  5 , loss:  2.2832553418695287\n",
      "Batch:  51 , correct:  4 , loss:  2.282350401085704\n",
      "Batch:  52 , correct:  5 , loss:  2.2824545889360444\n",
      "Batch:  53 , correct:  4 , loss:  2.282556734703229\n",
      "Batch:  54 , correct:  5 , loss:  2.282660037488029\n",
      "Batch:  55 , correct:  2 , loss:  2.2817552361855946\n",
      "Batch:  56 , correct:  4 , loss:  2.281858772740028\n",
      "Batch:  57 , correct:  3 , loss:  2.2819628002663728\n",
      "Batch:  58 , correct:  5 , loss:  2.282065259409906\n",
      "Batch:  59 , correct:  2 , loss:  2.2821688429662736\n",
      "Batch:  60 , correct:  4 , loss:  2.28227102948345\n",
      "Batch:  61 , correct:  3 , loss:  2.282374676565745\n",
      "Batch:  62 , correct:  3 , loss:  2.2824780178560644\n",
      "Batch:  63 , correct:  3 , loss:  2.2825804621914947\n",
      "Batch:  64 , correct:  4 , loss:  2.2826835485033308\n",
      "Batch:  65 , correct:  0 , loss:  2.2827872104745586\n",
      "Batch:  66 , correct:  3 , loss:  2.2828912983430367\n",
      "Batch:  67 , correct:  5 , loss:  2.2829944415120456\n",
      "Batch:  68 , correct:  4 , loss:  2.283098581981159\n",
      "Batch:  69 , correct:  5 , loss:  2.2832027957643293\n",
      "Batch:  70 , correct:  5 , loss:  2.2833048846107773\n",
      "Batch:  71 , correct:  2 , loss:  2.2834081401383863\n",
      "Batch:  72 , correct:  2 , loss:  2.283511966779793\n",
      "Batch:  73 , correct:  8 , loss:  2.2836121754718293\n",
      "Batch:  74 , correct:  4 , loss:  2.2837145024751595\n",
      "Batch:  75 , correct:  2 , loss:  2.2838175846703277\n",
      "Batch:  76 , correct:  1 , loss:  2.2829125387424343\n",
      "Batch:  77 , correct:  3 , loss:  2.2830128700643257\n",
      "Batch:  78 , correct:  3 , loss:  2.283113283926856\n",
      "Batch:  79 , correct:  3 , loss:  2.282208380164464\n",
      "Batch:  80 , correct:  2 , loss:  2.2823116646029806\n",
      "Batch:  81 , correct:  3 , loss:  2.2824136539186206\n",
      "Batch:  82 , correct:  2 , loss:  2.282514202955768\n",
      "Batch:  83 , correct:  2 , loss:  2.282616657887447\n",
      "Batch:  84 , correct:  2 , loss:  2.282720868878721\n",
      "Batch:  85 , correct:  5 , loss:  2.281816183068659\n",
      "Batch:  86 , correct:  1 , loss:  2.2819188365415597\n",
      "Batch:  87 , correct:  1 , loss:  2.2820222305310023\n",
      "Batch:  88 , correct:  2 , loss:  2.2821257183758736\n",
      "Batch:  89 , correct:  2 , loss:  2.282226595747158\n",
      "Batch:  90 , correct:  1 , loss:  2.2823271906748817\n",
      "Batch:  91 , correct:  2 , loss:  2.2824292154841634\n",
      "Batch:  92 , correct:  1 , loss:  2.2825329109417094\n",
      "Batch:  93 , correct:  3 , loss:  2.282637250407522\n",
      "Batch:  94 , correct:  5 , loss:  2.2827410085467292\n",
      "Batch:  95 , correct:  2 , loss:  2.2828416347413922\n",
      "Batch:  96 , correct:  2 , loss:  2.2829454004891\n",
      "Batch:  97 , correct:  3 , loss:  2.2830474588925704\n",
      "Batch:  98 , correct:  4 , loss:  2.2831513527737455\n",
      "Batch:  99 , correct:  3 , loss:  2.2832530651302196\n",
      "Batch:  100 , correct:  2 , loss:  2.283356305907983\n",
      "Batch:  101 , correct:  5 , loss:  2.283458390721935\n",
      "Batch:  102 , correct:  0 , loss:  2.283561689477405\n",
      "Batch:  103 , correct:  3 , loss:  2.283664774299743\n",
      "Batch:  104 , correct:  4 , loss:  2.2837665608802085\n",
      "Batch:  105 , correct:  2 , loss:  2.283868408186423\n",
      "Batch:  106 , correct:  3 , loss:  2.2839704847717766\n",
      "Batch:  107 , correct:  1 , loss:  2.2840721310725436\n",
      "Batch:  108 , correct:  1 , loss:  2.2831677142237674\n",
      "Batch:  109 , correct:  2 , loss:  2.283268391776309\n",
      "Batch:  110 , correct:  3 , loss:  2.283371731867819\n",
      "Batch:  111 , correct:  3 , loss:  2.283473503772821\n",
      "Batch:  112 , correct:  2 , loss:  2.283576622828849\n",
      "Batch:  113 , correct:  4 , loss:  2.283680000466816\n",
      "Batch:  114 , correct:  6 , loss:  2.283784144842372\n",
      "Batch:  115 , correct:  3 , loss:  2.2838862155905257\n",
      "Batch:  116 , correct:  5 , loss:  2.2839896384578746\n",
      "Batch:  117 , correct:  4 , loss:  2.284091394433705\n",
      "Batch:  118 , correct:  7 , loss:  2.2831869594805156\n",
      "Batch:  119 , correct:  2 , loss:  2.2832888663923794\n",
      "Batch:  120 , correct:  4 , loss:  2.2833926150882555\n",
      "Batch:  121 , correct:  1 , loss:  2.2834955682124067\n",
      "Batch:  122 , correct:  6 , loss:  2.2835997308823672\n",
      "Batch:  123 , correct:  8 , loss:  2.2837016632730265\n",
      "Batch:  124 , correct:  5 , loss:  2.283804731353531\n",
      "Batch:  125 , correct:  4 , loss:  2.2839076982569018\n",
      "Batch:  126 , correct:  2 , loss:  2.284011421646781\n",
      "Batch:  127 , correct:  5 , loss:  2.2841134796706952\n",
      "Batch:  128 , correct:  4 , loss:  2.2842165571471904\n",
      "Batch:  129 , correct:  5 , loss:  2.2843203182437204\n",
      "Batch:  130 , correct:  1 , loss:  2.284423464978967\n",
      "Batch:  131 , correct:  5 , loss:  2.2835189525604465\n",
      "Batch:  132 , correct:  2 , loss:  2.2836222694953667\n",
      "Batch:  133 , correct:  4 , loss:  2.2837256577954776\n",
      "Batch:  134 , correct:  4 , loss:  2.28382901458041\n",
      "Batch:  135 , correct:  2 , loss:  2.2839309044778133\n",
      "Batch:  136 , correct:  2 , loss:  2.2840338432412377\n",
      "Batch:  137 , correct:  3 , loss:  2.284137242456495\n",
      "Batch:  138 , correct:  0 , loss:  2.2842402455897464\n",
      "Batch:  139 , correct:  5 , loss:  2.284344256952571\n",
      "Batch:  140 , correct:  3 , loss:  2.2844483493003533\n",
      "Batch:  141 , correct:  2 , loss:  2.2845525272876483\n",
      "Batch:  142 , correct:  6 , loss:  2.2846558420634264\n",
      "Batch:  143 , correct:  4 , loss:  2.284759509157948\n",
      "Batch:  144 , correct:  3 , loss:  2.2848628694312785\n",
      "Batch:  145 , correct:  6 , loss:  2.2849665941457338\n",
      "Batch:  146 , correct:  3 , loss:  2.2850668368897757\n",
      "Batch:  147 , correct:  2 , loss:  2.285170980471913\n",
      "Batch:  148 , correct:  5 , loss:  2.285275211070124\n",
      "Batch:  149 , correct:  5 , loss:  2.284370543199342\n",
      "Total Loss =  2.2833065306832396\n",
      "\n",
      "EPOCH:  1\n",
      "Batch:  0 , correct:  2 , loss:  2.284473978509362\n",
      "Batch:  1 , correct:  4 , loss:  2.284577289391748\n",
      "Batch:  2 , correct:  2 , loss:  2.2846791274511875\n",
      "Batch:  3 , correct:  0 , loss:  2.284782594326124\n",
      "Batch:  4 , correct:  1 , loss:  2.2848863604874947\n",
      "Batch:  5 , correct:  2 , loss:  2.2849892049885145\n",
      "Batch:  6 , correct:  4 , loss:  2.285092516516\n",
      "Batch:  7 , correct:  3 , loss:  2.285195438838137\n",
      "Batch:  8 , correct:  3 , loss:  2.2852988068314906\n",
      "Batch:  9 , correct:  2 , loss:  2.285400391881988\n",
      "Batch:  10 , correct:  2 , loss:  2.2855021685358494\n",
      "Batch:  11 , correct:  4 , loss:  2.284597441441468\n",
      "Batch:  12 , correct:  3 , loss:  2.284700954103086\n",
      "Batch:  13 , correct:  1 , loss:  2.2848022030174353\n",
      "Batch:  14 , correct:  8 , loss:  2.2849035526629704\n",
      "Batch:  15 , correct:  3 , loss:  2.2839989590430494\n",
      "Batch:  16 , correct:  4 , loss:  2.284101994107595\n",
      "Batch:  17 , correct:  3 , loss:  2.2842055908992385\n",
      "Batch:  18 , correct:  4 , loss:  2.284305781741834\n",
      "Batch:  19 , correct:  7 , loss:  2.2844099865150924\n",
      "Batch:  20 , correct:  1 , loss:  2.2845136243312294\n",
      "Batch:  21 , correct:  3 , loss:  2.2846152669224025\n",
      "Batch:  22 , correct:  2 , loss:  2.2847195234620674\n",
      "Batch:  23 , correct:  4 , loss:  2.283814936442262\n",
      "Batch:  24 , correct:  3 , loss:  2.2839187136235513\n",
      "Batch:  25 , correct:  6 , loss:  2.2833550056474197\n",
      "Batch:  26 , correct:  4 , loss:  2.283457486883762\n",
      "Batch:  27 , correct:  3 , loss:  2.283220039504408\n",
      "Batch:  28 , correct:  4 , loss:  2.2833228206698375\n",
      "Batch:  29 , correct:  7 , loss:  2.2827621633149224\n",
      "Batch:  30 , correct:  0 , loss:  2.2828625744973117\n",
      "Batch:  31 , correct:  4 , loss:  2.2829642358768703\n",
      "Batch:  32 , correct:  5 , loss:  2.2820654482805494\n",
      "Batch:  33 , correct:  4 , loss:  2.2821698768724796\n",
      "Batch:  34 , correct:  0 , loss:  2.2822704157781137\n",
      "Batch:  35 , correct:  3 , loss:  2.2823724943313444\n",
      "Batch:  36 , correct:  3 , loss:  2.2824748798039356\n",
      "Batch:  37 , correct:  6 , loss:  2.282574471878756\n",
      "Batch:  38 , correct:  6 , loss:  2.2816757677221378\n",
      "Batch:  39 , correct:  2 , loss:  2.281778360408355\n",
      "Batch:  40 , correct:  5 , loss:  2.281878959291637\n",
      "Batch:  41 , correct:  5 , loss:  2.281981146789823\n",
      "Batch:  42 , correct:  2 , loss:  2.282082831629965\n",
      "Batch:  43 , correct:  0 , loss:  2.2821834649036576\n",
      "Batch:  44 , correct:  2 , loss:  2.28228589747027\n",
      "Batch:  45 , correct:  0 , loss:  2.2823884084945663\n",
      "Batch:  46 , correct:  3 , loss:  2.2824890747930993\n",
      "Batch:  47 , correct:  4 , loss:  2.282589819187897\n",
      "Batch:  48 , correct:  3 , loss:  2.2826923738570324\n",
      "Batch:  49 , correct:  4 , loss:  2.281793653395463\n",
      "Batch:  50 , correct:  4 , loss:  2.281896019655097\n",
      "Batch:  51 , correct:  3 , loss:  2.2819977326215675\n",
      "Batch:  52 , correct:  4 , loss:  2.2821021794208503\n",
      "Batch:  53 , correct:  3 , loss:  2.282204327099833\n",
      "Batch:  54 , correct:  1 , loss:  2.28230671512769\n",
      "Batch:  55 , correct:  4 , loss:  2.2824089322424395\n",
      "Batch:  56 , correct:  3 , loss:  2.281510256183023\n",
      "Batch:  57 , correct:  3 , loss:  2.2816135174093253\n",
      "Batch:  58 , correct:  3 , loss:  2.281714342065111\n",
      "Batch:  59 , correct:  3 , loss:  2.2818176638899783\n",
      "Batch:  60 , correct:  3 , loss:  2.2819221502896734\n",
      "Batch:  61 , correct:  3 , loss:  2.2820245936734955\n",
      "Batch:  62 , correct:  2 , loss:  2.2811260067202066\n",
      "Batch:  63 , correct:  6 , loss:  2.2812306363174706\n",
      "Batch:  64 , correct:  2 , loss:  2.2813332198156364\n",
      "Batch:  65 , correct:  2 , loss:  2.281434108764122\n",
      "Batch:  66 , correct:  4 , loss:  2.2805356319642383\n",
      "Batch:  67 , correct:  2 , loss:  2.2796373528345604\n",
      "Batch:  68 , correct:  6 , loss:  2.2797399614192515\n",
      "Batch:  69 , correct:  3 , loss:  2.2788418360541245\n",
      "Batch:  70 , correct:  2 , loss:  2.2789446083967912\n",
      "Batch:  71 , correct:  4 , loss:  2.2790465201621553\n",
      "Batch:  72 , correct:  4 , loss:  2.279149293184412\n",
      "Batch:  73 , correct:  1 , loss:  2.2782512906658683\n",
      "Batch:  74 , correct:  4 , loss:  2.278353817428074\n",
      "Batch:  75 , correct:  1 , loss:  2.2784574168340255\n",
      "Batch:  76 , correct:  4 , loss:  2.278557128832349\n",
      "Batch:  77 , correct:  2 , loss:  2.278660007403466\n",
      "Batch:  78 , correct:  4 , loss:  2.278763642394739\n",
      "Batch:  79 , correct:  4 , loss:  2.2788634023290606\n",
      "Batch:  80 , correct:  2 , loss:  2.2789661725801267\n",
      "Batch:  81 , correct:  3 , loss:  2.2790659939123024\n",
      "Batch:  82 , correct:  3 , loss:  2.2791696544369273\n",
      "Batch:  83 , correct:  1 , loss:  2.2792706839065806\n",
      "Batch:  84 , correct:  2 , loss:  2.279371790389694\n",
      "Batch:  85 , correct:  6 , loss:  2.2794744640152205\n",
      "Batch:  86 , correct:  2 , loss:  2.2795742809605284\n",
      "Batch:  87 , correct:  4 , loss:  2.279676645294106\n",
      "Batch:  88 , correct:  1 , loss:  2.2797794043146826\n",
      "Batch:  89 , correct:  2 , loss:  2.2798820987442276\n",
      "Batch:  90 , correct:  3 , loss:  2.279984813165835\n",
      "Batch:  91 , correct:  3 , loss:  2.2800844737477757\n",
      "Batch:  92 , correct:  3 , loss:  2.280187250540129\n",
      "Batch:  93 , correct:  1 , loss:  2.2802901070104733\n",
      "Batch:  94 , correct:  2 , loss:  2.2803919208936216\n",
      "Batch:  95 , correct:  2 , loss:  2.2804944104602036\n",
      "Batch:  96 , correct:  0 , loss:  2.2805940652976275\n",
      "Batch:  97 , correct:  4 , loss:  2.280696591599209\n",
      "Batch:  98 , correct:  7 , loss:  2.2807999214280517\n",
      "Batch:  99 , correct:  5 , loss:  2.2799016363300213\n",
      "Batch:  100 , correct:  2 , loss:  2.2800051383186806\n",
      "Batch:  101 , correct:  2 , loss:  2.280107015723156\n",
      "Batch:  102 , correct:  2 , loss:  2.280209631541488\n",
      "Batch:  103 , correct:  7 , loss:  2.2803115658228363\n",
      "Batch:  104 , correct:  5 , loss:  2.2804142434060264\n",
      "Batch:  105 , correct:  2 , loss:  2.2805150811747223\n",
      "Batch:  106 , correct:  2 , loss:  2.2806178145575458\n",
      "Batch:  107 , correct:  1 , loss:  2.2807206254771764\n",
      "Batch:  108 , correct:  5 , loss:  2.28082256878746\n",
      "Batch:  109 , correct:  1 , loss:  2.2809268567839607\n",
      "Batch:  110 , correct:  3 , loss:  2.2810312240427493\n",
      "Batch:  111 , correct:  3 , loss:  2.281130781424658\n",
      "Batch:  112 , correct:  1 , loss:  2.281231569440635\n",
      "Batch:  113 , correct:  1 , loss:  2.2813342277896917\n",
      "Batch:  114 , correct:  2 , loss:  2.2814369554845007\n",
      "Batch:  115 , correct:  2 , loss:  2.281539315304514\n",
      "Batch:  116 , correct:  4 , loss:  2.2816401299676774\n",
      "Batch:  117 , correct:  3 , loss:  2.2817429005946304\n",
      "Batch:  118 , correct:  2 , loss:  2.281842412500654\n",
      "Batch:  119 , correct:  5 , loss:  2.2819447979341954\n",
      "Batch:  120 , correct:  5 , loss:  2.2820456313615525\n",
      "Batch:  121 , correct:  5 , loss:  2.2821474019216637\n",
      "Batch:  122 , correct:  3 , loss:  2.282248290488229\n",
      "Batch:  123 , correct:  0 , loss:  2.2823510443053623\n",
      "Batch:  124 , correct:  0 , loss:  2.2824538680721367\n",
      "Batch:  125 , correct:  1 , loss:  2.282554794717119\n",
      "Batch:  126 , correct:  6 , loss:  2.282654242913854\n",
      "Batch:  127 , correct:  3 , loss:  2.2827562848333383\n",
      "Batch:  128 , correct:  2 , loss:  2.2828591366613376\n",
      "Batch:  129 , correct:  3 , loss:  2.282961607639099\n",
      "Batch:  130 , correct:  2 , loss:  2.2830645832348884\n",
      "Batch:  131 , correct:  4 , loss:  2.2831668291590135\n",
      "Batch:  132 , correct:  1 , loss:  2.283270848737546\n",
      "Batch:  133 , correct:  6 , loss:  2.2833736904092214\n",
      "Batch:  134 , correct:  4 , loss:  2.283476696656479\n",
      "Batch:  135 , correct:  2 , loss:  2.2835760592916943\n",
      "Batch:  136 , correct:  5 , loss:  2.2836776249422024\n",
      "Batch:  137 , correct:  2 , loss:  2.2837800424418884\n",
      "Batch:  138 , correct:  3 , loss:  2.28388404107682\n",
      "Batch:  139 , correct:  3 , loss:  2.2839865014265994\n",
      "Batch:  140 , correct:  3 , loss:  2.284090564967935\n",
      "Batch:  141 , correct:  3 , loss:  2.2841947066525314\n",
      "Batch:  142 , correct:  1 , loss:  2.284296814821043\n",
      "Batch:  143 , correct:  5 , loss:  2.284397480795278\n",
      "Batch:  144 , correct:  4 , loss:  2.283498457102085\n",
      "Batch:  145 , correct:  0 , loss:  2.2836000414076705\n",
      "Batch:  146 , correct:  7 , loss:  2.2837028187949593\n",
      "Batch:  147 , correct:  1 , loss:  2.2838036024947828\n",
      "Batch:  148 , correct:  5 , loss:  2.2839064448518402\n",
      "Batch:  149 , correct:  8 , loss:  2.28400728204918\n",
      "Total Loss =  2.2820639403859\n",
      "\n",
      "EPOCH:  2\n",
      "Batch:  0 , correct:  2 , loss:  2.284109441258103\n",
      "Batch:  1 , correct:  4 , loss:  2.2842116806344337\n",
      "Batch:  2 , correct:  4 , loss:  2.284313992773311\n",
      "Batch:  3 , correct:  3 , loss:  2.2834149840043425\n",
      "Batch:  4 , correct:  4 , loss:  2.2835174680563477\n",
      "Batch:  5 , correct:  3 , loss:  2.2836190455543868\n",
      "Batch:  6 , correct:  7 , loss:  2.2837182929089437\n",
      "Batch:  7 , correct:  2 , loss:  2.2838208154614814\n",
      "Batch:  8 , correct:  3 , loss:  2.2839236472795315\n",
      "Batch:  9 , correct:  3 , loss:  2.2840262198343044\n",
      "Batch:  10 , correct:  6 , loss:  2.2841254957603394\n",
      "Batch:  11 , correct:  5 , loss:  2.2832265158444485\n",
      "Batch:  12 , correct:  7 , loss:  2.2833293520066515\n",
      "Batch:  13 , correct:  3 , loss:  2.283430218462658\n",
      "Batch:  14 , correct:  2 , loss:  2.283533132524618\n",
      "Batch:  15 , correct:  0 , loss:  2.28363480036\n",
      "Batch:  16 , correct:  2 , loss:  2.283737435914013\n",
      "Batch:  17 , correct:  4 , loss:  2.2838383219452014\n",
      "Batch:  18 , correct:  6 , loss:  2.2839412574979763\n",
      "Batch:  19 , correct:  0 , loss:  2.283042329959034\n",
      "Batch:  20 , correct:  1 , loss:  2.2831454251296543\n",
      "Batch:  21 , correct:  4 , loss:  2.283248269063605\n",
      "Batch:  22 , correct:  2 , loss:  2.2833514285460574\n",
      "Batch:  23 , correct:  7 , loss:  2.283454330585916\n",
      "Batch:  24 , correct:  2 , loss:  2.283555278032151\n",
      "Batch:  25 , correct:  5 , loss:  2.2836579324611983\n",
      "Batch:  26 , correct:  6 , loss:  2.2837595776431217\n",
      "Batch:  27 , correct:  0 , loss:  2.2838634826075324\n",
      "Batch:  28 , correct:  2 , loss:  2.2839648889618975\n",
      "Batch:  29 , correct:  0 , loss:  2.2840670557325367\n",
      "Batch:  30 , correct:  3 , loss:  2.2841697100555334\n",
      "Batch:  31 , correct:  2 , loss:  2.2842727856509644\n",
      "Batch:  32 , correct:  2 , loss:  2.2843744052378745\n",
      "Batch:  33 , correct:  3 , loss:  2.283475385624536\n",
      "Batch:  34 , correct:  4 , loss:  2.2835782640888644\n",
      "Batch:  35 , correct:  2 , loss:  2.2836774280045113\n",
      "Batch:  36 , correct:  4 , loss:  2.2837788891904114\n",
      "Batch:  37 , correct:  5 , loss:  2.2838804190224304\n",
      "Batch:  38 , correct:  1 , loss:  2.283982037930785\n",
      "Batch:  39 , correct:  6 , loss:  2.2840828726111275\n",
      "Batch:  40 , correct:  1 , loss:  2.283183909980588\n",
      "Batch:  41 , correct:  3 , loss:  2.283287831568198\n",
      "Batch:  42 , correct:  4 , loss:  2.282389030796264\n",
      "Batch:  43 , correct:  2 , loss:  2.282492037547842\n",
      "Batch:  44 , correct:  6 , loss:  2.28259480203519\n",
      "Batch:  45 , correct:  2 , loss:  2.281696151036167\n",
      "Batch:  46 , correct:  4 , loss:  2.2818002696779343\n",
      "Batch:  47 , correct:  2 , loss:  2.281899594126117\n",
      "Batch:  48 , correct:  3 , loss:  2.2820013968494512\n",
      "Batch:  49 , correct:  3 , loss:  2.2821007879598056\n",
      "Batch:  50 , correct:  3 , loss:  2.282203851127856\n",
      "Batch:  51 , correct:  5 , loss:  2.2823079708443474\n",
      "Batch:  52 , correct:  4 , loss:  2.2824110883231645\n",
      "Batch:  53 , correct:  2 , loss:  2.281512464036806\n",
      "Batch:  54 , correct:  2 , loss:  2.2816157490235462\n",
      "Batch:  55 , correct:  3 , loss:  2.2817191089436344\n",
      "Batch:  56 , correct:  4 , loss:  2.281821341610863\n",
      "Batch:  57 , correct:  4 , loss:  2.281924484179978\n",
      "Batch:  58 , correct:  2 , loss:  2.2820254510186504\n",
      "Batch:  59 , correct:  6 , loss:  2.2821264915721273\n",
      "Batch:  60 , correct:  3 , loss:  2.2822258579107784\n",
      "Batch:  61 , correct:  2 , loss:  2.2823290271248813\n",
      "Batch:  62 , correct:  0 , loss:  2.281430421751762\n",
      "Batch:  63 , correct:  3 , loss:  2.2815299320763134\n",
      "Batch:  64 , correct:  5 , loss:  2.2806314905911873\n",
      "Batch:  65 , correct:  3 , loss:  2.280734343250367\n",
      "Batch:  66 , correct:  3 , loss:  2.2798360660963612\n",
      "Batch:  67 , correct:  2 , loss:  2.2799395952957435\n",
      "Batch:  68 , correct:  1 , loss:  2.279041542417544\n",
      "Batch:  69 , correct:  3 , loss:  2.279143483729347\n",
      "Batch:  70 , correct:  5 , loss:  2.279245439804565\n",
      "Batch:  71 , correct:  5 , loss:  2.2783475216148963\n",
      "Batch:  72 , correct:  4 , loss:  2.2784519670246497\n",
      "Batch:  73 , correct:  4 , loss:  2.2785540662662336\n",
      "Batch:  74 , correct:  2 , loss:  2.2786571418965202\n",
      "Batch:  75 , correct:  3 , loss:  2.278759292259078\n",
      "Batch:  76 , correct:  3 , loss:  2.2778614765555627\n",
      "Batch:  77 , correct:  6 , loss:  2.277963793843489\n",
      "Batch:  78 , correct:  5 , loss:  2.2770661428165457\n",
      "Batch:  79 , correct:  1 , loss:  2.2771698635787883\n",
      "Batch:  80 , correct:  1 , loss:  2.277273669753982\n",
      "Batch:  81 , correct:  1 , loss:  2.277377416810471\n",
      "Batch:  82 , correct:  1 , loss:  2.277480629568721\n",
      "Batch:  83 , correct:  4 , loss:  2.276583075289011\n",
      "Batch:  84 , correct:  2 , loss:  2.276683032774546\n",
      "Batch:  85 , correct:  2 , loss:  2.2767876034597108\n",
      "Batch:  86 , correct:  3 , loss:  2.2768922645633785\n",
      "Batch:  87 , correct:  2 , loss:  2.27699468229038\n",
      "Batch:  88 , correct:  4 , loss:  2.27709673293339\n",
      "Batch:  89 , correct:  1 , loss:  2.2761992633282513\n",
      "Batch:  90 , correct:  4 , loss:  2.276301470019957\n",
      "Batch:  91 , correct:  0 , loss:  2.276406241779089\n",
      "Batch:  92 , correct:  2 , loss:  2.276511081431279\n",
      "Batch:  93 , correct:  5 , loss:  2.2766135732227992\n",
      "Batch:  94 , correct:  4 , loss:  2.2767148803335533\n",
      "Batch:  95 , correct:  2 , loss:  2.2768186746881938\n",
      "Batch:  96 , correct:  1 , loss:  2.276918588021174\n",
      "Batch:  97 , correct:  5 , loss:  2.277022303960971\n",
      "Batch:  98 , correct:  3 , loss:  2.2771222731361\n",
      "Batch:  99 , correct:  1 , loss:  2.2772235806533874\n",
      "Batch:  100 , correct:  5 , loss:  2.277326726281493\n",
      "Batch:  101 , correct:  5 , loss:  2.277426731605595\n",
      "Batch:  102 , correct:  5 , loss:  2.2775280772827755\n",
      "Batch:  103 , correct:  3 , loss:  2.2776304642975793\n",
      "Batch:  104 , correct:  3 , loss:  2.2777336343009926\n",
      "Batch:  105 , correct:  4 , loss:  2.2778356080384063\n",
      "Batch:  106 , correct:  1 , loss:  2.277935614037448\n",
      "Batch:  107 , correct:  3 , loss:  2.2780380223655774\n",
      "Batch:  108 , correct:  2 , loss:  2.2781426295044507\n",
      "Batch:  109 , correct:  5 , loss:  2.2782462471612805\n",
      "Batch:  110 , correct:  5 , loss:  2.2773485332904895\n",
      "Batch:  111 , correct:  3 , loss:  2.2774486420555133\n",
      "Batch:  112 , correct:  2 , loss:  2.2775511510405964\n",
      "Batch:  113 , correct:  1 , loss:  2.2776543383909664\n",
      "Batch:  114 , correct:  4 , loss:  2.2777569069957844\n",
      "Batch:  115 , correct:  2 , loss:  2.276859302171028\n",
      "Batch:  116 , correct:  2 , loss:  2.276962937119381\n",
      "Batch:  117 , correct:  2 , loss:  2.2770631260412397\n",
      "Batch:  118 , correct:  3 , loss:  2.2771651373532946\n",
      "Batch:  119 , correct:  1 , loss:  2.2772678124162415\n",
      "Batch:  120 , correct:  5 , loss:  2.2763703017038175\n",
      "Batch:  121 , correct:  2 , loss:  2.2764722848653216\n",
      "Batch:  122 , correct:  4 , loss:  2.2765770137852983\n",
      "Batch:  123 , correct:  1 , loss:  2.2766772844261385\n",
      "Batch:  124 , correct:  5 , loss:  2.2767809630962708\n",
      "Batch:  125 , correct:  5 , loss:  2.276881289260843\n",
      "Batch:  126 , correct:  2 , loss:  2.2769840235309946\n",
      "Batch:  127 , correct:  4 , loss:  2.2770860576399095\n",
      "Batch:  128 , correct:  3 , loss:  2.277188007758487\n",
      "Batch:  129 , correct:  4 , loss:  2.2772900958344864\n",
      "Batch:  130 , correct:  4 , loss:  2.2773912797127758\n",
      "Batch:  131 , correct:  6 , loss:  2.2774940123760405\n",
      "Batch:  132 , correct:  4 , loss:  2.2765964540625174\n",
      "Batch:  133 , correct:  5 , loss:  2.2766996380448417\n",
      "Batch:  134 , correct:  5 , loss:  2.276801666086254\n",
      "Batch:  135 , correct:  5 , loss:  2.2769049073516214\n",
      "Batch:  136 , correct:  4 , loss:  2.2770095510340753\n",
      "Batch:  137 , correct:  4 , loss:  2.2771123600169174\n",
      "Batch:  138 , correct:  3 , loss:  2.2772159537365715\n",
      "Batch:  139 , correct:  3 , loss:  2.2773196235699804\n",
      "Batch:  140 , correct:  3 , loss:  2.2774233770072883\n",
      "Batch:  141 , correct:  4 , loss:  2.27752454030482\n",
      "Batch:  142 , correct:  0 , loss:  2.277626574728751\n",
      "Batch:  143 , correct:  4 , loss:  2.2777267416902323\n",
      "Batch:  144 , correct:  3 , loss:  2.2778305115694546\n",
      "Batch:  145 , correct:  0 , loss:  2.2779343630350817\n",
      "Batch:  146 , correct:  3 , loss:  2.278036239188567\n",
      "Batch:  147 , correct:  4 , loss:  2.2781401542954614\n",
      "Batch:  148 , correct:  1 , loss:  2.2782420972098882\n",
      "Batch:  149 , correct:  1 , loss:  2.2783431920193298\n",
      "Total Loss =  2.2799167322072957\n",
      "\n",
      "EPOCH:  3\n",
      "Batch:  0 , correct:  3 , loss:  2.278445840342943\n",
      "Batch:  1 , correct:  1 , loss:  2.2785497697704846\n",
      "Batch:  2 , correct:  5 , loss:  2.2786524726034525\n",
      "Batch:  3 , correct:  4 , loss:  2.278755444222707\n",
      "Batch:  4 , correct:  4 , loss:  2.278858211936856\n",
      "Batch:  5 , correct:  8 , loss:  2.2789600762112867\n",
      "Batch:  6 , correct:  2 , loss:  2.279061134559227\n",
      "Batch:  7 , correct:  1 , loss:  2.27916305706887\n",
      "Batch:  8 , correct:  2 , loss:  2.2792630234952527\n",
      "Batch:  9 , correct:  2 , loss:  2.279366890468329\n",
      "Batch:  10 , correct:  3 , loss:  2.279469827682328\n",
      "Batch:  11 , correct:  2 , loss:  2.2795728378420157\n",
      "Batch:  12 , correct:  5 , loss:  2.279675538917242\n",
      "Batch:  13 , correct:  0 , loss:  2.2797772758137023\n",
      "Batch:  14 , correct:  4 , loss:  2.279881438197998\n",
      "Batch:  15 , correct:  3 , loss:  2.27998135068941\n",
      "Batch:  16 , correct:  3 , loss:  2.27908328608434\n",
      "Batch:  17 , correct:  2 , loss:  2.279185137817013\n",
      "Batch:  18 , correct:  5 , loss:  2.2792882066882925\n",
      "Batch:  19 , correct:  3 , loss:  2.279390123934714\n",
      "Batch:  20 , correct:  4 , loss:  2.2794943687691522\n",
      "Batch:  21 , correct:  2 , loss:  2.2785963932000572\n",
      "Batch:  22 , correct:  1 , loss:  2.2786974341971864\n",
      "Batch:  23 , correct:  3 , loss:  2.278800427582205\n",
      "Batch:  24 , correct:  2 , loss:  2.278904794553775\n",
      "Batch:  25 , correct:  5 , loss:  2.2790048082667327\n",
      "Batch:  26 , correct:  2 , loss:  2.2791067860825756\n",
      "Batch:  27 , correct:  4 , loss:  2.279209801849741\n",
      "Batch:  28 , correct:  6 , loss:  2.2793108186095523\n",
      "Batch:  29 , correct:  3 , loss:  2.2794134765730085\n",
      "Batch:  30 , correct:  1 , loss:  2.2795154698161464\n",
      "Batch:  31 , correct:  3 , loss:  2.279617215309591\n",
      "Batch:  32 , correct:  2 , loss:  2.2797192660857353\n",
      "Batch:  33 , correct:  3 , loss:  2.279820279598584\n",
      "Batch:  34 , correct:  2 , loss:  2.2799220608152977\n",
      "Batch:  35 , correct:  3 , loss:  2.28002198541461\n",
      "Batch:  36 , correct:  4 , loss:  2.279123908904078\n",
      "Batch:  37 , correct:  4 , loss:  2.2792250433986148\n",
      "Batch:  38 , correct:  3 , loss:  2.279329320959811\n",
      "Batch:  39 , correct:  3 , loss:  2.27943143543137\n",
      "Batch:  40 , correct:  2 , loss:  2.2785334831349457\n",
      "Batch:  41 , correct:  4 , loss:  2.278634736696444\n",
      "Batch:  42 , correct:  5 , loss:  2.2787369935906234\n",
      "Batch:  43 , correct:  0 , loss:  2.2788399904307806\n",
      "Batch:  44 , correct:  4 , loss:  2.2789443459335637\n",
      "Batch:  45 , correct:  4 , loss:  2.279046642793395\n",
      "Batch:  46 , correct:  2 , loss:  2.279149015919743\n",
      "Batch:  47 , correct:  3 , loss:  2.2792520310469753\n",
      "Batch:  48 , correct:  3 , loss:  2.2793556016371896\n",
      "Batch:  49 , correct:  4 , loss:  2.2794584464219683\n",
      "Batch:  50 , correct:  2 , loss:  2.2795596182285354\n",
      "Batch:  51 , correct:  2 , loss:  2.2796626491694942\n",
      "Batch:  52 , correct:  4 , loss:  2.2797669419190543\n",
      "Batch:  53 , correct:  6 , loss:  2.279869275811974\n",
      "Batch:  54 , correct:  3 , loss:  2.279971723667351\n",
      "Batch:  55 , correct:  1 , loss:  2.279073652807552\n",
      "Batch:  56 , correct:  0 , loss:  2.279176793460406\n",
      "Batch:  57 , correct:  4 , loss:  2.2792792454360096\n",
      "Batch:  58 , correct:  2 , loss:  2.279382812377829\n",
      "Batch:  59 , correct:  1 , loss:  2.279485981348415\n",
      "Batch:  60 , correct:  2 , loss:  2.279589607896263\n",
      "Batch:  61 , correct:  3 , loss:  2.279692845746636\n",
      "Batch:  62 , correct:  4 , loss:  2.2797939699816108\n",
      "Batch:  63 , correct:  4 , loss:  2.279897266994687\n",
      "Batch:  64 , correct:  4 , loss:  2.2799984471050565\n",
      "Batch:  65 , correct:  3 , loss:  2.2801008389107573\n",
      "Batch:  66 , correct:  5 , loss:  2.279202744348039\n",
      "Batch:  67 , correct:  3 , loss:  2.2793070229053476\n",
      "Batch:  68 , correct:  3 , loss:  2.2794086311233523\n",
      "Batch:  69 , correct:  2 , loss:  2.279512274671547\n",
      "Batch:  70 , correct:  2 , loss:  2.2786143084803094\n",
      "Batch:  71 , correct:  5 , loss:  2.2787156537411764\n",
      "Batch:  72 , correct:  0 , loss:  2.27881906940104\n",
      "Batch:  73 , correct:  4 , loss:  2.2789204710716358\n",
      "Batch:  74 , correct:  4 , loss:  2.2790229146712093\n",
      "Batch:  75 , correct:  3 , loss:  2.27912539525447\n",
      "Batch:  76 , correct:  5 , loss:  2.279226839831174\n",
      "Batch:  77 , correct:  3 , loss:  2.2793283715941066\n",
      "Batch:  78 , correct:  2 , loss:  2.279432629256507\n",
      "Batch:  79 , correct:  6 , loss:  2.279535128940058\n",
      "Batch:  80 , correct:  0 , loss:  2.279636694776424\n",
      "Batch:  81 , correct:  4 , loss:  2.2797392531341374\n",
      "Batch:  82 , correct:  4 , loss:  2.2798435358368287\n",
      "Batch:  83 , correct:  2 , loss:  2.2799460917926537\n",
      "Batch:  84 , correct:  5 , loss:  2.2790480306142284\n",
      "Batch:  85 , correct:  2 , loss:  2.2791507143135754\n",
      "Batch:  86 , correct:  1 , loss:  2.2792523553053523\n",
      "Batch:  87 , correct:  3 , loss:  2.2793538819063186\n",
      "Batch:  88 , correct:  4 , loss:  2.2794571621131463\n",
      "Batch:  89 , correct:  4 , loss:  2.278559204124688\n",
      "Batch:  90 , correct:  1 , loss:  2.2786608688457575\n",
      "Batch:  91 , correct:  2 , loss:  2.2787652660083877\n",
      "Batch:  92 , correct:  0 , loss:  2.278867935388301\n",
      "Batch:  93 , correct:  2 , loss:  2.2789696430260347\n",
      "Batch:  94 , correct:  3 , loss:  2.279073175359338\n",
      "Batch:  95 , correct:  3 , loss:  2.279174931850493\n",
      "Batch:  96 , correct:  4 , loss:  2.27927852677369\n",
      "Batch:  97 , correct:  3 , loss:  2.279381199010263\n",
      "Batch:  98 , correct:  2 , loss:  2.2794838202109373\n",
      "Batch:  99 , correct:  4 , loss:  2.27958707979666\n",
      "Batch:  100 , correct:  6 , loss:  2.2796864889090256\n",
      "Batch:  101 , correct:  5 , loss:  2.278788484007484\n",
      "Batch:  102 , correct:  2 , loss:  2.2788921539232416\n",
      "Batch:  103 , correct:  1 , loss:  2.278995911102948\n",
      "Batch:  104 , correct:  5 , loss:  2.27909927324034\n",
      "Batch:  105 , correct:  5 , loss:  2.279201477483312\n",
      "Batch:  106 , correct:  3 , loss:  2.279304140152388\n",
      "Batch:  107 , correct:  4 , loss:  2.2784062090920107\n",
      "Batch:  108 , correct:  4 , loss:  2.2785078095595885\n",
      "Batch:  109 , correct:  3 , loss:  2.2786106084709834\n",
      "Batch:  110 , correct:  3 , loss:  2.2777128215544673\n",
      "Batch:  111 , correct:  4 , loss:  2.277816717157589\n",
      "Batch:  112 , correct:  5 , loss:  2.2779194854818448\n",
      "Batch:  113 , correct:  5 , loss:  2.278021828669459\n",
      "Batch:  114 , correct:  4 , loss:  2.27812576456131\n",
      "Batch:  115 , correct:  3 , loss:  2.2782292030973217\n",
      "Batch:  116 , correct:  2 , loss:  2.2783327209330277\n",
      "Batch:  117 , correct:  3 , loss:  2.2784350844388945\n",
      "Batch:  118 , correct:  2 , loss:  2.278536682790088\n",
      "Batch:  119 , correct:  2 , loss:  2.2786391127468657\n",
      "Batch:  120 , correct:  4 , loss:  2.2787418855039685\n",
      "Batch:  121 , correct:  3 , loss:  2.278843523151367\n",
      "Batch:  122 , correct:  6 , loss:  2.278947391003448\n",
      "Batch:  123 , correct:  2 , loss:  2.279048987839036\n",
      "Batch:  124 , correct:  2 , loss:  2.2791529175766096\n",
      "Batch:  125 , correct:  2 , loss:  2.279254568853295\n",
      "Batch:  126 , correct:  2 , loss:  2.2793579849999857\n",
      "Batch:  127 , correct:  3 , loss:  2.279457251347498\n",
      "Batch:  128 , correct:  5 , loss:  2.279559598653771\n",
      "Batch:  129 , correct:  8 , loss:  2.279658919949243\n",
      "Batch:  130 , correct:  2 , loss:  2.2787609287304647\n",
      "Batch:  131 , correct:  4 , loss:  2.278862560905647\n",
      "Batch:  132 , correct:  1 , loss:  2.2789620385146607\n",
      "Batch:  133 , correct:  3 , loss:  2.2790637378062395\n",
      "Batch:  134 , correct:  6 , loss:  2.279165417479478\n",
      "Batch:  135 , correct:  1 , loss:  2.279269408169665\n",
      "Batch:  136 , correct:  2 , loss:  2.2793718938979604\n",
      "Batch:  137 , correct:  6 , loss:  2.279473605887545\n",
      "Batch:  138 , correct:  2 , loss:  2.279575398995205\n",
      "Batch:  139 , correct:  3 , loss:  2.2786774172077893\n",
      "Batch:  140 , correct:  4 , loss:  2.2777796184168384\n",
      "Batch:  141 , correct:  2 , loss:  2.27788379525979\n",
      "Batch:  142 , correct:  6 , loss:  2.277988053293901\n",
      "Batch:  143 , correct:  2 , loss:  2.278090697167858\n",
      "Batch:  144 , correct:  3 , loss:  2.278193419920968\n",
      "Batch:  145 , correct:  2 , loss:  2.277295722227379\n",
      "Batch:  146 , correct:  5 , loss:  2.277397561615541\n",
      "Batch:  147 , correct:  4 , loss:  2.2775010580855013\n",
      "Batch:  148 , correct:  4 , loss:  2.2776029583725714\n",
      "Batch:  149 , correct:  1 , loss:  2.2777057851273166\n",
      "Total Loss =  2.2790680393802756\n",
      "\n",
      "EPOCH:  4\n",
      "Batch:  0 , correct:  2 , loss:  2.2778053000670946\n",
      "Batch:  1 , correct:  3 , loss:  2.2779072314392845\n",
      "Batch:  2 , correct:  4 , loss:  2.2780091569224084\n",
      "Batch:  3 , correct:  2 , loss:  2.2771114924741416\n",
      "Batch:  4 , correct:  4 , loss:  2.2772158130996325\n",
      "Batch:  5 , correct:  5 , loss:  2.2763183120289554\n",
      "Batch:  6 , correct:  5 , loss:  2.2754209971352193\n",
      "Batch:  7 , correct:  6 , loss:  2.2755235648336383\n",
      "Batch:  8 , correct:  2 , loss:  2.275625771988728\n",
      "Batch:  9 , correct:  5 , loss:  2.2757294255572305\n",
      "Batch:  10 , correct:  4 , loss:  2.2758339151098594\n",
      "Batch:  11 , correct:  1 , loss:  2.275936664786935\n",
      "Batch:  12 , correct:  1 , loss:  2.2760388896980506\n",
      "Batch:  13 , correct:  5 , loss:  2.2761416975711604\n",
      "Batch:  14 , correct:  2 , loss:  2.27524441443899\n",
      "Batch:  15 , correct:  4 , loss:  2.2753470221685013\n",
      "Batch:  16 , correct:  2 , loss:  2.2744498962019657\n",
      "Batch:  17 , correct:  0 , loss:  2.2745529312445205\n",
      "Batch:  18 , correct:  3 , loss:  2.27465697050758\n",
      "Batch:  19 , correct:  2 , loss:  2.274760063885786\n",
      "Batch:  20 , correct:  0 , loss:  2.274864654764188\n",
      "Batch:  21 , correct:  5 , loss:  2.2749676444836244\n",
      "Batch:  22 , correct:  6 , loss:  2.2750673227403824\n",
      "Batch:  23 , correct:  6 , loss:  2.2741702593539324\n",
      "Batch:  24 , correct:  2 , loss:  2.274270103733681\n",
      "Batch:  25 , correct:  3 , loss:  2.2743732738481226\n",
      "Batch:  26 , correct:  2 , loss:  2.2744756227021097\n",
      "Batch:  27 , correct:  1 , loss:  2.274579307218467\n",
      "Batch:  28 , correct:  4 , loss:  2.274683941736286\n",
      "Batch:  29 , correct:  4 , loss:  2.27478601971394\n",
      "Batch:  30 , correct:  1 , loss:  2.274888616877994\n",
      "Batch:  31 , correct:  1 , loss:  2.2749917604690313\n",
      "Batch:  32 , correct:  3 , loss:  2.2750957219073906\n",
      "Batch:  33 , correct:  1 , loss:  2.2752003465146413\n",
      "Batch:  34 , correct:  3 , loss:  2.2753024225459617\n",
      "Batch:  35 , correct:  4 , loss:  2.275405326378297\n",
      "Batch:  36 , correct:  1 , loss:  2.2745081965471763\n",
      "Batch:  37 , correct:  4 , loss:  2.2746122442595618\n",
      "Batch:  38 , correct:  3 , loss:  2.2747152893272666\n",
      "Batch:  39 , correct:  5 , loss:  2.2748175736391354\n",
      "Batch:  40 , correct:  2 , loss:  2.2749206733486047\n",
      "Batch:  41 , correct:  0 , loss:  2.275024275459602\n",
      "Batch:  42 , correct:  0 , loss:  2.275127433412373\n",
      "Batch:  43 , correct:  2 , loss:  2.275227096054007\n",
      "Batch:  44 , correct:  5 , loss:  2.2753316982663074\n",
      "Batch:  45 , correct:  2 , loss:  2.2754347609499503\n",
      "Batch:  46 , correct:  1 , loss:  2.2755379358118044\n",
      "Batch:  47 , correct:  4 , loss:  2.2756415075220806\n",
      "Batch:  48 , correct:  2 , loss:  2.2757437123444775\n",
      "Batch:  49 , correct:  4 , loss:  2.2758473504887906\n",
      "Batch:  50 , correct:  3 , loss:  2.274950125998115\n",
      "Batch:  51 , correct:  2 , loss:  2.2750547886098254\n",
      "Batch:  52 , correct:  2 , loss:  2.275158562237082\n",
      "Batch:  53 , correct:  4 , loss:  2.2742614855846353\n",
      "Batch:  54 , correct:  7 , loss:  2.2743654892569816\n",
      "Batch:  55 , correct:  1 , loss:  2.2734685755000905\n",
      "Batch:  56 , correct:  1 , loss:  2.273571809989027\n",
      "Batch:  57 , correct:  6 , loss:  2.2736755025201894\n",
      "Batch:  58 , correct:  1 , loss:  2.273775024751533\n",
      "Batch:  59 , correct:  4 , loss:  2.2738770496600536\n",
      "Batch:  60 , correct:  7 , loss:  2.273981092469168\n",
      "Batch:  61 , correct:  2 , loss:  2.273084345296359\n",
      "Batch:  62 , correct:  2 , loss:  2.2731866159374072\n",
      "Batch:  63 , correct:  4 , loss:  2.2722900218579976\n",
      "Batch:  64 , correct:  1 , loss:  2.2723897249154046\n",
      "Batch:  65 , correct:  3 , loss:  2.272491931734953\n",
      "Batch:  66 , correct:  2 , loss:  2.272596160449875\n",
      "Batch:  67 , correct:  3 , loss:  2.2726985384279805\n",
      "Batch:  68 , correct:  5 , loss:  2.2728036697429177\n",
      "Batch:  69 , correct:  2 , loss:  2.2729076567954305\n",
      "Batch:  70 , correct:  3 , loss:  2.272011132042076\n",
      "Batch:  71 , correct:  2 , loss:  2.272114985875173\n",
      "Batch:  72 , correct:  0 , loss:  2.2722192894247613\n",
      "Batch:  73 , correct:  3 , loss:  2.2723190071772414\n",
      "Batch:  74 , correct:  9 , loss:  2.2724233742445943\n",
      "Batch:  75 , correct:  2 , loss:  2.2725256785794934\n",
      "Batch:  76 , correct:  3 , loss:  2.272628731970653\n",
      "Batch:  77 , correct:  3 , loss:  2.2727310970252326\n",
      "Batch:  78 , correct:  5 , loss:  2.271834608427786\n",
      "Batch:  79 , correct:  4 , loss:  2.2719344072368273\n",
      "Batch:  80 , correct:  3 , loss:  2.272037581571955\n",
      "Batch:  81 , correct:  6 , loss:  2.271141228764806\n",
      "Batch:  82 , correct:  4 , loss:  2.27024506663873\n",
      "Batch:  83 , correct:  5 , loss:  2.270347626093786\n",
      "Batch:  84 , correct:  2 , loss:  2.2704502536227293\n",
      "Batch:  85 , correct:  4 , loss:  2.2705525588809263\n",
      "Batch:  86 , correct:  6 , loss:  2.270657828536182\n",
      "Batch:  87 , correct:  3 , loss:  2.27075777127919\n",
      "Batch:  88 , correct:  3 , loss:  2.270861893104927\n",
      "Batch:  89 , correct:  2 , loss:  2.269965785473148\n",
      "Batch:  90 , correct:  1 , loss:  2.270071171530825\n",
      "Batch:  91 , correct:  1 , loss:  2.2701735613311893\n",
      "Batch:  92 , correct:  5 , loss:  2.2702761278660346\n",
      "Batch:  93 , correct:  4 , loss:  2.270381560665347\n",
      "Batch:  94 , correct:  4 , loss:  2.270483990412339\n",
      "Batch:  95 , correct:  5 , loss:  2.270588436120951\n",
      "Batch:  96 , correct:  2 , loss:  2.2706909199431937\n",
      "Batch:  97 , correct:  4 , loss:  2.2707954207209955\n",
      "Batch:  98 , correct:  5 , loss:  2.270897963259624\n",
      "Batch:  99 , correct:  5 , loss:  2.271002055984089\n",
      "Batch:  100 , correct:  4 , loss:  2.27110581420138\n",
      "Batch:  101 , correct:  3 , loss:  2.271211173100864\n",
      "Batch:  102 , correct:  1 , loss:  2.271313638210835\n",
      "Batch:  103 , correct:  4 , loss:  2.271417751385648\n",
      "Batch:  104 , correct:  3 , loss:  2.2715215225916743\n",
      "Batch:  105 , correct:  7 , loss:  2.2716239275167784\n",
      "Batch:  106 , correct:  5 , loss:  2.2717292831974913\n",
      "Batch:  107 , correct:  3 , loss:  2.2708329922108095\n",
      "Batch:  108 , correct:  3 , loss:  2.27093747115872\n",
      "Batch:  109 , correct:  0 , loss:  2.2710420257521244\n",
      "Batch:  110 , correct:  2 , loss:  2.2711445267562502\n",
      "Batch:  111 , correct:  1 , loss:  2.2712471099962506\n",
      "Batch:  112 , correct:  6 , loss:  2.2713495696800576\n",
      "Batch:  113 , correct:  2 , loss:  2.271453359843563\n",
      "Batch:  114 , correct:  2 , loss:  2.2715579089826106\n",
      "Batch:  115 , correct:  2 , loss:  2.2716604118550165\n",
      "Batch:  116 , correct:  3 , loss:  2.2717629885268344\n",
      "Batch:  117 , correct:  1 , loss:  2.2718659120960925\n",
      "Batch:  118 , correct:  2 , loss:  2.271970482255973\n",
      "Batch:  119 , correct:  2 , loss:  2.272075773749205\n",
      "Batch:  120 , correct:  6 , loss:  2.272179523323203\n",
      "Batch:  121 , correct:  3 , loss:  2.2722820931241863\n",
      "Batch:  122 , correct:  1 , loss:  2.2723850079734227\n",
      "Batch:  123 , correct:  4 , loss:  2.2724874473777326\n",
      "Batch:  124 , correct:  3 , loss:  2.2725913171547876\n",
      "Batch:  125 , correct:  2 , loss:  2.2716948494958795\n",
      "Batch:  126 , correct:  4 , loss:  2.271797527508354\n",
      "Batch:  127 , correct:  2 , loss:  2.271897064316508\n",
      "Batch:  128 , correct:  3 , loss:  2.271000741860237\n",
      "Batch:  129 , correct:  4 , loss:  2.271104611735828\n",
      "Batch:  130 , correct:  3 , loss:  2.2712069076209573\n",
      "Batch:  131 , correct:  2 , loss:  2.271311517351572\n",
      "Batch:  132 , correct:  6 , loss:  2.2714142669098383\n",
      "Batch:  133 , correct:  2 , loss:  2.2715189273149203\n",
      "Batch:  134 , correct:  3 , loss:  2.2716212392226978\n",
      "Batch:  135 , correct:  1 , loss:  2.271725190094402\n",
      "Batch:  136 , correct:  4 , loss:  2.2718276599397558\n",
      "Batch:  137 , correct:  2 , loss:  2.2719304186448404\n",
      "Batch:  138 , correct:  4 , loss:  2.2720333166986197\n",
      "Batch:  139 , correct:  3 , loss:  2.272137071999064\n",
      "Batch:  140 , correct:  4 , loss:  2.2722395664721056\n",
      "Batch:  141 , correct:  4 , loss:  2.272344172038884\n",
      "Batch:  142 , correct:  4 , loss:  2.272443604602789\n",
      "Batch:  143 , correct:  2 , loss:  2.272547491805301\n",
      "Batch:  144 , correct:  3 , loss:  2.27265256701084\n",
      "Batch:  145 , correct:  4 , loss:  2.2727563069334655\n",
      "Batch:  146 , correct:  6 , loss:  2.27286022688536\n",
      "Batch:  147 , correct:  4 , loss:  2.2729653447760607\n",
      "Batch:  148 , correct:  5 , loss:  2.2730693261060217\n",
      "Batch:  149 , correct:  2 , loss:  2.2731744965884513\n",
      "Total Loss =  2.273113162367153\n",
      "\n",
      "EPOCH:  5\n",
      "Batch:  0 , correct:  2 , loss:  2.2732772429972337\n",
      "Batch:  1 , correct:  4 , loss:  2.2733765967519894\n",
      "Batch:  2 , correct:  3 , loss:  2.272479966660356\n",
      "Batch:  3 , correct:  7 , loss:  2.271583530992446\n",
      "Batch:  4 , correct:  3 , loss:  2.271685731019988\n",
      "Batch:  5 , correct:  3 , loss:  2.2707894508516824\n",
      "Batch:  6 , correct:  3 , loss:  2.2708918206464843\n",
      "Batch:  7 , correct:  5 , loss:  2.27099437616969\n",
      "Batch:  8 , correct:  3 , loss:  2.2710971015239663\n",
      "Batch:  9 , correct:  5 , loss:  2.271201261977778\n",
      "Batch:  10 , correct:  4 , loss:  2.2703050870721575\n",
      "Batch:  11 , correct:  1 , loss:  2.270409411077768\n",
      "Batch:  12 , correct:  4 , loss:  2.2705124196236595\n",
      "Batch:  13 , correct:  5 , loss:  2.2696163843476826\n",
      "Batch:  14 , correct:  2 , loss:  2.2697192957819095\n",
      "Batch:  15 , correct:  3 , loss:  2.269824008913486\n",
      "Batch:  16 , correct:  2 , loss:  2.2699269779245363\n",
      "Batch:  17 , correct:  2 , loss:  2.2700313791224134\n",
      "Batch:  18 , correct:  4 , loss:  2.2691354405407425\n",
      "Batch:  19 , correct:  3 , loss:  2.2692351404474946\n",
      "Batch:  20 , correct:  4 , loss:  2.2693382318188493\n",
      "Batch:  21 , correct:  1 , loss:  2.2694379891258123\n",
      "Batch:  22 , correct:  6 , loss:  2.2695411023997054\n",
      "Batch:  23 , correct:  3 , loss:  2.2696442329355575\n",
      "Batch:  24 , correct:  1 , loss:  2.2697480884132006\n",
      "Batch:  25 , correct:  3 , loss:  2.2698527339954504\n",
      "Batch:  26 , correct:  2 , loss:  2.269955208926509\n",
      "Batch:  27 , correct:  2 , loss:  2.269059444436822\n",
      "Batch:  28 , correct:  5 , loss:  2.2691593962765495\n",
      "Batch:  29 , correct:  3 , loss:  2.268263798241624\n",
      "Batch:  30 , correct:  5 , loss:  2.268367168323161\n",
      "Batch:  31 , correct:  3 , loss:  2.268472041351453\n",
      "Batch:  32 , correct:  2 , loss:  2.2685767457586072\n",
      "Batch:  33 , correct:  0 , loss:  2.2686816676744472\n",
      "Batch:  34 , correct:  2 , loss:  2.26878575781842\n",
      "Batch:  35 , correct:  2 , loss:  2.2688857586559403\n",
      "Batch:  36 , correct:  2 , loss:  2.2689879371866266\n",
      "Batch:  37 , correct:  5 , loss:  2.269092075270529\n",
      "Batch:  38 , correct:  4 , loss:  2.2691967528029116\n",
      "Batch:  39 , correct:  2 , loss:  2.26830114803261\n",
      "Batch:  40 , correct:  2 , loss:  2.267405732422764\n",
      "Batch:  41 , correct:  4 , loss:  2.267510959342487\n",
      "Batch:  42 , correct:  3 , loss:  2.2676111223861595\n",
      "Batch:  43 , correct:  2 , loss:  2.2677154332751814\n",
      "Batch:  44 , correct:  2 , loss:  2.267820299289205\n",
      "Batch:  45 , correct:  3 , loss:  2.2679246718976187\n",
      "Batch:  46 , correct:  4 , loss:  2.2680295949138682\n",
      "Batch:  47 , correct:  1 , loss:  2.2681297566613097\n",
      "Batch:  48 , correct:  4 , loss:  2.2682330558680124\n",
      "Batch:  49 , correct:  3 , loss:  2.2683355709544815\n",
      "Batch:  50 , correct:  7 , loss:  2.2684404487141485\n",
      "Batch:  51 , correct:  1 , loss:  2.26854327080243\n",
      "Batch:  52 , correct:  4 , loss:  2.268645821218027\n",
      "Batch:  53 , correct:  2 , loss:  2.2687507017340516\n",
      "Batch:  54 , correct:  3 , loss:  2.2688539845536257\n",
      "Batch:  55 , correct:  7 , loss:  2.2689588547129147\n",
      "Batch:  56 , correct:  4 , loss:  2.2690638051015726\n",
      "Batch:  57 , correct:  1 , loss:  2.2681682336704485\n",
      "Batch:  58 , correct:  1 , loss:  2.2682708597487413\n",
      "Batch:  59 , correct:  2 , loss:  2.2683759595002133\n",
      "Batch:  60 , correct:  3 , loss:  2.2684809525826486\n",
      "Batch:  61 , correct:  2 , loss:  2.2685859183455848\n",
      "Batch:  62 , correct:  2 , loss:  2.2676905639104263\n",
      "Batch:  63 , correct:  5 , loss:  2.2677947740044733\n",
      "Batch:  64 , correct:  3 , loss:  2.2678996614428035\n",
      "Batch:  65 , correct:  3 , loss:  2.2680046245697616\n",
      "Batch:  66 , correct:  3 , loss:  2.2681073306598165\n",
      "Batch:  67 , correct:  6 , loss:  2.268209354344681\n",
      "Batch:  68 , correct:  5 , loss:  2.2683095282743277\n",
      "Batch:  69 , correct:  1 , loss:  2.2684127490818433\n",
      "Batch:  70 , correct:  3 , loss:  2.2685129856132473\n",
      "Batch:  71 , correct:  1 , loss:  2.26861794577369\n",
      "Batch:  72 , correct:  6 , loss:  2.268720630903017\n",
      "Batch:  73 , correct:  3 , loss:  2.2688247381522073\n",
      "Batch:  74 , correct:  6 , loss:  2.268928921906247\n",
      "Batch:  75 , correct:  3 , loss:  2.2690337771662064\n",
      "Batch:  76 , correct:  4 , loss:  2.2681383314592436\n",
      "Batch:  77 , correct:  5 , loss:  2.268243354607791\n",
      "Batch:  78 , correct:  3 , loss:  2.2683484525284334\n",
      "Batch:  79 , correct:  3 , loss:  2.2684527383300526\n",
      "Batch:  80 , correct:  4 , loss:  2.268554695661993\n",
      "Batch:  81 , correct:  3 , loss:  2.268657400562559\n",
      "Batch:  82 , correct:  3 , loss:  2.267762030342506\n",
      "Batch:  83 , correct:  2 , loss:  2.2678670521058493\n",
      "Batch:  84 , correct:  3 , loss:  2.267972046666666\n",
      "Batch:  85 , correct:  3 , loss:  2.2680771185945336\n",
      "Batch:  86 , correct:  3 , loss:  2.2681799205162383\n",
      "Batch:  87 , correct:  4 , loss:  2.268280111205377\n",
      "Batch:  88 , correct:  5 , loss:  2.2683803887257956\n",
      "Batch:  89 , correct:  4 , loss:  2.268480737558398\n",
      "Batch:  90 , correct:  3 , loss:  2.268583555297162\n",
      "Batch:  91 , correct:  2 , loss:  2.2686854927449605\n",
      "Batch:  92 , correct:  2 , loss:  2.268788549685029\n",
      "Batch:  93 , correct:  2 , loss:  2.2688914020204853\n",
      "Batch:  94 , correct:  3 , loss:  2.2689943467473124\n",
      "Batch:  95 , correct:  3 , loss:  2.2690993195477946\n",
      "Batch:  96 , correct:  4 , loss:  2.2692023898107974\n",
      "Batch:  97 , correct:  1 , loss:  2.269307223958469\n",
      "Batch:  98 , correct:  2 , loss:  2.269411312917382\n",
      "Batch:  99 , correct:  4 , loss:  2.269516210484125\n",
      "Batch:  100 , correct:  2 , loss:  2.2696192979350487\n",
      "Batch:  101 , correct:  4 , loss:  2.2687237409148766\n",
      "Batch:  102 , correct:  6 , loss:  2.268828772672058\n",
      "Batch:  103 , correct:  1 , loss:  2.267933370909147\n",
      "Batch:  104 , correct:  2 , loss:  2.2680384620902365\n",
      "Batch:  105 , correct:  4 , loss:  2.2681436285163312\n",
      "Batch:  106 , correct:  5 , loss:  2.2682487818037456\n",
      "Batch:  107 , correct:  2 , loss:  2.268351237985186\n",
      "Batch:  108 , correct:  1 , loss:  2.268453130554246\n",
      "Batch:  109 , correct:  3 , loss:  2.2685563369960695\n",
      "Batch:  110 , correct:  4 , loss:  2.268658290592779\n",
      "Batch:  111 , correct:  5 , loss:  2.267762927254483\n",
      "Batch:  112 , correct:  3 , loss:  2.2678681533708818\n",
      "Batch:  113 , correct:  1 , loss:  2.2679733661205383\n",
      "Batch:  114 , correct:  4 , loss:  2.268078656961277\n",
      "Batch:  115 , correct:  4 , loss:  2.268181148765737\n",
      "Batch:  116 , correct:  1 , loss:  2.2682831781376405\n",
      "Batch:  117 , correct:  2 , loss:  2.268387979612149\n",
      "Batch:  118 , correct:  5 , loss:  2.2684920618321494\n",
      "Batch:  119 , correct:  2 , loss:  2.2685972389558757\n",
      "Batch:  120 , correct:  4 , loss:  2.2687000516818143\n",
      "Batch:  121 , correct:  5 , loss:  2.2688020741972936\n",
      "Batch:  122 , correct:  3 , loss:  2.2689052152772295\n",
      "Batch:  123 , correct:  1 , loss:  2.2690084402895336\n",
      "Batch:  124 , correct:  2 , loss:  2.2691112661815747\n",
      "Batch:  125 , correct:  2 , loss:  2.2692141722199306\n",
      "Batch:  126 , correct:  1 , loss:  2.2693193193586456\n",
      "Batch:  127 , correct:  3 , loss:  2.2694222443511363\n",
      "Batch:  128 , correct:  1 , loss:  2.269525139396664\n",
      "Batch:  129 , correct:  1 , loss:  2.2696301124931035\n",
      "Batch:  130 , correct:  4 , loss:  2.2697301945686217\n",
      "Batch:  131 , correct:  6 , loss:  2.26983244470687\n",
      "Batch:  132 , correct:  2 , loss:  2.269935357148206\n",
      "Batch:  133 , correct:  3 , loss:  2.270038200495322\n",
      "Batch:  134 , correct:  3 , loss:  2.2701411699541576\n",
      "Batch:  135 , correct:  5 , loss:  2.2692457669720114\n",
      "Batch:  136 , correct:  3 , loss:  2.2693459333872323\n",
      "Batch:  137 , correct:  3 , loss:  2.2684506845814125\n",
      "Batch:  138 , correct:  2 , loss:  2.268555060394515\n",
      "Batch:  139 , correct:  3 , loss:  2.268657276757633\n",
      "Batch:  140 , correct:  1 , loss:  2.2687604345961443\n",
      "Batch:  141 , correct:  5 , loss:  2.2688634276729105\n",
      "Batch:  142 , correct:  1 , loss:  2.2689666368884884\n",
      "Batch:  143 , correct:  7 , loss:  2.269071632124666\n",
      "Batch:  144 , correct:  6 , loss:  2.2691718306884217\n",
      "Batch:  145 , correct:  4 , loss:  2.269274031878891\n",
      "Batch:  146 , correct:  6 , loss:  2.268378807821203\n",
      "Batch:  147 , correct:  2 , loss:  2.2684821178327352\n",
      "Batch:  148 , correct:  1 , loss:  2.2685871520477208\n",
      "Batch:  149 , correct:  3 , loss:  2.268690523874257\n",
      "Total Loss =  2.268954887741895\n",
      "\n",
      "EPOCH:  6\n",
      "Batch:  0 , correct:  2 , loss:  2.267795419225581\n",
      "Batch:  1 , correct:  3 , loss:  2.2678985108971657\n",
      "Batch:  2 , correct:  3 , loss:  2.268000875829881\n",
      "Batch:  3 , correct:  2 , loss:  2.268104061600684\n",
      "Batch:  4 , correct:  3 , loss:  2.2682062835843624\n",
      "Batch:  5 , correct:  3 , loss:  2.2683085463769896\n",
      "Batch:  6 , correct:  1 , loss:  2.2674138248275177\n",
      "Batch:  7 , correct:  1 , loss:  2.2675161896640055\n",
      "Batch:  8 , correct:  3 , loss:  2.26761859638459\n",
      "Batch:  9 , correct:  5 , loss:  2.26772108110281\n",
      "Batch:  10 , correct:  2 , loss:  2.267825903438768\n",
      "Batch:  11 , correct:  3 , loss:  2.267929241169999\n",
      "Batch:  12 , correct:  3 , loss:  2.2680328885866925\n",
      "Batch:  13 , correct:  3 , loss:  2.2681367644464943\n",
      "Batch:  14 , correct:  2 , loss:  2.2682401293722365\n",
      "Batch:  15 , correct:  3 , loss:  2.268342432990544\n",
      "Batch:  16 , correct:  0 , loss:  2.268447229967333\n",
      "Batch:  17 , correct:  3 , loss:  2.268550642184801\n",
      "Batch:  18 , correct:  4 , loss:  2.2686509357219236\n",
      "Batch:  19 , correct:  7 , loss:  2.268751314218122\n",
      "Batch:  20 , correct:  4 , loss:  2.268855146661212\n",
      "Batch:  21 , correct:  4 , loss:  2.2689574883634775\n",
      "Batch:  22 , correct:  3 , loss:  2.269057904233767\n",
      "Batch:  23 , correct:  3 , loss:  2.2691617766476297\n",
      "Batch:  24 , correct:  2 , loss:  2.2692652807603424\n",
      "Batch:  25 , correct:  4 , loss:  2.269367478886619\n",
      "Batch:  26 , correct:  3 , loss:  2.269467912549069\n",
      "Batch:  27 , correct:  1 , loss:  2.2695712187522075\n",
      "Batch:  28 , correct:  1 , loss:  2.269675776969418\n",
      "Batch:  29 , correct:  3 , loss:  2.2697779943284497\n",
      "Batch:  30 , correct:  6 , loss:  2.2688829673585387\n",
      "Batch:  31 , correct:  2 , loss:  2.2689865356721035\n",
      "Batch:  32 , correct:  1 , loss:  2.2690901773275445\n",
      "Batch:  33 , correct:  2 , loss:  2.2691948278713654\n",
      "Batch:  34 , correct:  0 , loss:  2.2692986730389078\n",
      "Batch:  35 , correct:  4 , loss:  2.269402362130338\n",
      "Batch:  36 , correct:  4 , loss:  2.2695062665893766\n",
      "Batch:  37 , correct:  6 , loss:  2.2696066797385566\n",
      "Batch:  38 , correct:  6 , loss:  2.2697071670933813\n",
      "Batch:  39 , correct:  4 , loss:  2.269809383834514\n",
      "Batch:  40 , correct:  5 , loss:  2.2699139945079607\n",
      "Batch:  41 , correct:  4 , loss:  2.2700172153397125\n",
      "Batch:  42 , correct:  3 , loss:  2.2701194681782058\n",
      "Batch:  43 , correct:  1 , loss:  2.2702227499382572\n",
      "Batch:  44 , correct:  1 , loss:  2.270326331114265\n",
      "Batch:  45 , correct:  4 , loss:  2.2704301533297717\n",
      "Batch:  46 , correct:  3 , loss:  2.2695349935594784\n",
      "Batch:  47 , correct:  4 , loss:  2.2686400227605934\n",
      "Batch:  48 , correct:  2 , loss:  2.2677452334078825\n",
      "Batch:  49 , correct:  1 , loss:  2.2678500579205\n",
      "Batch:  50 , correct:  3 , loss:  2.2679507163889947\n",
      "Batch:  51 , correct:  2 , loss:  2.2680548186972156\n",
      "Batch:  52 , correct:  2 , loss:  2.2681585677009912\n",
      "Batch:  53 , correct:  4 , loss:  2.2682620426542286\n",
      "Batch:  54 , correct:  2 , loss:  2.268365824810062\n",
      "Batch:  55 , correct:  5 , loss:  2.2684706301075384\n",
      "Batch:  56 , correct:  3 , loss:  2.2685712592535157\n",
      "Batch:  57 , correct:  3 , loss:  2.267676483437365\n",
      "Batch:  58 , correct:  6 , loss:  2.2677806484277427\n",
      "Batch:  59 , correct:  1 , loss:  2.266886034916446\n",
      "Batch:  60 , correct:  4 , loss:  2.2669882953865215\n",
      "Batch:  61 , correct:  6 , loss:  2.267090645548804\n",
      "Batch:  62 , correct:  4 , loss:  2.2671942404704866\n",
      "Batch:  63 , correct:  1 , loss:  2.2672991833583054\n",
      "Batch:  64 , correct:  9 , loss:  2.267403065449852\n",
      "Batch:  65 , correct:  3 , loss:  2.2665085319498925\n",
      "Batch:  66 , correct:  5 , loss:  2.266613120560174\n",
      "Batch:  67 , correct:  6 , loss:  2.266716819201403\n",
      "Batch:  68 , correct:  6 , loss:  2.2668205963472796\n",
      "Batch:  69 , correct:  2 , loss:  2.266924832688979\n",
      "Batch:  70 , correct:  3 , loss:  2.267029155323293\n",
      "Batch:  71 , correct:  3 , loss:  2.2671298850961428\n",
      "Batch:  72 , correct:  5 , loss:  2.267233800309881\n",
      "Batch:  73 , correct:  5 , loss:  2.2673387400603913\n",
      "Batch:  74 , correct:  4 , loss:  2.266444218943261\n",
      "Batch:  75 , correct:  0 , loss:  2.2665482754180473\n",
      "Batch:  76 , correct:  1 , loss:  2.2666506505979296\n",
      "Batch:  77 , correct:  1 , loss:  2.2667529680599983\n",
      "Batch:  78 , correct:  3 , loss:  2.2668567420425263\n",
      "Batch:  79 , correct:  3 , loss:  2.266960596672043\n",
      "Batch:  80 , correct:  1 , loss:  2.2670649116438435\n",
      "Batch:  81 , correct:  6 , loss:  2.267169386650877\n",
      "Batch:  82 , correct:  2 , loss:  2.267273277183387\n",
      "Batch:  83 , correct:  8 , loss:  2.267377815466572\n",
      "Batch:  84 , correct:  5 , loss:  2.267480087326542\n",
      "Batch:  85 , correct:  3 , loss:  2.2675849774067802\n",
      "Batch:  86 , correct:  1 , loss:  2.267688883630516\n",
      "Batch:  87 , correct:  1 , loss:  2.267793831665916\n",
      "Batch:  88 , correct:  5 , loss:  2.2678980825234087\n",
      "Batch:  89 , correct:  3 , loss:  2.268003089771949\n",
      "Batch:  90 , correct:  5 , loss:  2.2681070145699653\n",
      "Batch:  91 , correct:  2 , loss:  2.2682120789560107\n",
      "Batch:  92 , correct:  0 , loss:  2.2683142846592785\n",
      "Batch:  93 , correct:  1 , loss:  2.268414767369512\n",
      "Batch:  94 , correct:  5 , loss:  2.268519175132362\n",
      "Batch:  95 , correct:  3 , loss:  2.2686212570096833\n",
      "Batch:  96 , correct:  2 , loss:  2.267726464747124\n",
      "Batch:  97 , correct:  3 , loss:  2.267830732012546\n",
      "Batch:  98 , correct:  2 , loss:  2.267933015877325\n",
      "Batch:  99 , correct:  4 , loss:  2.2680369613274385\n",
      "Batch:  100 , correct:  6 , loss:  2.2681412594493677\n",
      "Batch:  101 , correct:  2 , loss:  2.268245264170745\n",
      "Batch:  102 , correct:  2 , loss:  2.2683497029069697\n",
      "Batch:  103 , correct:  1 , loss:  2.26845198851658\n",
      "Batch:  104 , correct:  4 , loss:  2.2685540906410218\n",
      "Batch:  105 , correct:  5 , loss:  2.267659314147709\n",
      "Batch:  106 , correct:  3 , loss:  2.2677630248330507\n",
      "Batch:  107 , correct:  3 , loss:  2.267867107579145\n",
      "Batch:  108 , correct:  1 , loss:  2.2679694864459674\n",
      "Batch:  109 , correct:  3 , loss:  2.2680699546123955\n",
      "Batch:  110 , correct:  2 , loss:  2.268174928747657\n",
      "Batch:  111 , correct:  3 , loss:  2.26827864360236\n",
      "Batch:  112 , correct:  1 , loss:  2.2683827193372976\n",
      "Batch:  113 , correct:  5 , loss:  2.2684869510878856\n",
      "Batch:  114 , correct:  2 , loss:  2.268591945145654\n",
      "Batch:  115 , correct:  5 , loss:  2.268697019107595\n",
      "Batch:  116 , correct:  2 , loss:  2.268799333317532\n",
      "Batch:  117 , correct:  5 , loss:  2.2689044676434778\n",
      "Batch:  118 , correct:  2 , loss:  2.2690064814487405\n",
      "Batch:  119 , correct:  3 , loss:  2.2691116721117726\n",
      "Batch:  120 , correct:  4 , loss:  2.2692120123102875\n",
      "Batch:  121 , correct:  1 , loss:  2.2683170969013893\n",
      "Batch:  122 , correct:  4 , loss:  2.2684202273306173\n",
      "Batch:  123 , correct:  2 , loss:  2.268524456952534\n",
      "Batch:  124 , correct:  1 , loss:  2.2686284873536713\n",
      "Batch:  125 , correct:  4 , loss:  2.268730556859153\n",
      "Batch:  126 , correct:  5 , loss:  2.2688348252218544\n",
      "Batch:  127 , correct:  4 , loss:  2.2689391769610356\n",
      "Batch:  128 , correct:  2 , loss:  2.2690436042349327\n",
      "Batch:  129 , correct:  4 , loss:  2.269145691216925\n",
      "Batch:  130 , correct:  2 , loss:  2.2692508586824176\n",
      "Batch:  131 , correct:  1 , loss:  2.269354838777112\n",
      "Batch:  132 , correct:  4 , loss:  2.2694570295959\n",
      "Batch:  133 , correct:  2 , loss:  2.2685620660287498\n",
      "Batch:  134 , correct:  2 , loss:  2.268664257686219\n",
      "Batch:  135 , correct:  4 , loss:  2.2687683579118776\n",
      "Batch:  136 , correct:  2 , loss:  2.2688706689408757\n",
      "Batch:  137 , correct:  7 , loss:  2.2689758919138945\n",
      "Batch:  138 , correct:  1 , loss:  2.2690800354935763\n",
      "Batch:  139 , correct:  3 , loss:  2.269184258213762\n",
      "Batch:  140 , correct:  1 , loss:  2.2692895181131343\n",
      "Batch:  141 , correct:  3 , loss:  2.2693938735527506\n",
      "Batch:  142 , correct:  7 , loss:  2.2694978936336407\n",
      "Batch:  143 , correct:  4 , loss:  2.2695999959694997\n",
      "Batch:  144 , correct:  4 , loss:  2.2697033224456282\n",
      "Batch:  145 , correct:  7 , loss:  2.268808307797023\n",
      "Batch:  146 , correct:  2 , loss:  2.268908520745553\n",
      "Batch:  147 , correct:  4 , loss:  2.269011993118756\n",
      "Batch:  148 , correct:  3 , loss:  2.2691162353596908\n",
      "Batch:  149 , correct:  1 , loss:  2.2692215083322735\n",
      "Total Loss =  2.26837791011848\n",
      "\n",
      "EPOCH:  7\n",
      "Batch:  0 , correct:  2 , loss:  2.269325560332858\n",
      "Batch:  1 , correct:  7 , loss:  2.269430896525186\n",
      "Batch:  2 , correct:  3 , loss:  2.269535152540244\n",
      "Batch:  3 , correct:  3 , loss:  2.269640542433202\n",
      "Batch:  4 , correct:  3 , loss:  2.2697426822021063\n",
      "Batch:  5 , correct:  3 , loss:  2.2698447376748767\n",
      "Batch:  6 , correct:  5 , loss:  2.2699490111819123\n",
      "Batch:  7 , correct:  6 , loss:  2.270054419744786\n",
      "Batch:  8 , correct:  4 , loss:  2.270158752754653\n",
      "Batch:  9 , correct:  5 , loss:  2.2702642198463336\n",
      "Batch:  10 , correct:  3 , loss:  2.270368168146493\n",
      "Batch:  11 , correct:  0 , loss:  2.2704681728337444\n",
      "Batch:  12 , correct:  4 , loss:  2.2705725206170233\n",
      "Batch:  13 , correct:  2 , loss:  2.269677320003981\n",
      "Batch:  14 , correct:  2 , loss:  2.269780636067596\n",
      "Batch:  15 , correct:  2 , loss:  2.26988076546594\n",
      "Batch:  16 , correct:  8 , loss:  2.269982863862539\n",
      "Batch:  17 , correct:  2 , loss:  2.2690877842439994\n",
      "Batch:  18 , correct:  2 , loss:  2.2691900350285703\n",
      "Batch:  19 , correct:  2 , loss:  2.2692941065414716\n",
      "Batch:  20 , correct:  5 , loss:  2.2693982676060216\n",
      "Batch:  21 , correct:  4 , loss:  2.2695025074157997\n",
      "Batch:  22 , correct:  4 , loss:  2.269606828510636\n",
      "Batch:  23 , correct:  2 , loss:  2.2697112548094642\n",
      "Batch:  24 , correct:  1 , loss:  2.2688162313007023\n",
      "Batch:  25 , correct:  2 , loss:  2.268916455720102\n",
      "Batch:  26 , correct:  6 , loss:  2.2690190955822485\n",
      "Batch:  27 , correct:  3 , loss:  2.2691236413280307\n",
      "Batch:  28 , correct:  4 , loss:  2.2692280403636467\n",
      "Batch:  29 , correct:  3 , loss:  2.2693313526706764\n",
      "Batch:  30 , correct:  3 , loss:  2.2694315775185574\n",
      "Batch:  31 , correct:  7 , loss:  2.2695361428291103\n",
      "Batch:  32 , correct:  5 , loss:  2.269639495424489\n",
      "Batch:  33 , correct:  3 , loss:  2.2697448694340245\n",
      "Batch:  34 , correct:  6 , loss:  2.269847442311171\n",
      "Batch:  35 , correct:  0 , loss:  2.2689523896008335\n",
      "Batch:  36 , correct:  0 , loss:  2.269057053020432\n",
      "Batch:  37 , correct:  3 , loss:  2.2691592259245827\n",
      "Batch:  38 , correct:  2 , loss:  2.2692646993651704\n",
      "Batch:  39 , correct:  1 , loss:  2.269369070859525\n",
      "Batch:  40 , correct:  4 , loss:  2.269473520874777\n",
      "Batch:  41 , correct:  4 , loss:  2.2695768870347974\n",
      "Batch:  42 , correct:  3 , loss:  2.269682374827053\n",
      "Batch:  43 , correct:  0 , loss:  2.2697849658539555\n",
      "Batch:  44 , correct:  2 , loss:  2.269887632082516\n",
      "Batch:  45 , correct:  5 , loss:  2.269992088757478\n",
      "Batch:  46 , correct:  3 , loss:  2.2700921851892444\n",
      "Batch:  47 , correct:  1 , loss:  2.270194893350213\n",
      "Batch:  48 , correct:  3 , loss:  2.270295056452206\n",
      "Batch:  49 , correct:  4 , loss:  2.270399523385139\n",
      "Batch:  50 , correct:  2 , loss:  2.270502804399456\n",
      "Batch:  51 , correct:  4 , loss:  2.270606164790297\n",
      "Batch:  52 , correct:  4 , loss:  2.2707077728157565\n",
      "Batch:  53 , correct:  2 , loss:  2.269812546101162\n",
      "Batch:  54 , correct:  0 , loss:  2.2699179882530887\n",
      "Batch:  55 , correct:  2 , loss:  2.2700225358711137\n",
      "Batch:  56 , correct:  0 , loss:  2.270126980792832\n",
      "Batch:  57 , correct:  5 , loss:  2.2702324656291983\n",
      "Batch:  58 , correct:  2 , loss:  2.2703344057084713\n",
      "Batch:  59 , correct:  5 , loss:  2.2704364300958333\n",
      "Batch:  60 , correct:  4 , loss:  2.2705400514837644\n",
      "Batch:  61 , correct:  3 , loss:  2.270644494925662\n",
      "Batch:  62 , correct:  3 , loss:  2.2707470992589545\n",
      "Batch:  63 , correct:  4 , loss:  2.2708486831351955\n",
      "Batch:  64 , correct:  5 , loss:  2.2699534228906493\n",
      "Batch:  65 , correct:  2 , loss:  2.2700568018390714\n",
      "Batch:  66 , correct:  3 , loss:  2.270162297892448\n",
      "Batch:  67 , correct:  4 , loss:  2.27026681669286\n",
      "Batch:  68 , correct:  2 , loss:  2.2703688563965008\n",
      "Batch:  69 , correct:  4 , loss:  2.2704743911668954\n",
      "Batch:  70 , correct:  0 , loss:  2.269579210247008\n",
      "Batch:  71 , correct:  1 , loss:  2.2696838497425214\n",
      "Batch:  72 , correct:  5 , loss:  2.269788344399483\n",
      "Batch:  73 , correct:  2 , loss:  2.2698904806155444\n",
      "Batch:  74 , correct:  3 , loss:  2.2699905165085084\n",
      "Batch:  75 , correct:  3 , loss:  2.270096132836825\n",
      "Batch:  76 , correct:  2 , loss:  2.2701987545899365\n",
      "Batch:  77 , correct:  4 , loss:  2.270301457610279\n",
      "Batch:  78 , correct:  3 , loss:  2.2704047655600026\n",
      "Batch:  79 , correct:  1 , loss:  2.2705103971445846\n",
      "Batch:  80 , correct:  3 , loss:  2.27061249411634\n",
      "Batch:  81 , correct:  2 , loss:  2.270718182019502\n",
      "Batch:  82 , correct:  2 , loss:  2.270820334446063\n",
      "Batch:  83 , correct:  2 , loss:  2.27092029063051\n",
      "Batch:  84 , correct:  3 , loss:  2.2710246351446353\n",
      "Batch:  85 , correct:  3 , loss:  2.2711290522201897\n",
      "Batch:  86 , correct:  2 , loss:  2.2712316713984047\n",
      "Batch:  87 , correct:  4 , loss:  2.2713361186580143\n",
      "Batch:  88 , correct:  0 , loss:  2.2714406412401997\n",
      "Batch:  89 , correct:  2 , loss:  2.2715439868808636\n",
      "Batch:  90 , correct:  3 , loss:  2.2716485743798502\n",
      "Batch:  91 , correct:  3 , loss:  2.2717541159405217\n",
      "Batch:  92 , correct:  1 , loss:  2.2718597406736447\n",
      "Batch:  93 , correct:  1 , loss:  2.271961014451318\n",
      "Batch:  94 , correct:  1 , loss:  2.271065551726958\n",
      "Batch:  95 , correct:  3 , loss:  2.2701702665029853\n",
      "Batch:  96 , correct:  4 , loss:  2.2702729807397506\n",
      "Batch:  97 , correct:  5 , loss:  2.2703729923083404\n",
      "Batch:  98 , correct:  4 , loss:  2.2704744711640314\n",
      "Batch:  99 , correct:  5 , loss:  2.270580263038739\n",
      "Batch:  100 , correct:  5 , loss:  2.2706817986583703\n",
      "Batch:  101 , correct:  2 , loss:  2.270784515543002\n",
      "Batch:  102 , correct:  2 , loss:  2.2708887544305028\n",
      "Batch:  103 , correct:  7 , loss:  2.270992122714028\n",
      "Batch:  104 , correct:  4 , loss:  2.2710920840205513\n",
      "Batch:  105 , correct:  2 , loss:  2.2711978461226523\n",
      "Batch:  106 , correct:  4 , loss:  2.2712993664418226\n",
      "Batch:  107 , correct:  4 , loss:  2.271403604107257\n",
      "Batch:  108 , correct:  3 , loss:  2.2715055277759784\n",
      "Batch:  109 , correct:  2 , loss:  2.2716075337511263\n",
      "Batch:  110 , correct:  4 , loss:  2.2717096181999694\n",
      "Batch:  111 , correct:  3 , loss:  2.2718126035330295\n",
      "Batch:  112 , correct:  5 , loss:  2.271918324196519\n",
      "Batch:  113 , correct:  5 , loss:  2.271022863888181\n",
      "Batch:  114 , correct:  7 , loss:  2.271125993599224\n",
      "Batch:  115 , correct:  2 , loss:  2.270230696125979\n",
      "Batch:  116 , correct:  5 , loss:  2.270336635378599\n",
      "Batch:  117 , correct:  3 , loss:  2.269441498513017\n",
      "Batch:  118 , correct:  3 , loss:  2.2695438254969047\n",
      "Batch:  119 , correct:  4 , loss:  2.2696471568864287\n",
      "Batch:  120 , correct:  5 , loss:  2.269747205257739\n",
      "Batch:  121 , correct:  3 , loss:  2.268852190759935\n",
      "Batch:  122 , correct:  2 , loss:  2.268955678605357\n",
      "Batch:  123 , correct:  4 , loss:  2.269061772311751\n",
      "Batch:  124 , correct:  1 , loss:  2.269165315262123\n",
      "Batch:  125 , correct:  2 , loss:  2.2692714697185856\n",
      "Batch:  126 , correct:  4 , loss:  2.2693738353433774\n",
      "Batch:  127 , correct:  8 , loss:  2.2694800523795546\n",
      "Batch:  128 , correct:  2 , loss:  2.2695845527945897\n",
      "Batch:  129 , correct:  3 , loss:  2.2696872084455006\n",
      "Batch:  130 , correct:  0 , loss:  2.2697934632185137\n",
      "Batch:  131 , correct:  4 , loss:  2.26989348634223\n",
      "Batch:  132 , correct:  4 , loss:  2.2699977123332715\n",
      "Batch:  133 , correct:  4 , loss:  2.2700991859897353\n",
      "Batch:  134 , correct:  5 , loss:  2.270199252175492\n",
      "Batch:  135 , correct:  2 , loss:  2.2702993953258686\n",
      "Batch:  136 , correct:  2 , loss:  2.2704009114524455\n",
      "Batch:  137 , correct:  3 , loss:  2.2695057669555676\n",
      "Batch:  138 , correct:  1 , loss:  2.269609220925603\n",
      "Batch:  139 , correct:  2 , loss:  2.2697135051755537\n",
      "Batch:  140 , correct:  2 , loss:  2.2698170257855486\n",
      "Batch:  141 , correct:  5 , loss:  2.269923253115756\n",
      "Batch:  142 , correct:  4 , loss:  2.269028205945327\n",
      "Batch:  143 , correct:  2 , loss:  2.268133340100725\n",
      "Batch:  144 , correct:  2 , loss:  2.267238661280314\n",
      "Batch:  145 , correct:  7 , loss:  2.267345212084336\n",
      "Batch:  146 , correct:  0 , loss:  2.267449008875656\n",
      "Batch:  147 , correct:  3 , loss:  2.2675556261621987\n",
      "Batch:  148 , correct:  7 , loss:  2.2676574001268\n",
      "Batch:  149 , correct:  2 , loss:  2.2677640721831613\n",
      "Total Loss =  2.2700218184689374\n",
      "\n",
      "EPOCH:  8\n",
      "Batch:  0 , correct:  4 , loss:  2.267867885527682\n",
      "Batch:  1 , correct:  1 , loss:  2.2679696950883494\n",
      "Batch:  2 , correct:  3 , loss:  2.2680720394930485\n",
      "Batch:  3 , correct:  8 , loss:  2.2671773680318164\n",
      "Batch:  4 , correct:  5 , loss:  2.266282880347526\n",
      "Batch:  5 , correct:  4 , loss:  2.2653885833438085\n",
      "Batch:  6 , correct:  7 , loss:  2.264494472182592\n",
      "Batch:  7 , correct:  4 , loss:  2.264595087572669\n",
      "Batch:  8 , correct:  0 , loss:  2.26469979230373\n",
      "Batch:  9 , correct:  2 , loss:  2.264804563534335\n",
      "Batch:  10 , correct:  3 , loss:  2.2649093282153165\n",
      "Batch:  11 , correct:  0 , loss:  2.265013426327501\n",
      "Batch:  12 , correct:  4 , loss:  2.2651140383081207\n",
      "Batch:  13 , correct:  4 , loss:  2.2652147248722567\n",
      "Batch:  14 , correct:  3 , loss:  2.2643206558153173\n",
      "Batch:  15 , correct:  2 , loss:  2.264425514888589\n",
      "Batch:  16 , correct:  5 , loss:  2.2645284443681106\n",
      "Batch:  17 , correct:  3 , loss:  2.264631447234219\n",
      "Batch:  18 , correct:  3 , loss:  2.2647348863310888\n",
      "Batch:  19 , correct:  2 , loss:  2.2638409074688832\n",
      "Batch:  20 , correct:  4 , loss:  2.2639458707886457\n",
      "Batch:  21 , correct:  6 , loss:  2.2630520560571634\n",
      "Batch:  22 , correct:  2 , loss:  2.2631548054272894\n",
      "Batch:  23 , correct:  3 , loss:  2.2632579924079272\n",
      "Batch:  24 , correct:  2 , loss:  2.262364321097733\n",
      "Batch:  25 , correct:  2 , loss:  2.2624692755008997\n",
      "Batch:  26 , correct:  1 , loss:  2.2625714977420675\n",
      "Batch:  27 , correct:  5 , loss:  2.2626724197495647\n",
      "Batch:  28 , correct:  3 , loss:  2.262775708384371\n",
      "Batch:  29 , correct:  0 , loss:  2.262880676340218\n",
      "Batch:  30 , correct:  3 , loss:  2.262987617813438\n",
      "Batch:  31 , correct:  4 , loss:  2.2630909423079064\n",
      "Batch:  32 , correct:  1 , loss:  2.263197941119838\n",
      "Batch:  33 , correct:  3 , loss:  2.2633006669859443\n",
      "Batch:  34 , correct:  4 , loss:  2.263405634451236\n",
      "Batch:  35 , correct:  5 , loss:  2.263509129307111\n",
      "Batch:  36 , correct:  1 , loss:  2.263612449731415\n",
      "Batch:  37 , correct:  3 , loss:  2.263715845494206\n",
      "Batch:  38 , correct:  5 , loss:  2.2638208288192696\n",
      "Batch:  39 , correct:  4 , loss:  2.262927044278656\n",
      "Batch:  40 , correct:  4 , loss:  2.2630291853476034\n",
      "Batch:  41 , correct:  2 , loss:  2.26313002077438\n",
      "Batch:  42 , correct:  5 , loss:  2.2632309386876344\n",
      "Batch:  43 , correct:  5 , loss:  2.2633379109538296\n",
      "Batch:  44 , correct:  6 , loss:  2.2624442215129403\n",
      "Batch:  45 , correct:  4 , loss:  2.2625483356585736\n",
      "Batch:  46 , correct:  1 , loss:  2.262651911755358\n",
      "Batch:  47 , correct:  1 , loss:  2.2627555639323997\n",
      "Batch:  48 , correct:  4 , loss:  2.2628577491053923\n",
      "Batch:  49 , correct:  3 , loss:  2.2629626188457816\n",
      "Batch:  50 , correct:  5 , loss:  2.2630653045924545\n",
      "Batch:  51 , correct:  1 , loss:  2.2631702391620543\n",
      "Batch:  52 , correct:  1 , loss:  2.2632729784607815\n",
      "Batch:  53 , correct:  4 , loss:  2.263377951933884\n",
      "Batch:  54 , correct:  1 , loss:  2.2634788249201474\n",
      "Batch:  55 , correct:  3 , loss:  2.263582164002535\n",
      "Batch:  56 , correct:  4 , loss:  2.2636890695580814\n",
      "Batch:  57 , correct:  5 , loss:  2.263796053018444\n",
      "Batch:  58 , correct:  3 , loss:  2.2639009362632576\n",
      "Batch:  59 , correct:  1 , loss:  2.264004287005748\n",
      "Batch:  60 , correct:  3 , loss:  2.2641092190095575\n",
      "Batch:  61 , correct:  4 , loss:  2.2642118792355417\n",
      "Batch:  62 , correct:  4 , loss:  2.2643152666481834\n",
      "Batch:  63 , correct:  8 , loss:  2.264420234254043\n",
      "Batch:  64 , correct:  5 , loss:  2.2635263202996447\n",
      "Batch:  65 , correct:  3 , loss:  2.2636333407783487\n",
      "Batch:  66 , correct:  4 , loss:  2.2627395878907324\n",
      "Batch:  67 , correct:  2 , loss:  2.2628404946653156\n",
      "Batch:  68 , correct:  2 , loss:  2.2629425761488386\n",
      "Batch:  69 , correct:  0 , loss:  2.263045373973868\n",
      "Batch:  70 , correct:  4 , loss:  2.263148907662405\n",
      "Batch:  71 , correct:  5 , loss:  2.2632540166663517\n",
      "Batch:  72 , correct:  1 , loss:  2.2633576031360167\n",
      "Batch:  73 , correct:  5 , loss:  2.263464659128478\n",
      "Batch:  74 , correct:  0 , loss:  2.2635695140219148\n",
      "Batch:  75 , correct:  4 , loss:  2.263673134987121\n",
      "Batch:  76 , correct:  3 , loss:  2.2637739645358836\n",
      "Batch:  77 , correct:  2 , loss:  2.2638759626034477\n",
      "Batch:  78 , correct:  4 , loss:  2.2629821564786754\n",
      "Batch:  79 , correct:  2 , loss:  2.263087116349477\n",
      "Batch:  80 , correct:  5 , loss:  2.2631942255613926\n",
      "Batch:  81 , correct:  4 , loss:  2.263299326776022\n",
      "Batch:  82 , correct:  2 , loss:  2.2634043156391\n",
      "Batch:  83 , correct:  4 , loss:  2.2635076340356073\n",
      "Batch:  84 , correct:  5 , loss:  2.262613913888058\n",
      "Batch:  85 , correct:  2 , loss:  2.262721117482906\n",
      "Batch:  86 , correct:  3 , loss:  2.262826224387629\n",
      "Batch:  87 , correct:  7 , loss:  2.262927130505842\n",
      "Batch:  88 , correct:  1 , loss:  2.2630305472761303\n",
      "Batch:  89 , correct:  3 , loss:  2.26313419027757\n",
      "Batch:  90 , correct:  9 , loss:  2.2622405397333853\n",
      "Batch:  91 , correct:  5 , loss:  2.2623441292630155\n",
      "Batch:  92 , correct:  3 , loss:  2.262446118097952\n",
      "Batch:  93 , correct:  3 , loss:  2.2625487981077743\n",
      "Batch:  94 , correct:  5 , loss:  2.2626522487230027\n",
      "Batch:  95 , correct:  2 , loss:  2.2627557826067313\n",
      "Batch:  96 , correct:  5 , loss:  2.2628569858699876\n",
      "Batch:  97 , correct:  3 , loss:  2.262961983802618\n",
      "Batch:  98 , correct:  0 , loss:  2.2630656656195827\n",
      "Batch:  99 , correct:  0 , loss:  2.263168321427504\n",
      "Batch:  100 , correct:  3 , loss:  2.2632733582511118\n",
      "Batch:  101 , correct:  1 , loss:  2.2633745551472324\n",
      "Batch:  102 , correct:  1 , loss:  2.263481640171567\n",
      "Batch:  103 , correct:  4 , loss:  2.2635835059704412\n",
      "Batch:  104 , correct:  3 , loss:  2.2636906478510403\n",
      "Batch:  105 , correct:  4 , loss:  2.263793279377961\n",
      "Batch:  106 , correct:  1 , loss:  2.2638944729903248\n",
      "Batch:  107 , correct:  6 , loss:  2.263998072753706\n",
      "Batch:  108 , correct:  2 , loss:  2.2641030469482346\n",
      "Batch:  109 , correct:  3 , loss:  2.264205696200451\n",
      "Batch:  110 , correct:  1 , loss:  2.2643128136332225\n",
      "Batch:  111 , correct:  1 , loss:  2.26441749083781\n",
      "Batch:  112 , correct:  3 , loss:  2.2645207739489446\n",
      "Batch:  113 , correct:  3 , loss:  2.264624351727096\n",
      "Batch:  114 , correct:  0 , loss:  2.2647269924530558\n",
      "Batch:  115 , correct:  1 , loss:  2.264829717035802\n",
      "Batch:  116 , correct:  3 , loss:  2.264936807658194\n",
      "Batch:  117 , correct:  1 , loss:  2.265043981079431\n",
      "Batch:  118 , correct:  4 , loss:  2.2651486137911405\n",
      "Batch:  119 , correct:  3 , loss:  2.265251837343929\n",
      "Batch:  120 , correct:  5 , loss:  2.2653550507146742\n",
      "Batch:  121 , correct:  0 , loss:  2.2654622391327752\n",
      "Batch:  122 , correct:  1 , loss:  2.2655670155890175\n",
      "Batch:  123 , correct:  7 , loss:  2.265670484269768\n",
      "Batch:  124 , correct:  2 , loss:  2.265773696473229\n",
      "Batch:  125 , correct:  2 , loss:  2.265876911401879\n",
      "Batch:  126 , correct:  2 , loss:  2.265980191513776\n",
      "Batch:  127 , correct:  4 , loss:  2.2660816604130747\n",
      "Batch:  128 , correct:  3 , loss:  2.2661888091792215\n",
      "Batch:  129 , correct:  1 , loss:  2.2662920363718353\n",
      "Batch:  130 , correct:  5 , loss:  2.266396515377122\n",
      "Batch:  131 , correct:  3 , loss:  2.2664998036584385\n",
      "Batch:  132 , correct:  5 , loss:  2.266603055389613\n",
      "Batch:  133 , correct:  3 , loss:  2.2657088069797027\n",
      "Batch:  134 , correct:  5 , loss:  2.2658160266085585\n",
      "Batch:  135 , correct:  5 , loss:  2.2659175380715366\n",
      "Batch:  136 , correct:  4 , loss:  2.2660200294804818\n",
      "Batch:  137 , correct:  3 , loss:  2.2651258987000698\n",
      "Batch:  138 , correct:  2 , loss:  2.2652267457889694\n",
      "Batch:  139 , correct:  3 , loss:  2.26533147359738\n",
      "Batch:  140 , correct:  3 , loss:  2.265436280152799\n",
      "Batch:  141 , correct:  5 , loss:  2.26554355207536\n",
      "Batch:  142 , correct:  3 , loss:  2.2656480866128783\n",
      "Batch:  143 , correct:  2 , loss:  2.265752929240385\n",
      "Batch:  144 , correct:  0 , loss:  2.2658537488998505\n",
      "Batch:  145 , correct:  1 , loss:  2.265957071530269\n",
      "Batch:  146 , correct:  4 , loss:  2.266064340774843\n",
      "Batch:  147 , correct:  7 , loss:  2.2661676038619913\n",
      "Batch:  148 , correct:  2 , loss:  2.265273445136054\n",
      "Batch:  149 , correct:  2 , loss:  2.264379467786592\n",
      "Total Loss =  2.264139582853733\n",
      "\n",
      "EPOCH:  9\n",
      "Batch:  0 , correct:  3 , loss:  2.2644810783790716\n",
      "Batch:  1 , correct:  3 , loss:  2.2645844456803994\n",
      "Batch:  2 , correct:  2 , loss:  2.2646878919927063\n",
      "Batch:  3 , correct:  1 , loss:  2.264791417533356\n",
      "Batch:  4 , correct:  4 , loss:  2.2648950224834126\n",
      "Batch:  5 , correct:  2 , loss:  2.2649959257386874\n",
      "Batch:  6 , correct:  5 , loss:  2.2651004729373323\n",
      "Batch:  7 , correct:  6 , loss:  2.2652050990050987\n",
      "Batch:  8 , correct:  2 , loss:  2.265307546207517\n",
      "Batch:  9 , correct:  2 , loss:  2.264413563446033\n",
      "Batch:  10 , correct:  1 , loss:  2.2645161732623365\n",
      "Batch:  11 , correct:  5 , loss:  2.264620921492019\n",
      "Batch:  12 , correct:  4 , loss:  2.2647243031090305\n",
      "Batch:  13 , correct:  4 , loss:  2.2648276442240767\n",
      "Batch:  14 , correct:  3 , loss:  2.264930269590672\n",
      "Batch:  15 , correct:  1 , loss:  2.2650336688242025\n",
      "Batch:  16 , correct:  4 , loss:  2.2651345531498137\n",
      "Batch:  17 , correct:  4 , loss:  2.2652393032408047\n",
      "Batch:  18 , correct:  2 , loss:  2.265342824342881\n",
      "Batch:  19 , correct:  0 , loss:  2.2654461622210813\n",
      "Batch:  20 , correct:  1 , loss:  2.2655533343189203\n",
      "Batch:  21 , correct:  6 , loss:  2.2656581106164078\n",
      "Batch:  22 , correct:  2 , loss:  2.2657627265882985\n",
      "Batch:  23 , correct:  3 , loss:  2.265865370525661\n",
      "Batch:  24 , correct:  5 , loss:  2.2659681207062374\n",
      "Batch:  25 , correct:  2 , loss:  2.266071688697224\n",
      "Batch:  26 , correct:  3 , loss:  2.2661722879733657\n",
      "Batch:  27 , correct:  2 , loss:  2.2662736213340917\n",
      "Batch:  28 , correct:  3 , loss:  2.2653791431827957\n",
      "Batch:  29 , correct:  4 , loss:  2.2654864486626476\n",
      "Batch:  30 , correct:  1 , loss:  2.2655892864981904\n",
      "Batch:  31 , correct:  2 , loss:  2.2656899899243395\n",
      "Batch:  32 , correct:  3 , loss:  2.265797347261261\n",
      "Batch:  33 , correct:  4 , loss:  2.2659002156337835\n",
      "Batch:  34 , correct:  3 , loss:  2.266001609200627\n",
      "Batch:  35 , correct:  4 , loss:  2.266104540086303\n",
      "Batch:  36 , correct:  8 , loss:  2.2662091657650594\n",
      "Batch:  37 , correct:  4 , loss:  2.266311874389968\n",
      "Batch:  38 , correct:  3 , loss:  2.266415327304819\n",
      "Batch:  39 , correct:  3 , loss:  2.2665182720051646\n",
      "Batch:  40 , correct:  3 , loss:  2.266621300643794\n",
      "Batch:  41 , correct:  4 , loss:  2.266725970745216\n",
      "Batch:  42 , correct:  5 , loss:  2.2668286697523192\n",
      "Batch:  43 , correct:  4 , loss:  2.2669321207632347\n",
      "Batch:  44 , correct:  3 , loss:  2.2670366836662277\n",
      "Batch:  45 , correct:  3 , loss:  2.267140039070828\n",
      "Batch:  46 , correct:  5 , loss:  2.2672435253340844\n",
      "Batch:  47 , correct:  4 , loss:  2.26734651322021\n",
      "Batch:  48 , correct:  5 , loss:  2.2664518020517574\n",
      "Batch:  49 , correct:  2 , loss:  2.266556513877169\n",
      "Batch:  50 , correct:  6 , loss:  2.266661153234965\n",
      "Batch:  51 , correct:  6 , loss:  2.266762402780119\n",
      "Batch:  52 , correct:  4 , loss:  2.2668654898763334\n",
      "Batch:  53 , correct:  7 , loss:  2.266969036921004\n",
      "Batch:  54 , correct:  2 , loss:  2.2670724081797458\n",
      "Batch:  55 , correct:  2 , loss:  2.267172829225083\n",
      "Batch:  56 , correct:  2 , loss:  2.2672754449124835\n",
      "Batch:  57 , correct:  3 , loss:  2.267378860168188\n",
      "Batch:  58 , correct:  10 , loss:  2.2674793169630596\n",
      "Batch:  59 , correct:  5 , loss:  2.2675798475217475\n",
      "Batch:  60 , correct:  2 , loss:  2.2666851039416454\n",
      "Batch:  61 , correct:  3 , loss:  2.2667886905264285\n",
      "Batch:  62 , correct:  2 , loss:  2.265894095453597\n",
      "Batch:  63 , correct:  3 , loss:  2.2659972688586802\n",
      "Batch:  64 , correct:  5 , loss:  2.266100515325079\n",
      "Batch:  65 , correct:  4 , loss:  2.266205196399293\n",
      "Batch:  66 , correct:  3 , loss:  2.2663084924815533\n",
      "Batch:  67 , correct:  3 , loss:  2.266409178961093\n",
      "Batch:  68 , correct:  3 , loss:  2.265514670657658\n",
      "Batch:  69 , correct:  1 , loss:  2.2656193262185553\n",
      "Batch:  70 , correct:  2 , loss:  2.2657201405374328\n",
      "Batch:  71 , correct:  4 , loss:  2.2658271827403698\n",
      "Batch:  72 , correct:  1 , loss:  2.265930566389733\n",
      "Batch:  73 , correct:  2 , loss:  2.266035240808935\n",
      "Batch:  74 , correct:  4 , loss:  2.266136076408797\n",
      "Batch:  75 , correct:  2 , loss:  2.266239529001444\n",
      "Batch:  76 , correct:  1 , loss:  2.2663441841850407\n",
      "Batch:  77 , correct:  4 , loss:  2.2664467852581667\n",
      "Batch:  78 , correct:  3 , loss:  2.2665502778774043\n",
      "Batch:  79 , correct:  6 , loss:  2.2666572582032947\n",
      "Batch:  80 , correct:  2 , loss:  2.2667580657582582\n",
      "Batch:  81 , correct:  1 , loss:  2.2668613643891318\n",
      "Batch:  82 , correct:  6 , loss:  2.2669639623589393\n",
      "Batch:  83 , correct:  4 , loss:  2.2670674489742235\n",
      "Batch:  84 , correct:  4 , loss:  2.267168446322501\n",
      "Batch:  85 , correct:  6 , loss:  2.2672692583338705\n",
      "Batch:  86 , correct:  2 , loss:  2.2663745784074747\n",
      "Batch:  87 , correct:  5 , loss:  2.266477519031465\n",
      "Batch:  88 , correct:  4 , loss:  2.2655833010681405\n",
      "Batch:  89 , correct:  6 , loss:  2.2656866792072035\n",
      "Batch:  90 , correct:  4 , loss:  2.2657901412830928\n",
      "Batch:  91 , correct:  0 , loss:  2.2658946825714765\n",
      "Batch:  92 , correct:  1 , loss:  2.266001565375248\n",
      "Batch:  93 , correct:  4 , loss:  2.2661050630039\n",
      "Batch:  94 , correct:  4 , loss:  2.2662060932779755\n",
      "Batch:  95 , correct:  3 , loss:  2.266307200506727\n",
      "Batch:  96 , correct:  2 , loss:  2.266410463963394\n",
      "Batch:  97 , correct:  3 , loss:  2.266513638703725\n",
      "Batch:  98 , correct:  2 , loss:  2.2666180018989666\n",
      "Batch:  99 , correct:  2 , loss:  2.2667214768818913\n",
      "Batch:  100 , correct:  4 , loss:  2.2668250352904273\n",
      "Batch:  101 , correct:  5 , loss:  2.2669294671987696\n",
      "Batch:  102 , correct:  0 , loss:  2.2670304122800053\n",
      "Batch:  103 , correct:  4 , loss:  2.2671334334134485\n",
      "Batch:  104 , correct:  2 , loss:  2.2672379063639005\n",
      "Batch:  105 , correct:  0 , loss:  2.267340982233828\n",
      "Batch:  106 , correct:  4 , loss:  2.2674476713796876\n",
      "Batch:  107 , correct:  2 , loss:  2.2665532536538677\n",
      "Batch:  108 , correct:  2 , loss:  2.2666564059088685\n",
      "Batch:  109 , correct:  3 , loss:  2.266760741939598\n",
      "Batch:  110 , correct:  3 , loss:  2.266867551601285\n",
      "Batch:  111 , correct:  6 , loss:  2.266971946914538\n",
      "Batch:  112 , correct:  3 , loss:  2.266077625980202\n",
      "Batch:  113 , correct:  2 , loss:  2.2651834856598345\n",
      "Batch:  114 , correct:  2 , loss:  2.265284615310883\n",
      "Batch:  115 , correct:  3 , loss:  2.2653916259792584\n",
      "Batch:  116 , correct:  1 , loss:  2.2654948791888794\n",
      "Batch:  117 , correct:  5 , loss:  2.265595926089146\n",
      "Batch:  118 , correct:  1 , loss:  2.265699173570792\n",
      "Batch:  119 , correct:  2 , loss:  2.265802507534915\n",
      "Batch:  120 , correct:  2 , loss:  2.2659095096232167\n",
      "Batch:  121 , correct:  3 , loss:  2.266010597526978\n",
      "Batch:  122 , correct:  4 , loss:  2.266113831296664\n",
      "Batch:  123 , correct:  2 , loss:  2.2662169411483744\n",
      "Batch:  124 , correct:  4 , loss:  2.2663202688842916\n",
      "Batch:  125 , correct:  2 , loss:  2.2664237139910854\n",
      "Batch:  126 , correct:  3 , loss:  2.266528104014342\n",
      "Batch:  127 , correct:  1 , loss:  2.2666313355168137\n",
      "Batch:  128 , correct:  7 , loss:  2.2667344391828035\n",
      "Batch:  129 , correct:  3 , loss:  2.266835467541229\n",
      "Batch:  130 , correct:  3 , loss:  2.2669386341984175\n",
      "Batch:  131 , correct:  4 , loss:  2.26704205594158\n",
      "Batch:  132 , correct:  2 , loss:  2.2671452808858796\n",
      "Batch:  133 , correct:  0 , loss:  2.2672495614540833\n",
      "Batch:  134 , correct:  0 , loss:  2.267352745624582\n",
      "Batch:  135 , correct:  1 , loss:  2.267456013042858\n",
      "Batch:  136 , correct:  3 , loss:  2.2675591997455755\n",
      "Batch:  137 , correct:  1 , loss:  2.2676599370027253\n",
      "Batch:  138 , correct:  1 , loss:  2.26776087863915\n",
      "Batch:  139 , correct:  1 , loss:  2.2678642363827324\n",
      "Batch:  140 , correct:  4 , loss:  2.267967443638821\n",
      "Batch:  141 , correct:  3 , loss:  2.2680741069310204\n",
      "Batch:  142 , correct:  5 , loss:  2.268177370554505\n",
      "Batch:  143 , correct:  5 , loss:  2.2682815110209185\n",
      "Batch:  144 , correct:  3 , loss:  2.2673869210913815\n",
      "Batch:  145 , correct:  4 , loss:  2.2674876990445703\n",
      "Batch:  146 , correct:  3 , loss:  2.267590842993201\n",
      "Batch:  147 , correct:  3 , loss:  2.267694069342017\n",
      "Batch:  148 , correct:  3 , loss:  2.2677974114468777\n",
      "Batch:  149 , correct:  3 , loss:  2.2679005910933316\n",
      "Total Loss =  2.2663579386285813\n",
      "\n"
     ]
    }
   ],
   "source": [
    "per_epoch_parameters = train(x_me, y_me, 10, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e79c642c-b71d-4ab1-b0a3-186870bca222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, x_test, y_test):\n",
    "    conv1 = conv2d(32, 3)\n",
    "    pool1 = maxpool2d(2)\n",
    "    conv2 = conv2d(64, 5)\n",
    "    pool2 = maxpool2d(2)\n",
    "    conv3 = conv2d(64, 3)\n",
    "    fc1 = fc_1(4096, 64)\n",
    "    fc2 = fc_2(64, 10)\n",
    "    fc2.weights = parameters[0]\n",
    "    fc1.weights = parameters[1]\n",
    "    fc2.bias = parameters[2]\n",
    "    fc1.bias = parameters[3]\n",
    "    conv3.kernel = parameters[4]\n",
    "    conv2.kernel = parameters[5]\n",
    "    conv1.kernel = parameters[6]\n",
    "    \n",
    "    count = 0\n",
    "    loss = 0\n",
    "    for i in range(len(x_test)):\n",
    "        if (i%50 == 0):\n",
    "            print(i)\n",
    "        a1 = conv1.forward_pass(x_test[i])\n",
    "        a2 = pool1.forward_pass(a1)\n",
    "        a3 = conv2.forward_pass(a2)\n",
    "        a4 = pool2.forward_pass(a3)\n",
    "        a5 = conv3.forward_pass(a4).flatten()\n",
    "        a6 = fc1.forward_pass(a5)\n",
    "        a7 = fc2.forward_pass(a6)\n",
    "        out = softmax(a7)\n",
    "        pred = out.argmax()\n",
    "        loss -= np.log(pred)\n",
    "        if y_test[i] == pred:\n",
    "            count += 1\n",
    "    return (count/len(x_test), loss/len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1080c06b-3478-4c48-a04d-606a5f22374c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 3, 32, 32), (1000,))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = unpickle(\"./content/test_batch\")\n",
    "x_load = test[b'data']\n",
    "y_load = test[b'labels']\n",
    "x_test = []\n",
    "y_test = []\n",
    "for i in range(1000):\n",
    "    x_test.append(x_train[i].reshape(3,32,32))\n",
    "    y_test.append(y_train[i])\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "52b2ec15-a387-4a5d-9a8c-b1ea05e2d483",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n"
     ]
    }
   ],
   "source": [
    "pred_0 = predict(per_epoch_parameters[0], x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba27b356-9616-494d-b003-5911ccbd6d43",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n"
     ]
    }
   ],
   "source": [
    "pred_1 = predict(per_epoch_parameters[1], x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "54ff4e1b-563b-4c29-a4f5-ef52f64b2077",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n"
     ]
    }
   ],
   "source": [
    "pred_2 = predict(per_epoch_parameters[2], x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c78cccd9-e0bc-4aa9-ba19-c8d5a84a9c9c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n"
     ]
    }
   ],
   "source": [
    "pred_3 = predict(per_epoch_parameters[3], x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "97424f1f-3c68-4830-82e2-865b455da101",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n"
     ]
    }
   ],
   "source": [
    "pred_4 = predict(per_epoch_parameters[4], x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "57b3171e-5929-48ab-b629-a8828edb9230",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n"
     ]
    }
   ],
   "source": [
    "pred_5 = predict(per_epoch_parameters[5], x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "75e6e3c6-0d44-4ba3-8d40-9786d168a9c0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n"
     ]
    }
   ],
   "source": [
    "pred_6 = predict(per_epoch_parameters[6], x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "759c4b6f-acd0-43b4-a73a-8fe571d7a65e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n"
     ]
    }
   ],
   "source": [
    "pred_7 = predict(per_epoch_parameters[7], x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cb99d77e-5648-4334-8a50-3d91fbdba9ee",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n"
     ]
    }
   ],
   "source": [
    "pred_8 = predict(per_epoch_parameters[8], x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "63972458-a1ce-4c52-b503-e01bc5ee074c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n"
     ]
    }
   ],
   "source": [
    "pred_9 = predict(per_epoch_parameters[9], x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "860daec2-7509-4484-8287-42995f9324dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.102, -1.9459101490553508),\n",
       " (0.099, -2.079441541679856),\n",
       " (0.099, -2.079441541679856),\n",
       " (0.099, -2.079441541679856),\n",
       " (0.099, -2.079441541679856),\n",
       " (0.099, -2.079441541679856),\n",
       " (0.099, -2.079441541679856),\n",
       " (0.099, -2.079441541679856),\n",
       " (0.099, -2.079441541679856),\n",
       " (0.099, -2.079441541679856))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_0, pred_1, pred_2, pred_3, pred_4, pred_5, pred_6, pred_7, pred_8, pred_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0ead95e4-5f9e-449c-8b22-086e1dc9eba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train(train_losses, val_losses, val_accuracy):\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    # fig.suptitle(f\"LR = {self.lr}, Optimizer = {self.config['optimizer']}\", fontsize=16)\n",
    "    ax0 = fig.add_subplot(121, title=\"Loss\")\n",
    "    ax0.plot(np.arange(len(train_losses)), train_losses, 'bo-', label = \"Train\")\n",
    "    ax0.plot(np.arange(len(val_losses)), val_losses, 'ro-', label = \"Val\")\n",
    "    ax0.set_xlabel(\"Epoch\")\n",
    "    ax0.set_ylabel(\"Loss value\")\n",
    "    ax0.legend()\n",
    "\n",
    "    ax1 = fig.add_subplot(122, title=\"Accuracy\")\n",
    "    # ax1.plot(np.arange(len(train_acc)), train_acc, 'bo-', label = \"Train\")\n",
    "    ax1.plot(np.arange(len(val_acc)), val_acc, 'ro-', label = \"Val\")\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Accuracy (%)\")\n",
    "    ax1.legend()\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "be516b51-d930-4ccc-b705-55a82670ff32",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_losses = [1.9459101490553508, 2.079441541679856, 2.079441541679856, 2.079441541679856, 2.079441541679856, 2.079441541679856, 2.079441541679856, 2.079441541679856, 2.079441541679856, 2.079441541679856]\n",
    "train_losses = [2.2833065306832396,2.2820639403859,2.2799167322072957,2.2790680393802756,2.273113162367153,2.268954887741895,2.26837791011848,2.2700218184689374,2.264139582853733,2.2663579386285813]\n",
    "val_acc = [10.2, 9.9, 9.9, 9.9, 9.9, 9.9, 9.9, 9.9, 9.9, 9.9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12561092-bb91-4dad-8db0-927755c35c76",
   "metadata": {},
   "source": [
    "## Plotting the Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "22076a7d-e0db-48af-bd5d-0ffe76e7db6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_train(train_losses, val_losses, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaef6d8-3040-44de-83e4-13bd2c79c5c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
